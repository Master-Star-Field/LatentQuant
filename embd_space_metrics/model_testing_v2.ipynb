{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Testing Notebook - CKA Evaluation\n",
        "\n",
        "This notebook evaluates quantized DeepLabV3 models by comparing original backbone features with quantized features using CKA (Centered Kernel Alignment) metric.\n",
        "\n",
        "## Overview\n",
        "- Load trained quantized models from `models/` folder\n",
        "- Extract features from Oxford-IIIT-Pet dataset\n",
        "- Compute CKA similarity between original and quantized features\n",
        "- Extensible framework for other metrics (CCA, Geometric Score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "%pip install torch-cka\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Progress bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Scientific computing for advanced metrics\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# Import quantization libraries\n",
        "from vector_quantize_pytorch import VectorQuantize, FSQ, ResidualVQ, LFQ\n",
        "\n",
        "# Import CKA\n",
        "from torch_cka import CKA\n",
        "\n",
        "# Device setup\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using NVIDIA GPU\")\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using Apple Silicon GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "    \n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Define paths\n",
        "MODELS_DIR = \"./models\"\n",
        "DATA_DIR = \"./data\"\n",
        "\n",
        "print(f\"Models directory: {MODELS_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Architecture Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SegmentationEncoder:\n",
        "    \"\"\"\n",
        "    Encoder с residual adapter (features + adapter(features))\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, device, feature_dim=2048, add_adapter=True):\n",
        "        self.backbone = backbone\n",
        "        self.device = device\n",
        "        self.add_adapter = add_adapter\n",
        "        \n",
        "        # Backbone frozen\n",
        "        self.backbone.eval()\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Residual adapter\n",
        "        if add_adapter:\n",
        "            self.adapter = nn.Sequential(\n",
        "                nn.Conv2d(feature_dim, feature_dim, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(feature_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
        "            ).to(device)\n",
        "            \n",
        "            # ВАЖНО: Последний слой инициализируем нулями\n",
        "            # Так в начале: features + 0 = features (identity)\n",
        "            nn.init.zeros_(self.adapter[3].weight)\n",
        "            nn.init.zeros_(self.adapter[3].bias)\n",
        "        else:\n",
        "            self.adapter = None\n",
        "        \n",
        "    def encode(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.backbone(images)['out']\n",
        "        \n",
        "        if self.adapter is not None:\n",
        "            # RESIDUAL: добавляем, а не заменяем!\n",
        "            features = features + self.adapter(features)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def parameters(self):\n",
        "        if self.adapter is not None:\n",
        "            return self.adapter.parameters()\n",
        "        return []\n",
        "\n",
        "\n",
        "class SegmentationDecoder:\n",
        "    \"\"\"\n",
        "    Decoder с residual adapter (latents + adapter(latents))\n",
        "    \"\"\"\n",
        "    def __init__(self, classifier, device, feature_dim=2048, add_adapter=True):\n",
        "        self.classifier = classifier\n",
        "        self.device = device\n",
        "        self.add_adapter = add_adapter\n",
        "        \n",
        "        # Classifier frozen\n",
        "        self.classifier.eval()\n",
        "        for param in self.classifier.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Residual adapter\n",
        "        if add_adapter:\n",
        "            self.adapter = nn.Sequential(\n",
        "                nn.Conv2d(feature_dim, feature_dim, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(feature_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
        "            ).to(device)\n",
        "            \n",
        "            # ВАЖНО: Последний слой инициализируем нулями\n",
        "            nn.init.zeros_(self.adapter[3].weight)\n",
        "            nn.init.zeros_(self.adapter[3].bias)\n",
        "        else:\n",
        "            self.adapter = None\n",
        "        \n",
        "    def decode(self, latents, target_size):\n",
        "        if self.adapter is not None:\n",
        "            # RESIDUAL: добавляем, а не заменяем!\n",
        "            latents = latents + self.adapter(latents)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = self.classifier(latents)\n",
        "            return F.interpolate(output, size=target_size, mode='bilinear')\n",
        "    \n",
        "    def parameters(self):\n",
        "        if self.adapter is not None:\n",
        "            return self.adapter.parameters()\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuantizerWrapper:\n",
        "    \"\"\"\n",
        "    Универсальная обертка для ВСЕХ quantizers (включая все WithProjection).\n",
        "    \"\"\"\n",
        "    def __init__(self, quantizer, name=\"Quantizer\"):\n",
        "        self.quantizer = quantizer\n",
        "        self.name = name\n",
        "        \n",
        "    def quantize_spatial(self, features):\n",
        "        \"\"\"Квантизует пространственные признаки [B, C, H, W]\"\"\"\n",
        "        B, C, H, W = features.shape\n",
        "        feat_seq = features.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
        "        \n",
        "        # FSQ и FSQWithProjection\n",
        "        if isinstance(self.quantizer, FSQ) or hasattr(self.quantizer, 'fsq'):\n",
        "            quant_seq, indices = self.quantizer(feat_seq)\n",
        "            loss = torch.tensor(0.0, device=features.device)\n",
        "            \n",
        "        # LFQ и LFQWithProjection\n",
        "        elif isinstance(self.quantizer, LFQ) or hasattr(self.quantizer, 'lfq'):\n",
        "            quant_seq, indices, loss = self.quantizer(feat_seq)\n",
        "            \n",
        "        # VQ, ResidualVQ и их WithProjection версии\n",
        "        elif isinstance(self.quantizer, (VectorQuantize, ResidualVQ)) or hasattr(self.quantizer, 'residual_vq'):\n",
        "            quant_seq, indices, loss = self.quantizer(feat_seq)\n",
        "            \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown quantizer type: {type(self.quantizer)}\")\n",
        "        \n",
        "        quantized = quant_seq.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        \n",
        "        if loss.numel() > 1:\n",
        "            loss = loss.float().mean()\n",
        "        \n",
        "        return quantized, loss\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"{self.name}({self.quantizer})\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantizer classes with projection layers (copied from original notebook)\n",
        "\n",
        "class FSQWithProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    FSQ требует обучаемого encoder/decoder.\n",
        "    С frozen backbone добавляем projection layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, levels, device):\n",
        "        super().__init__()\n",
        "        self.num_levels = len(levels)\n",
        "        # Project от input_dim к num_levels\n",
        "        self.project_in = nn.Linear(input_dim, self.num_levels).to(device)\n",
        "        self.fsq = FSQ(levels=levels, dim=self.num_levels).to(device)\n",
        "        # Project обратно к input_dim\n",
        "        self.project_out = nn.Linear(self.num_levels, input_dim).to(device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: [B, N, input_dim]\n",
        "        x_proj = self.project_in(x)  # [B, N, num_levels]\n",
        "        quantized, indices = self.fsq(x_proj)  # FSQ\n",
        "        x_out = self.project_out(quantized)  # [B, N, input_dim]\n",
        "        return x_out, indices\n",
        "    \n",
        "    def parameters(self):\n",
        "        return list(self.project_in.parameters()) + list(self.project_out.parameters())\n",
        "\n",
        "\n",
        "class ResidualVQWithProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    ResidualVQ в 2048D = 4×256×2048 = огромный codebook!\n",
        "    Решение: Project к низкой размерности: 2048 → 64D → 2048\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_quantizers, codebook_size, decay, commitment_weight, device):\n",
        "        super().__init__()\n",
        "        # Уменьшаем размерность для компактного codebook\n",
        "        self.bottleneck_dim = 64\n",
        "        \n",
        "        self.project_in = nn.Linear(input_dim, self.bottleneck_dim).to(device)\n",
        "        \n",
        "        self.residual_vq = ResidualVQ(\n",
        "            dim=self.bottleneck_dim,  # Работаем в 64D!\n",
        "            num_quantizers=num_quantizers,\n",
        "            codebook_size=codebook_size,\n",
        "            decay=decay,\n",
        "            commitment_weight=commitment_weight\n",
        "        ).to(device)\n",
        "        \n",
        "        self.project_out = nn.Linear(self.bottleneck_dim, input_dim).to(device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: [B, N, input_dim]\n",
        "        x_proj = self.project_in(x)  # [B, N, 64]\n",
        "        quantized, indices, loss = self.residual_vq(x_proj)\n",
        "        x_out = self.project_out(quantized)  # [B, N, input_dim]\n",
        "        return x_out, indices, loss\n",
        "    \n",
        "    def parameters(self):\n",
        "        return list(self.project_in.parameters()) + list(self.residual_vq.parameters()) + list(self.project_out.parameters())\n",
        "\n",
        "\n",
        "class LFQWithProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    LFQ требует низкоразмерного пространства (как в доке FashionMNIST).\n",
        "    Project: 2048 → log2(512)=9D → 2048\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, codebook_size, entropy_loss_weight, diversity_gamma, spherical, device):\n",
        "        super().__init__()\n",
        "        import math\n",
        "        # Как в доке: quantize_dim = log2(codebook_size)\n",
        "        self.quantize_dim = int(math.log2(codebook_size))\n",
        "        \n",
        "        # Project к низкой размерности + нормализация\n",
        "        self.project_in = nn.Sequential(\n",
        "            nn.Linear(input_dim, self.quantize_dim),\n",
        "            nn.LayerNorm(self.quantize_dim)\n",
        "        ).to(device)\n",
        "        \n",
        "        # LFQ в низкой размерности\n",
        "        self.lfq = LFQ(\n",
        "            dim=self.quantize_dim,\n",
        "            codebook_size=codebook_size,\n",
        "            entropy_loss_weight=entropy_loss_weight,\n",
        "            diversity_gamma=diversity_gamma,\n",
        "            spherical=spherical\n",
        "        ).to(device)\n",
        "        \n",
        "        # Project обратно\n",
        "        self.project_out = nn.Linear(self.quantize_dim, input_dim).to(device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: [B, N, input_dim]\n",
        "        x_proj = self.project_in(x)  # [B, N, quantize_dim]\n",
        "        quantized, indices, entropy_loss = self.lfq(x_proj)\n",
        "        x_out = self.project_out(quantized)  # [B, N, input_dim]\n",
        "        return x_out, indices, entropy_loss\n",
        "    \n",
        "    def parameters(self):\n",
        "        return list(self.project_in.parameters()) + list(self.lfq.parameters()) + list(self.project_out.parameters())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Loading Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_from_checkpoint(model_path, device):\n",
        "    \"\"\"\n",
        "    Load a complete model from saved checkpoint\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to .pth file\n",
        "        device: torch.device\n",
        "    \n",
        "    Returns:\n",
        "        encoder, decoder, quantizer_wrapper, model_info\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    \n",
        "    # Create base DeepLabV3 model\n",
        "    weights = DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
        "    seg_model = deeplabv3_resnet50(weights=weights).to(device)\n",
        "    \n",
        "    # Load backbone and classifier state dicts\n",
        "    if 'backbone_state_dict' in checkpoint:\n",
        "        seg_model.backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
        "    if 'classifier_state_dict' in checkpoint:\n",
        "        seg_model.classifier.load_state_dict(checkpoint['classifier_state_dict'])\n",
        "    \n",
        "    # Create encoder and decoder\n",
        "    encoder = SegmentationEncoder(\n",
        "        seg_model.backbone, \n",
        "        device, \n",
        "        feature_dim=2048, \n",
        "        add_adapter=checkpoint.get('has_encoder_adapter', True)\n",
        "    )\n",
        "    \n",
        "    decoder = SegmentationDecoder(\n",
        "        seg_model.classifier, \n",
        "        device, \n",
        "        feature_dim=2048, \n",
        "        add_adapter=checkpoint.get('has_decoder_adapter', True)\n",
        "    )\n",
        "    \n",
        "    # Load adapter weights if available\n",
        "    if checkpoint.get('encoder_adapter_state_dict') is not None and encoder.adapter is not None:\n",
        "        encoder.adapter.load_state_dict(checkpoint['encoder_adapter_state_dict'])\n",
        "        print(\"✓ Loaded encoder adapter weights\")\n",
        "    \n",
        "    if checkpoint.get('decoder_adapter_state_dict') is not None and decoder.adapter is not None:\n",
        "        decoder.adapter.load_state_dict(checkpoint['decoder_adapter_state_dict'])\n",
        "        print(\"✓ Loaded decoder adapter weights\")\n",
        "    \n",
        "    # Create quantizer based on method\n",
        "    method = checkpoint.get('method', 'Unknown')\n",
        "    quantizer_wrapper = None\n",
        "    \n",
        "    if method == 'VQ-EMA':\n",
        "        quantizer = VectorQuantize(\n",
        "            dim=2048,\n",
        "            codebook_size=512,\n",
        "            decay=0.8,\n",
        "            commitment_weight=1.0,\n",
        "            threshold_ema_dead_code=2\n",
        "        ).to(device)\n",
        "        \n",
        "        if checkpoint.get('quantizer_state_dict') is not None:\n",
        "            quantizer.load_state_dict(checkpoint['quantizer_state_dict'])\n",
        "        \n",
        "        quantizer_wrapper = QuantizerWrapper(quantizer, \"VQ-EMA\")\n",
        "        \n",
        "    elif method == 'FSQ':\n",
        "        quantizer = FSQWithProjection(\n",
        "            input_dim=2048,\n",
        "            levels=[8, 5, 5, 5],\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        if checkpoint.get('quantizer_state_dict') is not None:\n",
        "            quantizer.load_state_dict(checkpoint['quantizer_state_dict'])\n",
        "        \n",
        "        quantizer_wrapper = QuantizerWrapper(quantizer, \"FSQ\")\n",
        "        \n",
        "    elif method == 'LFQ':\n",
        "        quantizer = LFQWithProjection(\n",
        "            input_dim=2048,\n",
        "            codebook_size=512,\n",
        "            entropy_loss_weight=0.02,\n",
        "            diversity_gamma=1.0,\n",
        "            spherical=True,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        if checkpoint.get('quantizer_state_dict') is not None:\n",
        "            quantizer.load_state_dict(checkpoint['quantizer_state_dict'])\n",
        "        \n",
        "        quantizer_wrapper = QuantizerWrapper(quantizer, \"LFQ\")\n",
        "        \n",
        "    elif method == 'ResidualVQ':\n",
        "        # Detect whether the saved quantizer used projection layers\n",
        "        q_sd = checkpoint.get('quantizer_state_dict') or {}\n",
        "        q_keys = list(q_sd.keys())\n",
        "        uses_projection = any(k.startswith('project_in') or k.startswith('residual_vq.') for k in q_keys)\n",
        "        plain_layout = any(k.startswith('layers.') for k in q_keys)\n",
        "\n",
        "        if uses_projection and not plain_layout:\n",
        "            # Saved as ResidualVQWithProjection\n",
        "            quantizer = ResidualVQWithProjection(\n",
        "                input_dim=2048,\n",
        "                num_quantizers=4,\n",
        "                codebook_size=256,\n",
        "                decay=0.8,\n",
        "                commitment_weight=1.0,\n",
        "                device=device\n",
        "            )\n",
        "        else:\n",
        "            # Saved as plain ResidualVQ (no projection)\n",
        "            quantizer = ResidualVQ(\n",
        "                dim=2048,\n",
        "                num_quantizers=4,\n",
        "                codebook_size=256,\n",
        "                decay=0.8,\n",
        "                commitment_weight=1.0\n",
        "            ).to(device)\n",
        "\n",
        "        if q_sd:\n",
        "            missing_ok = not uses_projection  # be strict when shapes match\n",
        "            quantizer.load_state_dict(q_sd, strict=uses_projection)\n",
        "        \n",
        "        quantizer_wrapper = QuantizerWrapper(quantizer, \"ResidualVQ\")\n",
        "    \n",
        "    if quantizer_wrapper is None:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "    \n",
        "    print(f\"✓ Successfully loaded {method} model\")\n",
        "    \n",
        "    return encoder, decoder, quantizer_wrapper, checkpoint\n",
        "\n",
        "\n",
        "def load_original_model(device):\n",
        "    \"\"\"\n",
        "    Load original DeepLabV3 model without quantization\n",
        "    \"\"\"\n",
        "    print(\"Loading original DeepLabV3 model...\")\n",
        "    \n",
        "    weights = DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
        "    seg_model = deeplabv3_resnet50(weights=weights).to(device)\n",
        "    \n",
        "    # Create encoder and decoder WITHOUT adapters for original model\n",
        "    encoder = SegmentationEncoder(seg_model.backbone, device, add_adapter=False)\n",
        "    decoder = SegmentationDecoder(seg_model.classifier, device, add_adapter=False)\n",
        "    \n",
        "    print(\"✓ Loaded original DeepLabV3 model\")\n",
        "    \n",
        "    return encoder, decoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset transforms (same as training)\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_mask = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=transforms.InterpolationMode.NEAREST),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "\n",
        "class PetDatasetWrapper(torch.utils.data.Dataset):\n",
        "    def __init__(self, pet_dataset, transform_image, transform_mask):\n",
        "        self.dataset = pet_dataset\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_mask = transform_mask\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = self.dataset[idx]\n",
        "        return self.transform_image(image), self.transform_mask(mask)\n",
        "\n",
        "# Load Oxford-IIIT-Pet dataset (robust download/validation)\n",
        "pet_root = DATA_DIR\n",
        "pet_dir = os.path.join(pet_root, 'oxford-iiit-pet')\n",
        "images_dir = os.path.join(pet_dir, 'images')\n",
        "trimaps_dir = os.path.join(pet_dir, 'annotations', 'trimaps')\n",
        "\n",
        "need_download = not (os.path.isdir(images_dir) and os.path.isdir(trimaps_dir))\n",
        "if need_download:\n",
        "    print(\"Dataset or annotations not found. Downloading...\")\n",
        "\n",
        "pet_dataset = OxfordIIITPet(\n",
        "    root=pet_root,\n",
        "    split='trainval',\n",
        "    target_types='segmentation',\n",
        "    download=need_download\n",
        ")\n",
        "\n",
        "# Validate sample access; if it fails, force re-download\n",
        "try:\n",
        "    _ = pet_dataset[0]\n",
        "except FileNotFoundError:\n",
        "    print(\"Detected missing files in dataset. Re-downloading annotations/images...\")\n",
        "    pet_dataset = OxfordIIITPet(\n",
        "        root=pet_root,\n",
        "        split='trainval',\n",
        "        target_types='segmentation',\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "wrapped = PetDatasetWrapper(pet_dataset, transform_image, transform_mask)\n",
        "\n",
        "# Use full training part (all images)\n",
        "test_subset = wrapped\n",
        "test_loader = DataLoader(test_subset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Train dataset ready: {len(test_subset)} images, {len(test_loader)} batches\")\n",
        "print(f\"Batch size: {test_loader.batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extensible Metric Framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Helper Functions for Advanced Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cca_svd(X, Y, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute CCA using SVD (numerically stable).\n",
        "    \n",
        "    Args:\n",
        "        X: [N, D1] features\n",
        "        Y: [N, D2] features\n",
        "        epsilon: regularization for numerical stability\n",
        "    \n",
        "    Returns:\n",
        "        canonical_correlations: torch.Tensor of correlations\n",
        "        U, V: canonical variables\n",
        "    \"\"\"\n",
        "    # Center\n",
        "    X = X - X.mean(dim=0, keepdim=True)\n",
        "    Y = Y - Y.mean(dim=0, keepdim=True)\n",
        "    \n",
        "    # Covariance matrices\n",
        "    N = X.size(0)\n",
        "    Cxx = (X.t() @ X) / (N - 1) + epsilon * torch.eye(X.size(1), device=X.device)\n",
        "    Cyy = (Y.t() @ Y) / (N - 1) + epsilon * torch.eye(Y.size(1), device=Y.device)\n",
        "    Cxy = (X.t() @ Y) / (N - 1)\n",
        "    \n",
        "    # Cholesky decomposition\n",
        "    try:\n",
        "        Lx = torch.linalg.cholesky(Cxx)\n",
        "        Ly = torch.linalg.cholesky(Cyy)\n",
        "    except:\n",
        "        # Fallback to eigendecomposition if Cholesky fails\n",
        "        Lx = torch.linalg.cholesky(Cxx + epsilon * 10 * torch.eye(Cxx.size(0), device=X.device))\n",
        "        Ly = torch.linalg.cholesky(Cyy + epsilon * 10 * torch.eye(Cyy.size(0), device=Y.device))\n",
        "    \n",
        "    # Solve for whitened cross-covariance\n",
        "    Lx_inv = torch.linalg.inv(Lx)\n",
        "    Ly_inv = torch.linalg.inv(Ly)\n",
        "    T = Lx_inv @ Cxy @ Ly_inv.t()\n",
        "    \n",
        "    # SVD\n",
        "    U, S, Vt = torch.linalg.svd(T, full_matrices=False)\n",
        "    \n",
        "    # Canonical correlations are singular values\n",
        "    return S.clamp(0.0, 1.0), U, Vt.t()\n",
        "\n",
        "\n",
        "def compute_pairwise_distances(X, metric='euclidean', max_samples=2000):\n",
        "    \"\"\"\n",
        "    Compute pairwise distances, optionally sampling for memory efficiency.\n",
        "    \n",
        "    Args:\n",
        "        X: [N, D] features\n",
        "        metric: distance metric\n",
        "        max_samples: if N > max_samples, randomly sample\n",
        "    \n",
        "    Returns:\n",
        "        distances: flattened distance vector\n",
        "    \"\"\"\n",
        "    X_np = X.cpu().numpy()\n",
        "    \n",
        "    if X_np.shape[0] > max_samples:\n",
        "        indices = np.random.choice(X_np.shape[0], max_samples, replace=False)\n",
        "        X_np = X_np[indices]\n",
        "    \n",
        "    # Compute pairwise distances (returns condensed form)\n",
        "    distances = pdist(X_np, metric=metric)\n",
        "    return distances\n",
        "\n",
        "\n",
        "def compute_knn_indices(X, k=10, device=None):\n",
        "    \"\"\"\n",
        "    Compute k-nearest neighbors for each sample.\n",
        "    \n",
        "    Args:\n",
        "        X: [N, D] features (torch tensor)\n",
        "        k: number of neighbors\n",
        "        device: computation device\n",
        "    \n",
        "    Returns:\n",
        "        knn_indices: [N, k] tensor of neighbor indices\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = X.device\n",
        "    \n",
        "    X = X.to(device)\n",
        "    \n",
        "    # Compute pairwise distances using batched approach for memory efficiency\n",
        "    N = X.size(0)\n",
        "    knn_indices = torch.zeros(N, k, dtype=torch.long, device='cpu')\n",
        "    \n",
        "    batch_size = min(500, N)\n",
        "    for i in range(0, N, batch_size):\n",
        "        end_i = min(i + batch_size, N)\n",
        "        X_batch = X[i:end_i]\n",
        "        \n",
        "        # Compute distances from batch to all points\n",
        "        dists = torch.cdist(X_batch, X, p=2)  # [batch_size, N]\n",
        "        \n",
        "        # Get k+1 nearest (including self), then exclude self\n",
        "        _, indices = torch.topk(dists, k + 1, largest=False, dim=1)\n",
        "        \n",
        "        # Remove self (first element is always self with distance 0)\n",
        "        knn_batch = indices[:, 1:k+1].cpu()\n",
        "        knn_indices[i:end_i] = knn_batch\n",
        "    \n",
        "    return knn_indices\n",
        "\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    \"\"\"Compute Jaccard similarity between two sets.\"\"\"\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimilarityMetric:\n",
        "    \"\"\"\n",
        "    Abstract base class for similarity metrics\n",
        "    \"\"\"\n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Compute similarity between two feature representations\n",
        "        \n",
        "        Args:\n",
        "            features_1: First feature tensor [N, D] or [B, C, H, W]\n",
        "            features_2: Second feature tensor [N, D] or [B, C, H, W]\n",
        "        \n",
        "        Returns:\n",
        "            similarity_score: float between 0 and 1\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class CKAMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Centered Kernel Alignment (CKA) metric implementation (linear CKA).\n",
        "    Memory-efficient: pools spatial dims to [N, C] and uses N×N Gram matrices.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device or torch.device('cpu')\n",
        "    \n",
        "    @staticmethod\n",
        "    def _center_gram(K: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Center an N×N Gram matrix.\"\"\"\n",
        "        mean_row = K.mean(dim=0, keepdim=True)\n",
        "        mean_col = K.mean(dim=1, keepdim=True)\n",
        "        mean_total = K.mean()\n",
        "        return K - mean_row - mean_col + mean_total\n",
        "    \n",
        "    @staticmethod\n",
        "    def _gram_linear(X: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute linear kernel Gram matrix K = X X^T (N×N).\"\"\"\n",
        "        return X @ X.t()\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Compute linear CKA similarity between two feature sets.\n",
        "        \n",
        "        Args:\n",
        "            features_1: [N, D] or [N, C, H, W]\n",
        "            features_2: [N, D] or [N, C, H, W]\n",
        "        Returns:\n",
        "            float in [0, 1]\n",
        "        \"\"\"\n",
        "        # Spatial pooling BEFORE GPU transfer (defensive, memory-efficient)\n",
        "        if features_1.dim() == 4:  # [N, C, H, W]\n",
        "            features_1 = features_1.mean(dim=(2, 3))  # [N, C]\n",
        "        if features_2.dim() == 4:\n",
        "            features_2 = features_2.mean(dim=(2, 3))\n",
        "        \n",
        "        X = features_1.to(self.device)\n",
        "        Y = features_2.to(self.device)\n",
        "        \n",
        "        # Center features (per-dimension)\n",
        "        X = X - X.mean(dim=0, keepdim=True)\n",
        "        Y = Y - Y.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        # Gram matrices (N×N), then center in RKHS\n",
        "        K = self._center_gram(self._gram_linear(X))\n",
        "        L = self._center_gram(self._gram_linear(Y))\n",
        "        \n",
        "        # HSIC and normalization\n",
        "        hsic = (K * L).sum()\n",
        "        norm_x = (K * K).sum().clamp_min(1e-12).sqrt()\n",
        "        norm_y = (L * L).sum().clamp_min(1e-12).sqrt()\n",
        "        cka = (hsic / (norm_x * norm_y)).clamp(0.0, 1.0)\n",
        "        return float(cka.item())\n",
        "\n",
        "\n",
        "class CCAMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Canonical Correlation Analysis (CCA) metric - placeholder for future implementation\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        print(\"CCA metric - placeholder implementation\")\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Placeholder CCA implementation\n",
        "        \"\"\"\n",
        "        # TODO: Implement CCA\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "class GeometricScoreMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Geometric Score metric - placeholder for future implementation\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        print(\"Geometric Score metric - placeholder implementation\")\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Placeholder Geometric Score implementation\n",
        "        \"\"\"\n",
        "        # TODO: Implement Geometric Score\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "# Metric registry for easy switching\n",
        "METRIC_REGISTRY = {\n",
        "    'cka': CKAMetric,\n",
        "    'pwcca': PWCCAMetric,\n",
        "    'geometry': GeometryScoreMetric,\n",
        "    'rsa': RSAMetric,\n",
        "    'relational': RelationalKnowledgeLossMetric,\n",
        "    'jaccard_knn': JaccardKNNMetric,\n",
        "    'cca': CCAMetric  # kept for backward compatibility\n",
        "}\n",
        "\n",
        "def create_metric(metric_name, device=None):\n",
        "    \"\"\"\n",
        "    Create a metric instance by name\n",
        "    \n",
        "    Args:\n",
        "        metric_name: metric key from METRIC_REGISTRY\n",
        "        device: torch.device\n",
        "    \n",
        "    Returns:\n",
        "        SimilarityMetric instance\n",
        "    \"\"\"\n",
        "    if metric_name not in METRIC_REGISTRY:\n",
        "        raise ValueError(f\"Unknown metric: {metric_name}. Available: {list(METRIC_REGISTRY.keys())}\")\n",
        "    \n",
        "    return METRIC_REGISTRY[metric_name](device=device)\n",
        "\n",
        "print(\"✓ Metric framework initialized\")\n",
        "print(f\"Available metrics: {list(METRIC_REGISTRY.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DEFENSIVE POOLING UPDATES FOR METRICS\n",
        "# ============================================================================\n",
        "# Update metric classes to pool BEFORE .to(device) for memory safety\n",
        "# This prevents OOM when features are accidentally passed as [N, C, H, W]\n",
        "# ============================================================================\n",
        "\n",
        "# Monkey-patch existing metric compute methods with defensive pooling\n",
        "\n",
        "def _defensive_pwcca_compute(self, features_1, features_2):\n",
        "    \"\"\"PWCCA with defensive pooling before GPU transfer\"\"\"\n",
        "    # Pool BEFORE .to(device)\n",
        "    if features_1.dim() == 4:\n",
        "        features_1 = features_1.mean(dim=(2, 3))\n",
        "    if features_2.dim() == 4:\n",
        "        features_2 = features_2.mean(dim=(2, 3))\n",
        "    \n",
        "    X = features_1.to(self.device)\n",
        "    Y = features_2.to(self.device)\n",
        "    \n",
        "    # Compute CCA\n",
        "    canonical_corrs, U, V = compute_cca_svd(X, Y)\n",
        "    \n",
        "    # Project onto canonical variables\n",
        "    X_proj = X @ U\n",
        "    Y_proj = Y @ V\n",
        "    \n",
        "    # Compute variance explained\n",
        "    var_X = X_proj.var(dim=0)\n",
        "    var_Y = Y_proj.var(dim=0)\n",
        "    \n",
        "    # Weight by minimum variance\n",
        "    weights = torch.minimum(var_X, var_Y)\n",
        "    weights = weights / weights.sum()\n",
        "    \n",
        "    # Weighted average\n",
        "    pwcca = (canonical_corrs * weights).sum()\n",
        "    return float(pwcca.item())\n",
        "\n",
        "def _defensive_rsa_compute(self, features_1, features_2):\n",
        "    \"\"\"RSA with defensive pooling before GPU transfer\"\"\"\n",
        "    # Pool BEFORE .to(device)\n",
        "    if features_1.dim() == 4:\n",
        "        features_1 = features_1.mean(dim=(2, 3))\n",
        "    if features_2.dim() == 4:\n",
        "        features_2 = features_2.mean(dim=(2, 3))\n",
        "    \n",
        "    X = features_1.to(self.device)\n",
        "    Y = features_2.to(self.device)\n",
        "    \n",
        "    # Sample if too large\n",
        "    if X.size(0) > self.max_samples:\n",
        "        indices = torch.randperm(X.size(0))[:self.max_samples]\n",
        "        X = X[indices]\n",
        "        Y = Y[indices]\n",
        "    \n",
        "    # Compute RDMs (1 - cosine similarity)\n",
        "    X_norm = torch.nn.functional.normalize(X, dim=1)\n",
        "    Y_norm = torch.nn.functional.normalize(Y, dim=1)\n",
        "    \n",
        "    rdm_X = 1 - (X_norm @ X_norm.t())\n",
        "    rdm_Y = 1 - (Y_norm @ Y_norm.t())\n",
        "    \n",
        "    # Flatten upper triangular (excluding diagonal)\n",
        "    mask = torch.triu(torch.ones_like(rdm_X), diagonal=1).bool()\n",
        "    rdm_X_flat = rdm_X[mask].cpu().numpy()\n",
        "    rdm_Y_flat = rdm_Y[mask].cpu().numpy()\n",
        "    \n",
        "    # Pearson correlation\n",
        "    corr, _ = pearsonr(rdm_X_flat, rdm_Y_flat)\n",
        "    return float(corr)\n",
        "\n",
        "def _defensive_relational_compute(self, features_1, features_2):\n",
        "    \"\"\"Relational Knowledge Loss with defensive pooling\"\"\"\n",
        "    # Pool BEFORE .to(device)\n",
        "    if features_1.dim() == 4:\n",
        "        features_1 = features_1.mean(dim=(2, 3))\n",
        "    if features_2.dim() == 4:\n",
        "        features_2 = features_2.mean(dim=(2, 3))\n",
        "    \n",
        "    X = features_1.to(self.device)\n",
        "    Y = features_2.to(self.device)\n",
        "    \n",
        "    # Compute k-NN\n",
        "    knn_X = compute_knn_indices(X, k=self.k, device=self.device)\n",
        "    knn_Y = compute_knn_indices(Y, k=self.k, device=self.device)\n",
        "    \n",
        "    # Compute loss\n",
        "    N = X.size(0)\n",
        "    total_lost = 0\n",
        "    for i in range(N):\n",
        "        neighbors_X = set(knn_X[i].tolist())\n",
        "        neighbors_Y = set(knn_Y[i].tolist())\n",
        "        lost = len(neighbors_X - neighbors_Y)\n",
        "        total_lost += lost\n",
        "    \n",
        "    loss = total_lost / (N * self.k)\n",
        "    return float(loss)\n",
        "\n",
        "def _defensive_jaccard_compute(self, features_1, features_2):\n",
        "    \"\"\"Jaccard KNN with defensive pooling\"\"\"\n",
        "    # Pool BEFORE .to(device)\n",
        "    if features_1.dim() == 4:\n",
        "        features_1 = features_1.mean(dim=(2, 3))\n",
        "    if features_2.dim() == 4:\n",
        "        features_2 = features_2.mean(dim=(2, 3))\n",
        "    \n",
        "    X = features_1.to(self.device)\n",
        "    Y = features_2.to(self.device)\n",
        "    \n",
        "    # Compute k-NN\n",
        "    knn_X = compute_knn_indices(X, k=self.k, device=self.device)\n",
        "    knn_Y = compute_knn_indices(Y, k=self.k, device=self.device)\n",
        "    \n",
        "    # Compute Jaccard\n",
        "    N = X.size(0)\n",
        "    jaccard_scores = []\n",
        "    for i in range(N):\n",
        "        neighbors_X = set(knn_X[i].tolist())\n",
        "        neighbors_Y = set(knn_Y[i].tolist())\n",
        "        jaccard_scores.append(jaccard_similarity(neighbors_X, neighbors_Y))\n",
        "    \n",
        "    return float(np.mean(jaccard_scores))\n",
        "\n",
        "# Apply monkey patches\n",
        "PWCCAMetric.compute = _defensive_pwcca_compute\n",
        "RSAMetric.compute = _defensive_rsa_compute\n",
        "RelationalKnowledgeLossMetric.compute = _defensive_relational_compute\n",
        "JaccardKNNMetric.compute = _defensive_jaccard_compute\n",
        "\n",
        "print(\"✓ Applied defensive pooling to all metrics (OOM protection)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PWCCAMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Projection Weighted Canonical Correlation Analysis (PWCCA).\n",
        "    Weights canonical correlations by variance explained.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device or torch.device('cpu')\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Compute PWCCA score.\n",
        "        \n",
        "        Args:\n",
        "            features_1: [N, D] or [N, C, H, W]\n",
        "            features_2: [N, D] or [N, C, H, W]\n",
        "        \n",
        "        Returns:\n",
        "            pwcca_score: float in [0, 1]\n",
        "        \"\"\"\n",
        "        X = features_1.to(self.device)\n",
        "        Y = features_2.to(self.device)\n",
        "        \n",
        "        # Pool spatial dimensions if needed\n",
        "        if X.dim() == 4:\n",
        "            X = X.mean(dim=(2, 3))\n",
        "        if Y.dim() == 4:\n",
        "            Y = Y.mean(dim=(2, 3))\n",
        "        \n",
        "        # Compute CCA\n",
        "        canonical_corrs, U, V = compute_cca_svd(X, Y)\n",
        "        \n",
        "        # Project onto canonical variables\n",
        "        X_proj = X @ U\n",
        "        Y_proj = Y @ V\n",
        "        \n",
        "        # Compute variance explained by each canonical variable\n",
        "        var_X = X_proj.var(dim=0)\n",
        "        var_Y = Y_proj.var(dim=0)\n",
        "        \n",
        "        # Weight by minimum variance (more conservative)\n",
        "        weights = torch.minimum(var_X, var_Y)\n",
        "        weights = weights / weights.sum()  # normalize\n",
        "        \n",
        "        # Weighted average of canonical correlations\n",
        "        pwcca = (canonical_corrs * weights).sum()\n",
        "        \n",
        "        return float(pwcca.item())\n",
        "\n",
        "\n",
        "class GeometryScoreMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Geometry Score: measures preservation of pairwise distances.\n",
        "    Uses Spearman correlation between distance matrices.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None, max_samples=2000):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.max_samples = max_samples\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Compute Geometry Score.\n",
        "        \n",
        "        Args:\n",
        "            features_1: [N, D] or [N, C, H, W]\n",
        "            features_2: [N, D] or [N, C, H, W]\n",
        "        \n",
        "        Returns:\n",
        "            geometry_score: float, Spearman correlation of distances\n",
        "        \"\"\"\n",
        "        X = features_1\n",
        "        Y = features_2\n",
        "        \n",
        "        # Pool spatial dimensions\n",
        "        if X.dim() == 4:\n",
        "            X = X.mean(dim=(2, 3))\n",
        "        if Y.dim() == 4:\n",
        "            Y = Y.mean(dim=(2, 3))\n",
        "        \n",
        "        # Compute pairwise distances\n",
        "        dist_X = compute_pairwise_distances(X, max_samples=self.max_samples)\n",
        "        dist_Y = compute_pairwise_distances(Y, max_samples=self.max_samples)\n",
        "        \n",
        "        # Spearman correlation\n",
        "        corr, _ = spearmanr(dist_X, dist_Y)\n",
        "        \n",
        "        return float(corr) if not np.isnan(corr) else 0.0\n",
        "\n",
        "\n",
        "class RSAMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Representational Similarity Analysis (RSA).\n",
        "    Compares representational dissimilarity matrices using Pearson correlation.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None, max_samples=2000):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.max_samples = max_samples\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Compute RSA score.\n",
        "        \n",
        "        Args:\n",
        "            features_1: [N, D] or [N, C, H, W]\n",
        "            features_2: [N, D] or [N, C, H, W]\n",
        "        \n",
        "        Returns:\n",
        "            rsa_score: float, Pearson correlation of RDMs\n",
        "        \"\"\"\n",
        "        X = features_1.to(self.device)\n",
        "        Y = features_2.to(self.device)\n",
        "        \n",
        "        # Pool spatial dimensions\n",
        "        if X.dim() == 4:\n",
        "            X = X.mean(dim=(2, 3))\n",
        "        if Y.dim() == 4:\n",
        "            Y = Y.mean(dim=(2, 3))\n",
        "        \n",
        "        # Sample if too large\n",
        "        if X.size(0) > self.max_samples:\n",
        "            indices = torch.randperm(X.size(0))[:self.max_samples]\n",
        "            X = X[indices]\n",
        "            Y = Y[indices]\n",
        "        \n",
        "        # Normalize features\n",
        "        X = F.normalize(X, p=2, dim=1)\n",
        "        Y = F.normalize(Y, p=2, dim=1)\n",
        "        \n",
        "        # Compute RDM (1 - cosine similarity)\n",
        "        rdm_X = 1 - (X @ X.t()).cpu().numpy()\n",
        "        rdm_Y = 1 - (Y @ Y.t()).cpu().numpy()\n",
        "        \n",
        "        # Extract upper triangular (excluding diagonal)\n",
        "        mask = np.triu(np.ones_like(rdm_X), k=1).astype(bool)\n",
        "        rdm_X_vec = rdm_X[mask]\n",
        "        rdm_Y_vec = rdm_Y[mask]\n",
        "        \n",
        "        # Pearson correlation\n",
        "        corr, _ = pearsonr(rdm_X_vec, rdm_Y_vec)\n",
        "        \n",
        "        return float(corr) if not np.isnan(corr) else 0.0\n",
        "\n",
        "\n",
        "class RelationalKnowledgeLossMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Relational Knowledge Loss: measures how well k-NN relationships are preserved.\n",
        "    Returns a LOSS value (lower is better).\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None, k=10):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.k = k\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Compute relational knowledge loss.\n",
        "        \n",
        "        Args:\n",
        "            features_1: [N, D] or [N, C, H, W]\n",
        "            features_2: [N, D] or [N, C, H, W]\n",
        "        \n",
        "        Returns:\n",
        "            loss: float in [0, 1], lower is better\n",
        "        \"\"\"\n",
        "        X = features_1.to(self.device)\n",
        "        Y = features_2.to(self.device)\n",
        "        \n",
        "        # Pool spatial dimensions\n",
        "        if X.dim() == 4:\n",
        "            X = X.mean(dim=(2, 3))\n",
        "        if Y.dim() == 4:\n",
        "            Y = Y.mean(dim=(2, 3))\n",
        "        \n",
        "        # Compute k-NN for both\n",
        "        knn_X = compute_knn_indices(X, k=self.k, device=self.device)\n",
        "        knn_Y = compute_knn_indices(Y, k=self.k, device=self.device)\n",
        "        \n",
        "        # For each sample, check how many of its k-NN in X are also in top-k in Y\n",
        "        N = X.size(0)\n",
        "        overlaps = []\n",
        "        \n",
        "        for i in range(N):\n",
        "            neighbors_X = set(knn_X[i].tolist())\n",
        "            neighbors_Y = set(knn_Y[i].tolist())\n",
        "            overlap = len(neighbors_X.intersection(neighbors_Y))\n",
        "            overlaps.append(overlap / self.k)\n",
        "        \n",
        "        # Average overlap\n",
        "        avg_overlap = np.mean(overlaps)\n",
        "        \n",
        "        # Return as loss (1 - overlap)\n",
        "        loss = 1.0 - avg_overlap\n",
        "        \n",
        "        return float(loss)\n",
        "\n",
        "\n",
        "class JaccardKNNMetric(SimilarityMetric):\n",
        "    \"\"\"\n",
        "    Jaccard KNN: Jaccard similarity of k-nearest neighbor sets.\n",
        "    \"\"\"\n",
        "    def __init__(self, device=None, k=10):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.k = k\n",
        "    \n",
        "    def compute(self, features_1, features_2):\n",
        "        \"\"\"\n",
        "        Compute average Jaccard similarity of k-NN sets.\n",
        "        \n",
        "        Args:\n",
        "            features_1: [N, D] or [N, C, H, W]\n",
        "            features_2: [N, D] or [N, C, H, W]\n",
        "        \n",
        "        Returns:\n",
        "            jaccard_score: float in [0, 1]\n",
        "        \"\"\"\n",
        "        X = features_1.to(self.device)\n",
        "        Y = features_2.to(self.device)\n",
        "        \n",
        "        # Pool spatial dimensions\n",
        "        if X.dim() == 4:\n",
        "            X = X.mean(dim=(2, 3))\n",
        "        if Y.dim() == 4:\n",
        "            Y = Y.mean(dim=(2, 3))\n",
        "        \n",
        "        # Compute k-NN for both\n",
        "        knn_X = compute_knn_indices(X, k=self.k, device=self.device)\n",
        "        knn_Y = compute_knn_indices(Y, k=self.k, device=self.device)\n",
        "        \n",
        "        # Compute Jaccard for each sample\n",
        "        N = X.size(0)\n",
        "        jaccard_scores = []\n",
        "        \n",
        "        for i in range(N):\n",
        "            neighbors_X = set(knn_X[i].tolist())\n",
        "            neighbors_Y = set(knn_Y[i].tolist())\n",
        "            jaccard = jaccard_similarity(neighbors_X, neighbors_Y)\n",
        "            jaccard_scores.append(jaccard)\n",
        "        \n",
        "        # Average Jaccard\n",
        "        avg_jaccard = np.mean(jaccard_scores)\n",
        "        \n",
        "        return float(avg_jaccard)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Extraction Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _extract_pooled_backbone_features(encoder, images, device):\n",
        "    with torch.no_grad():\n",
        "        feats = encoder.backbone(images)['out']  # [B, C, H, W]\n",
        "        pooled = feats.mean(dim=(2, 3))          # [B, C]\n",
        "    return pooled\n",
        "\n",
        "\n",
        "def _extract_pooled_quantized_features(encoder, quantizer_wrapper, decoder, images, device):\n",
        "    with torch.no_grad():\n",
        "        encoded = encoder.encode(images)                         # [B, C, H, W]\n",
        "        quantized, _ = quantizer_wrapper.quantize_spatial(encoded)\n",
        "        if decoder.adapter is not None:\n",
        "            quantized = quantized + decoder.adapter(quantized)\n",
        "        pooled = quantized.mean(dim=(2, 3))                     # [B, C]\n",
        "    return pooled\n",
        "\n",
        "\n",
        "def _compute_mean_over_loader(extractor_fn, loader, device, print_every=20):\n",
        "    mean = None\n",
        "    count = 0\n",
        "    for batch_idx, (images, _) in enumerate(loader):\n",
        "        if batch_idx % print_every == 0:\n",
        "            print(f\"  [Pass 1] Batch {batch_idx}/{len(loader)}\")\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        X = extractor_fn(images)\n",
        "        if mean is None:\n",
        "            mean = X.sum(dim=0)\n",
        "        else:\n",
        "            mean += X.sum(dim=0)\n",
        "        count += X.size(0)\n",
        "    mean = mean / max(count, 1)\n",
        "    return mean, count\n",
        "\n",
        "\n",
        "def compute_linear_cka_streaming(original_encoder, quantized_encoder, quantizer_wrapper, decoder, loader, device, print_every=20):\n",
        "    \"\"\"\n",
        "    Two-pass streaming linear CKA using covariance matrices; avoids storing all features.\n",
        "    \"\"\"\n",
        "    original_encoder.backbone.eval()\n",
        "    quantized_encoder.backbone.eval()\n",
        "    decoder.classifier.eval()\n",
        "\n",
        "    # Define extractors bound with correct modules\n",
        "    def ext_orig(imgs):\n",
        "        return _extract_pooled_backbone_features(original_encoder, imgs, device)\n",
        "    def ext_quant(imgs):\n",
        "        return _extract_pooled_quantized_features(quantized_encoder, quantizer_wrapper, decoder, imgs, device)\n",
        "\n",
        "    # Pass 1: means\n",
        "    print(\"  Computing means (original)...\")\n",
        "    mean_X, N = _compute_mean_over_loader(ext_orig, loader, device, print_every)\n",
        "    print(\"  Computing means (quantized)...\")\n",
        "    mean_Y, N2 = _compute_mean_over_loader(ext_quant, loader, device, print_every)\n",
        "\n",
        "    # Safety if dataset is empty\n",
        "    if N == 0 or N2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Pass 2: covariance accumulators\n",
        "    print(\"  Accumulating covariances...\")\n",
        "    Cxx = None\n",
        "    Cyy = None\n",
        "    Cxy = None\n",
        "\n",
        "    for batch_idx, (images, _) in enumerate(loader):\n",
        "        if batch_idx % print_every == 0:\n",
        "            print(f\"  [Pass 2] Batch {batch_idx}/{len(loader)}\")\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        X = ext_orig(images) - mean_X\n",
        "        Y = ext_quant(images) - mean_Y\n",
        "        # [B, C] -> [C, C]\n",
        "        Cxx_b = X.t() @ X\n",
        "        Cyy_b = Y.t() @ Y\n",
        "        Cxy_b = X.t() @ Y\n",
        "        if Cxx is None:\n",
        "            Cxx, Cyy, Cxy = Cxx_b, Cyy_b, Cxy_b\n",
        "        else:\n",
        "            Cxx += Cxx_b\n",
        "            Cyy += Cyy_b\n",
        "            Cxy += Cxy_b\n",
        "\n",
        "    # Linear CKA from covariances\n",
        "    num = torch.norm(Cxy, p='fro') ** 2\n",
        "    den = torch.norm(Cxx, p='fro') * torch.norm(Cyy, p='fro')\n",
        "    cka = (num / den.clamp_min(1e-12)).clamp(0.0, 1.0)\n",
        "    return float(cka.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_original_features(encoder, data_loader, device):\n",
        "    \"\"\"\n",
        "    Extract features from original backbone (no quantization, no adapters)\n",
        "    \n",
        "    Args:\n",
        "        encoder: SegmentationEncoder WITHOUT adapters\n",
        "        data_loader: DataLoader\n",
        "        device: torch.device\n",
        "    \n",
        "    Returns:\n",
        "        features: torch.Tensor [N, C, H, W] - original backbone features\n",
        "    \"\"\"\n",
        "    print(\"Extracting original backbone features...\")\n",
        "    \n",
        "    all_features = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, _) in enumerate(data_loader):\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"  Processing batch {batch_idx}/{len(data_loader)}\")\n",
        "            \n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Extract features directly from backbone (no adapters)\n",
        "            features = encoder.backbone(images)['out']\n",
        "            all_features.append(features.cpu())\n",
        "    \n",
        "    # Concatenate all features\n",
        "    features = torch.cat(all_features, dim=0)\n",
        "    print(f\"✓ Extracted original features: {features.shape}\")\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_quantized_features(encoder, quantizer_wrapper, decoder, data_loader, device):\n",
        "    \"\"\"\n",
        "    Extract features from quantized model (encoder → quantizer → decoder with adapters)\n",
        "    \n",
        "    Args:\n",
        "        encoder: SegmentationEncoder WITH adapters\n",
        "        quantizer_wrapper: QuantizerWrapper\n",
        "        decoder: SegmentationDecoder WITH adapters\n",
        "        data_loader: DataLoader\n",
        "        device: torch.device\n",
        "    \n",
        "    Returns:\n",
        "        features: torch.Tensor [N, C, H, W] - quantized features\n",
        "    \"\"\"\n",
        "    print(f\"Extracting quantized features from {quantizer_wrapper.name}...\")\n",
        "    \n",
        "    all_features = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, _) in enumerate(data_loader):\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"  Processing batch {batch_idx}/{len(data_loader)}\")\n",
        "            \n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Extract features with encoder (with adapters)\n",
        "            encoded_features = encoder.encode(images)\n",
        "            \n",
        "            # Quantize features\n",
        "            quantized_features, _ = quantizer_wrapper.quantize_spatial(encoded_features)\n",
        "            \n",
        "            # Apply decoder adapter if available\n",
        "            if decoder.adapter is not None:\n",
        "                final_features = quantized_features + decoder.adapter(quantized_features)\n",
        "            else:\n",
        "                final_features = quantized_features\n",
        "            \n",
        "            all_features.append(final_features.cpu())\n",
        "    \n",
        "    # Concatenate all features\n",
        "    features = torch.cat(all_features, dim=0)\n",
        "    print(f\"✓ Extracted quantized features: {features.shape}\")\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_features_for_comparison(original_encoder, quantized_encoder, quantizer_wrapper, \n",
        "                                 quantized_decoder, data_loader, device):\n",
        "    \"\"\"\n",
        "    Extract both original and quantized features for comparison\n",
        "    \n",
        "    Args:\n",
        "        original_encoder: SegmentationEncoder WITHOUT adapters\n",
        "        quantized_encoder: SegmentationEncoder WITH adapters\n",
        "        quantizer_wrapper: QuantizerWrapper\n",
        "        quantized_decoder: SegmentationDecoder WITH adapters\n",
        "        data_loader: DataLoader\n",
        "        device: torch.device\n",
        "    \n",
        "    Returns:\n",
        "        original_features, quantized_features: torch.Tensor\n",
        "    \"\"\"\n",
        "    print(\"Extracting features for comparison...\")\n",
        "    \n",
        "    original_features = extract_original_features(original_encoder, data_loader, device)\n",
        "    quantized_features = extract_quantized_features(\n",
        "        quantized_encoder, quantizer_wrapper, quantized_decoder, data_loader, device\n",
        "    )\n",
        "    \n",
        "    return original_features, quantized_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MEMORY-EFFICIENT POOLED FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "# These functions perform spatial pooling DURING extraction (not after),\n",
        "# reducing memory from [N, C, H, W] (30 GB) to [N, C] (30 MB).\n",
        "# All metrics already perform spatial pooling, so this doesn't change results.\n",
        "# ============================================================================\n",
        "\n",
        "def extract_original_features_pooled(encoder, data_loader, device):\n",
        "    \"\"\"\n",
        "    Extract spatially-pooled features from original backbone (memory-efficient version).\n",
        "    \n",
        "    Performs spatial pooling on GPU before transferring to CPU, saving 1000× memory.\n",
        "    \n",
        "    Args:\n",
        "        encoder: SegmentationEncoder WITHOUT adapters\n",
        "        data_loader: DataLoader\n",
        "        device: torch.device\n",
        "    \n",
        "    Returns:\n",
        "        features: torch.Tensor [N, C] - spatially pooled backbone features\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(data_loader, desc=\"Extracting original (pooled)\", leave=False):\n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Extract features from backbone\n",
        "            features = encoder.backbone(images)['out']  # [B, C, H, W]\n",
        "            \n",
        "            # ★ SPATIAL POOLING ON GPU (before CPU transfer)\n",
        "            features_pooled = features.mean(dim=(2, 3))  # [B, C]\n",
        "            \n",
        "            all_features.append(features_pooled.cpu())\n",
        "    \n",
        "    # Concatenate all features\n",
        "    features = torch.cat(all_features, dim=0)  # [N, C]\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def extract_quantized_features_pooled(encoder, quantizer_wrapper, decoder, data_loader, device):\n",
        "    \"\"\"\n",
        "    Extract spatially-pooled quantized features (memory-efficient version).\n",
        "    \n",
        "    Performs spatial pooling on GPU before transferring to CPU, saving 1000× memory.\n",
        "    \n",
        "    Args:\n",
        "        encoder: SegmentationEncoder WITH adapters\n",
        "        quantizer_wrapper: QuantizerWrapper\n",
        "        decoder: SegmentationDecoder WITH adapters\n",
        "        data_loader: DataLoader\n",
        "        device: torch.device\n",
        "    \n",
        "    Returns:\n",
        "        features: torch.Tensor [N, C] - spatially pooled quantized features\n",
        "    \"\"\"\n",
        "    all_features = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(data_loader, desc=f\"Extracting {quantizer_wrapper.name} (pooled)\", leave=False):\n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Extract features with encoder (with adapters)\n",
        "            encoded_features = encoder.encode(images)\n",
        "            \n",
        "            # Quantize features\n",
        "            quantized_features, _ = quantizer_wrapper.quantize_spatial(encoded_features)\n",
        "            \n",
        "            # Apply decoder adapter if available\n",
        "            if decoder.adapter is not None:\n",
        "                final_features = quantized_features + decoder.adapter(quantized_features)\n",
        "            else:\n",
        "                final_features = quantized_features\n",
        "            \n",
        "            # ★ SPATIAL POOLING ON GPU (before CPU transfer)\n",
        "            final_features_pooled = final_features.mean(dim=(2, 3))  # [B, C]\n",
        "            \n",
        "            all_features.append(final_features_pooled.cpu())\n",
        "    \n",
        "    # Concatenate all features\n",
        "    features = torch.cat(all_features, dim=0)  # [N, C]\n",
        "    \n",
        "    return features\n",
        "\n",
        "print(\"✓ Memory-efficient pooled extraction functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPROVED STREAMING CKA WITH TQDM PROGRESS BARS\n",
        "# ============================================================================\n",
        "\n",
        "def _compute_mean_over_loader_tqdm(extractor_fn, loader, device, desc=\"Computing means\"):\n",
        "    \"\"\"Compute mean features with tqdm progress bar\"\"\"\n",
        "    mean = None\n",
        "    count = 0\n",
        "    for images, _ in tqdm(loader, desc=f\"    {desc}\", leave=False):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        X = extractor_fn(images)\n",
        "        if mean is None:\n",
        "            mean = X.sum(dim=0)\n",
        "        else:\n",
        "            mean += X.sum(dim=0)\n",
        "        count += X.size(0)\n",
        "    mean = mean / max(count, 1)\n",
        "    return mean, count\n",
        "\n",
        "def compute_linear_cka_streaming_tqdm(original_encoder, quantized_encoder, quantizer_wrapper, decoder, loader, device):\n",
        "    \"\"\"\n",
        "    Two-pass streaming linear CKA using covariance matrices with tqdm progress bars\n",
        "    \"\"\"\n",
        "    original_encoder.backbone.eval()\n",
        "    quantized_encoder.backbone.eval()\n",
        "    decoder.classifier.eval()\n",
        "\n",
        "    # Define extractors\n",
        "    def ext_orig(imgs):\n",
        "        return _extract_pooled_backbone_features(original_encoder, imgs, device)\n",
        "    def ext_quant(imgs):\n",
        "        return _extract_pooled_quantized_features(quantized_encoder, quantizer_wrapper, decoder, imgs, device)\n",
        "\n",
        "    # Pass 1: means\n",
        "    mean_X, N = _compute_mean_over_loader_tqdm(ext_orig, loader, device, \"Pass 1: Original\")\n",
        "    mean_Y, N2 = _compute_mean_over_loader_tqdm(ext_quant, loader, device, \"Pass 1: Quantized\")\n",
        "\n",
        "    if N == 0 or N2 == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Pass 2: covariances\n",
        "    Cxx = None\n",
        "    Cyy = None\n",
        "    Cxy = None\n",
        "\n",
        "    for images, _ in tqdm(loader, desc=\"    Pass 2: Covariances\", leave=False):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        X = ext_orig(images) - mean_X\n",
        "        Y = ext_quant(images) - mean_Y\n",
        "        \n",
        "        Cxx_b = X.t() @ X\n",
        "        Cyy_b = Y.t() @ Y\n",
        "        Cxy_b = X.t() @ Y\n",
        "        \n",
        "        if Cxx is None:\n",
        "            Cxx = Cxx_b\n",
        "            Cyy = Cyy_b\n",
        "            Cxy = Cxy_b\n",
        "        else:\n",
        "            Cxx += Cxx_b\n",
        "            Cyy += Cyy_b\n",
        "            Cxy += Cxy_b\n",
        "\n",
        "    Cxx = Cxx / max(N - 1, 1)\n",
        "    Cyy = Cyy / max(N2 - 1, 1)\n",
        "    Cxy = Cxy / max(N - 1, 1)\n",
        "\n",
        "    # HSIC computation\n",
        "    hsic = torch.trace(Cxx @ Cyy.t()) + torch.trace(Cxx) * torch.trace(Cyy) - 2 * torch.trace(Cxy)\n",
        "    norm_x = torch.sqrt(torch.trace(Cxx @ Cxx) + torch.trace(Cxx)**2)\n",
        "    norm_y = torch.sqrt(torch.trace(Cyy @ Cyy) + torch.trace(Cyy)**2)\n",
        "    \n",
        "    cka = hsic / (norm_x * norm_y + 1e-12)\n",
        "    return float(cka.clamp(0.0, 1.0).item())\n",
        "\n",
        "# Update the function pointer\n",
        "compute_linear_cka_streaming = compute_linear_cka_streaming_tqdm\n",
        "\n",
        "print(\"✓ Streaming CKA updated with tqdm progress bars\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_multiple_metrics(models_dir, data_loader, device, metric_names):\n",
        "    \"\"\"\n",
        "    Evaluate all quantized models with multiple metrics efficiently.\n",
        "    \n",
        "    Extracts pooled features ONCE per model and reuses them across ALL metrics.\n",
        "    No redundant feature extraction - significant speedup!\n",
        "    \n",
        "    Args:\n",
        "        models_dir: Path to models directory\n",
        "        data_loader: DataLoader for test data\n",
        "        device: torch.device\n",
        "        metric_names: list of metric names to compute\n",
        "    \n",
        "    Returns:\n",
        "        results: dict[model_name -> dict[metric_name -> score]]\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"MULTI-METRIC EVALUATION ({len(metric_names)} metrics)\")\n",
        "    print(f\"Metrics: {', '.join(metric_names)}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Load original model\n",
        "    original_encoder, original_decoder = load_original_model(device)\n",
        "    \n",
        "    # Extract original features ONCE for ALL metrics (pooled, memory-efficient)\n",
        "    print(\"\\n📊 Extracting original features (pooled, memory-efficient)...\")\n",
        "    original_features = extract_original_features_pooled(original_encoder, data_loader, device)\n",
        "    print(f\"✓ Pooled features: {original_features.shape} (~{original_features.numel() * 4 / 1024**2:.1f} MB)\")\n",
        "    \n",
        "    # Find all quantized model files\n",
        "    model_files = {\n",
        "        'VQ-EMA': os.path.join(models_dir, 'vq_ema_full_model.pth'),\n",
        "        'FSQ': os.path.join(models_dir, 'fsq_full_model.pth'),\n",
        "        'LFQ': os.path.join(models_dir, 'lfq_full_model.pth'),\n",
        "        'ResidualVQ': os.path.join(models_dir, 'residualvq_full_model.pth')\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Evaluate each model with all metrics\n",
        "    models_pbar = tqdm(model_files.items(), desc=\"📦 Models\", leave=True)\n",
        "    \n",
        "    for method_name, model_path in models_pbar:\n",
        "        if not os.path.exists(model_path):\n",
        "            models_pbar.write(f\"⚠ Model not found: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        models_pbar.set_postfix_str(f\"{method_name}\")\n",
        "        \n",
        "        # Load quantized model\n",
        "        encoder, decoder, quantizer_wrapper, checkpoint = load_model_from_checkpoint(model_path, device)\n",
        "        \n",
        "        # Extract quantized features ONCE for ALL metrics (pooled, memory-efficient)\n",
        "        quantized_features = extract_quantized_features_pooled(\n",
        "            encoder, quantizer_wrapper, decoder, data_loader, device\n",
        "        )\n",
        "        \n",
        "        # Compute each metric\n",
        "        model_results = {}\n",
        "        \n",
        "        metrics_pbar = tqdm(metric_names, desc=f\"  📊 {method_name} metrics\", leave=False)\n",
        "        \n",
        "        for metric_name in metrics_pbar:\n",
        "            metrics_pbar.set_postfix_str(f\"{metric_name}\")\n",
        "            \n",
        "            # All metrics now use pre-extracted pooled features (no redundant extraction!)\n",
        "            metric = create_metric(metric_name, device=device)\n",
        "            score = metric.compute(original_features, quantized_features)\n",
        "            \n",
        "            model_results[metric_name] = score\n",
        "            models_pbar.write(f\"  ✓ {method_name} - {metric_name}: {score:.4f}\")\n",
        "        \n",
        "        results[method_name] = {\n",
        "            'scores': model_results,\n",
        "            'checkpoint_info': {\n",
        "                'has_encoder_adapter': checkpoint.get('has_encoder_adapter', False),\n",
        "                'has_decoder_adapter': checkpoint.get('has_decoder_adapter', False)\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MULTI-METRIC EVALUATION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_all_models(models_dir, data_loader, device, metric_name='cka'):\n",
        "    \"\"\"\n",
        "    Evaluate all quantized models and compare with original\n",
        "    \n",
        "    Args:\n",
        "        models_dir: Path to models directory\n",
        "        data_loader: DataLoader for test data\n",
        "        device: torch.device\n",
        "        metric_name: Name of metric to use ('cka', 'cca', 'geometric')\n",
        "    \n",
        "    Returns:\n",
        "        results: dict with scores for each method\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(f\"EVALUATION WITH {metric_name.upper()} METRIC\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Create metric\n",
        "    metric = create_metric(metric_name, device=device)\n",
        "    \n",
        "    # Load original model\n",
        "    original_encoder, original_decoder = load_original_model(device)\n",
        "    \n",
        "    # Extract original features once\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    original_features = extract_original_features(original_encoder, data_loader, device)\n",
        "    \n",
        "    # Find all quantized model files\n",
        "    model_files = {\n",
        "        'VQ-EMA': os.path.join(models_dir, 'vq_ema_full_model.pth'),\n",
        "        'FSQ': os.path.join(models_dir, 'fsq_full_model.pth'),\n",
        "        'LFQ': os.path.join(models_dir, 'lfq_full_model.pth'),\n",
        "        'ResidualVQ': os.path.join(models_dir, 'residualvq_full_model.pth')\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Evaluate each quantized model\n",
        "    for method_name, model_path in model_files.items():\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"\\n⚠ Model not found: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"Evaluating {method_name}...\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        # Load quantized model\n",
        "        encoder, decoder, quantizer_wrapper, checkpoint = load_model_from_checkpoint(model_path, device)\n",
        "        \n",
        "        # Extract quantized features\n",
        "        quantized_features = extract_quantized_features(\n",
        "            encoder, quantizer_wrapper, decoder, data_loader, device\n",
        "        )\n",
        "        \n",
        "        # Compute similarity metric\n",
        "        print(f\"\\nComputing {metric_name.upper()} similarity...\")\n",
        "        score = metric.compute(original_features, quantized_features)\n",
        "        \n",
        "        results[method_name] = {\n",
        "            'score': score,\n",
        "            'method': method_name,\n",
        "            'checkpoint_info': {\n",
        "                'has_encoder_adapter': checkpoint.get('has_encoder_adapter', False),\n",
        "                'has_decoder_adapter': checkpoint.get('has_decoder_adapter', False)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        print(f\"✓ {method_name} {metric_name.upper()} score: {score:.4f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EVALUATION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_multi_metric_results(results, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize multi-metric evaluation results with heatmap and radar chart.\n",
        "    \n",
        "    Args:\n",
        "        results: dict from evaluate_with_multiple_metrics\n",
        "        save_path: Optional path to save figure\n",
        "    \"\"\"\n",
        "    # Extract data\n",
        "    models = list(results.keys())\n",
        "    all_metrics = list(results[models[0]]['scores'].keys())\n",
        "    \n",
        "    # Create data matrix: models × metrics\n",
        "    data = []\n",
        "    for model in models:\n",
        "        scores = [results[model]['scores'][m] for m in all_metrics]\n",
        "        data.append(scores)\n",
        "    \n",
        "    data = np.array(data)\n",
        "    \n",
        "    fig = plt.figure(figsize=(18, 6))\n",
        "    \n",
        "    # Plot 1: Heatmap\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    im = ax1.imshow(data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
        "    ax1.set_xticks(np.arange(len(all_metrics)))\n",
        "    ax1.set_yticks(np.arange(len(models)))\n",
        "    ax1.set_xticklabels(all_metrics, rotation=45, ha='right')\n",
        "    ax1.set_yticklabels(models)\n",
        "    ax1.set_title('Multi-Metric Heatmap', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Add values in cells\n",
        "    for i in range(len(models)):\n",
        "        for j in range(len(all_metrics)):\n",
        "            text = ax1.text(j, i, f'{data[i, j]:.3f}',\n",
        "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
        "    \n",
        "    plt.colorbar(im, ax=ax1, label='Score')\n",
        "    \n",
        "    # Plot 2: Grouped bar chart\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    x = np.arange(len(models))\n",
        "    width = 0.8 / len(all_metrics)\n",
        "    \n",
        "    for i, metric in enumerate(all_metrics):\n",
        "        scores = [results[m]['scores'][metric] for m in models]\n",
        "        ax2.bar(x + i * width, scores, width, label=metric)\n",
        "    \n",
        "    ax2.set_xlabel('Models', fontweight='bold')\n",
        "    ax2.set_ylabel('Score', fontweight='bold')\n",
        "    ax2.set_title('Metrics Comparison by Model', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xticks(x + width * (len(all_metrics) - 1) / 2)\n",
        "    ax2.set_xticklabels(models, rotation=15, ha='right')\n",
        "    ax2.legend(loc='upper left', fontsize=8)\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Radar chart (only if we have <= 6 metrics)\n",
        "    if len(all_metrics) <= 8:\n",
        "        ax3 = plt.subplot(1, 3, 3, projection='polar')\n",
        "        \n",
        "        angles = np.linspace(0, 2 * np.pi, len(all_metrics), endpoint=False).tolist()\n",
        "        angles += angles[:1]  # close the plot\n",
        "        \n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(models)))\n",
        "        \n",
        "        for idx, model in enumerate(models):\n",
        "            values = [results[model]['scores'][m] for m in all_metrics]\n",
        "            values += values[:1]  # close the plot\n",
        "            ax3.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[idx])\n",
        "            ax3.fill(angles, values, alpha=0.15, color=colors[idx])\n",
        "        \n",
        "        ax3.set_xticks(angles[:-1])\n",
        "        ax3.set_xticklabels(all_metrics, fontsize=9)\n",
        "        ax3.set_ylim(0, 1)\n",
        "        ax3.set_title('Radar Chart: All Metrics', fontsize=14, fontweight='bold', pad=20)\n",
        "        ax3.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "        ax3.grid(True)\n",
        "    else:\n",
        "        ax3 = plt.subplot(1, 3, 3)\n",
        "        ax3.text(0.5, 0.5, f'Radar chart\\navailable for\\n≤8 metrics\\n({len(all_metrics)} provided)',\n",
        "                ha='center', va='center', fontsize=12)\n",
        "        ax3.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Figure saved to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_multi_metric_summary(results):\n",
        "    \"\"\"\n",
        "    Print formatted summary of multi-metric results.\n",
        "    \n",
        "    Args:\n",
        "        results: dict from evaluate_with_multiple_metrics\n",
        "    \"\"\"\n",
        "    models = list(results.keys())\n",
        "    all_metrics = list(results[models[0]]['scores'].keys())\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MULTI-METRIC EVALUATION SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Print scores table\n",
        "    print(f\"\\n{'Model':<15}\", end='')\n",
        "    for metric in all_metrics:\n",
        "        print(f\"{metric:<12}\", end='')\n",
        "    print()\n",
        "    print(\"-\" * (15 + 12 * len(all_metrics)))\n",
        "    \n",
        "    for model in models:\n",
        "        print(f\"{model:<15}\", end='')\n",
        "        for metric in all_metrics:\n",
        "            score = results[model]['scores'][metric]\n",
        "            print(f\"{score:<12.4f}\", end='')\n",
        "        print()\n",
        "    \n",
        "    # Best model per metric\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"Best Model per Metric:\")\n",
        "    print(\"-\"*80)\n",
        "    \n",
        "    for metric in all_metrics:\n",
        "        # Note: for 'relational' lower is better (it's a loss)\n",
        "        if metric == 'relational':\n",
        "            best_model = min(models, key=lambda m: results[m]['scores'][metric])\n",
        "            best_score = results[best_model]['scores'][metric]\n",
        "            print(f\"  {metric:<20}: {best_model:<15} (loss: {best_score:.4f})\")\n",
        "        else:\n",
        "            best_model = max(models, key=lambda m: results[m]['scores'][metric])\n",
        "            best_score = results[best_model]['scores'][metric]\n",
        "            print(f\"  {metric:<20}: {best_model:<15} ({best_score:.4f})\")\n",
        "    \n",
        "    # Average rank\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"Average Ranking (lower is better):\")\n",
        "    print(\"-\"*80)\n",
        "    \n",
        "    ranks = {}\n",
        "    for metric in all_metrics:\n",
        "        # Sort models by this metric\n",
        "        if metric == 'relational':\n",
        "            sorted_models = sorted(models, key=lambda m: results[m]['scores'][metric])\n",
        "        else:\n",
        "            sorted_models = sorted(models, key=lambda m: results[m]['scores'][metric], reverse=True)\n",
        "        \n",
        "        for rank, model in enumerate(sorted_models, start=1):\n",
        "            ranks[model] = ranks.get(model, []) + [rank]\n",
        "    \n",
        "    avg_ranks = {m: np.mean(ranks[m]) for m in models}\n",
        "    sorted_by_avg = sorted(avg_ranks.items(), key=lambda x: x[1])\n",
        "    \n",
        "    for model, avg_rank in sorted_by_avg:\n",
        "        print(f\"  {model:<15}: {avg_rank:.2f}\")\n",
        "    \n",
        "    print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Visualization and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_results(results, metric_name='CKA', save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize evaluation results\n",
        "    \n",
        "    Args:\n",
        "        results: dict with evaluation results\n",
        "        metric_name: Name of metric used\n",
        "        save_path: Optional path to save figure\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    methods = list(results.keys())\n",
        "    scores = [results[m]['score'] for m in methods]\n",
        "    \n",
        "    # Color scheme\n",
        "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
        "    \n",
        "    # Plot 1: Bar chart\n",
        "    bars = ax1.bar(methods, scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "    ax1.set_ylabel(f'{metric_name} Similarity Score', fontsize=14, fontweight='bold')\n",
        "    ax1.set_title(f'{metric_name} Similarity: Original vs Quantized Features', fontsize=16, fontweight='bold')\n",
        "    ax1.set_ylim(0, 1.0)\n",
        "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax1.axhline(y=0.5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Baseline (0.5)')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, (method, score) in enumerate(zip(methods, scores)):\n",
        "        ax1.text(i, score + 0.02, f'{score:.4f}', ha='center', va='bottom', \n",
        "                fontsize=12, fontweight='bold')\n",
        "    \n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.set_xticklabels(methods, rotation=15, ha='right')\n",
        "    \n",
        "    # Plot 2: Comparison table as text\n",
        "    ax2.axis('off')\n",
        "    \n",
        "    # Create table data\n",
        "    table_data = [['Method', f'{metric_name} Score', 'Rank']]\n",
        "    \n",
        "    # Sort by score\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1]['score'], reverse=True)\n",
        "    \n",
        "    for rank, (method, data) in enumerate(sorted_results, start=1):\n",
        "        score = data['score']\n",
        "        table_data.append([method, f'{score:.4f}', str(rank)])\n",
        "    \n",
        "    # Create table\n",
        "    table = ax2.table(cellText=table_data, cellLoc='center', loc='center',\n",
        "                     colWidths=[0.3, 0.3, 0.2])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(12)\n",
        "    table.scale(1, 2.5)\n",
        "    \n",
        "    # Style header\n",
        "    for i in range(3):\n",
        "        table[(0, i)].set_facecolor('#34495e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    # Style data rows\n",
        "    for i in range(1, len(table_data)):\n",
        "        for j in range(3):\n",
        "            if i % 2 == 0:\n",
        "                table[(i, j)].set_facecolor('#ecf0f1')\n",
        "            else:\n",
        "                table[(i, j)].set_facecolor('#ffffff')\n",
        "    \n",
        "    ax2.set_title('Ranking by Similarity Score', fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"✓ Figure saved to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_summary(results, metric_name='CKA'):\n",
        "    \"\"\"\n",
        "    Print a summary of evaluation results\n",
        "    \n",
        "    Args:\n",
        "        results: dict with evaluation results\n",
        "        metric_name: Name of metric used\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"{metric_name} EVALUATION SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Sort by score\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1]['score'], reverse=True)\n",
        "    \n",
        "    for rank, (method, data) in enumerate(sorted_results, start=1):\n",
        "        score = data['score']\n",
        "        print(f\"\\n{rank}. {method}\")\n",
        "        print(f\"   {metric_name} Score: {score:.4f}\")\n",
        "        print(f\"   Encoder Adapter: {'Yes' if data['checkpoint_info']['has_encoder_adapter'] else 'No'}\")\n",
        "        print(f\"   Decoder Adapter: {'Yes' if data['checkpoint_info']['has_decoder_adapter'] else 'No'}\")\n",
        "    \n",
        "    # Statistics\n",
        "    scores = [data['score'] for data in results.values()]\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"Statistics:\")\n",
        "    print(f\"  Mean {metric_name} score: {np.mean(scores):.4f}\")\n",
        "    print(f\"  Std {metric_name} score: {np.std(scores):.4f}\")\n",
        "    print(f\"  Min {metric_name} score: {np.min(scores):.4f} ({sorted_results[-1][0]})\")\n",
        "    print(f\"  Max {metric_name} score: {np.max(scores):.4f} ({sorted_results[0][0]})\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "def save_results_to_json(results, output_path, metric_name='CKA'):\n",
        "    \"\"\"\n",
        "    Save results to JSON file\n",
        "    \n",
        "    Args:\n",
        "        results: dict with evaluation results\n",
        "        output_path: Path to save JSON\n",
        "        metric_name: Name of metric used\n",
        "    \"\"\"\n",
        "    output_data = {\n",
        "        'metric': metric_name,\n",
        "        'results': results,\n",
        "        'summary': {\n",
        "            'mean_score': float(np.mean([r['score'] for r in results.values()])),\n",
        "            'std_score': float(np.std([r['score'] for r in results.values()])),\n",
        "            'min_score': float(np.min([r['score'] for r in results.values()])),\n",
        "            'max_score': float(np.max([r['score'] for r in results.values()]))\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(output_data, f, indent=2)\n",
        "    \n",
        "    print(f\"✓ Results saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Run Evaluation\n",
        "\n",
        "Execute the evaluation pipeline with CKA metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Multi-Metric Evaluation\n",
        "\n",
        "Evaluate all models with multiple metrics at once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print multi-metric summary\n",
        "print_multi_metric_summary(multi_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize multi-metric results\n",
        "visualize_multi_metric_results(multi_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save multi-metric results to JSON\n",
        "multi_output_path = './evaluation_results_multi_metric.json'\n",
        "\n",
        "output_data = {\n",
        "    'metrics_evaluated': METRICS_TO_EVALUATE,\n",
        "    'models': {}\n",
        "}\n",
        "\n",
        "for model_name, model_data in multi_results.items():\n",
        "    output_data['models'][model_name] = {\n",
        "        'scores': model_data['scores'],\n",
        "        'checkpoint_info': model_data['checkpoint_info']\n",
        "    }\n",
        "\n",
        "with open(multi_output_path, 'w') as f:\n",
        "    json.dump(output_data, f, indent=2)\n",
        "\n",
        "print(f\"✓ Multi-metric results saved to: {multi_output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Extended Multi-Metric Framework\n",
        "\n",
        "This notebook provides a comprehensive framework for evaluating quantized DeepLabV3 models using **6 different similarity metrics**.\n",
        "\n",
        "### Available Metrics:\n",
        "\n",
        "1. **CKA (Centered Kernel Alignment)** \n",
        "   - Measures linear similarity between feature representations\n",
        "   - Uses streaming computation for memory efficiency\n",
        "   - Higher is better (0-1 range)\n",
        "\n",
        "2. **PWCCA (Projection Weighted CCA)**\n",
        "   - Weighted canonical correlation analysis\n",
        "   - Weights correlations by explained variance\n",
        "   - More robust than standard CCA\n",
        "   - Higher is better (0-1 range)\n",
        "\n",
        "3. **Geometry Score**\n",
        "   - Measures preservation of pairwise distances\n",
        "   - Uses Spearman correlation between distance matrices\n",
        "   - Tests if relative distances are preserved\n",
        "   - Higher is better (-1 to 1 range, typically 0-1)\n",
        "\n",
        "4. **RSA (Representational Similarity Analysis)**\n",
        "   - Compares representational dissimilarity matrices\n",
        "   - Uses Pearson correlation of RDMs\n",
        "   - Standard metric in neuroscience/ML\n",
        "   - Higher is better (-1 to 1 range)\n",
        "\n",
        "5. **Relational Knowledge Loss**\n",
        "   - Measures preservation of k-NN relationships\n",
        "   - Checks if nearest neighbors remain similar after quantization\n",
        "   - **Lower is better** (0-1 range, it's a loss metric)\n",
        "\n",
        "6. **Jaccard KNN**\n",
        "   - Jaccard similarity of k-nearest neighbor sets\n",
        "   - Direct measure of neighborhood preservation\n",
        "   - Higher is better (0-1 range)\n",
        "\n",
        "### Usage Examples:\n",
        "\n",
        "```python\n",
        "# Single metric evaluation\n",
        "results = evaluate_all_models(\n",
        "    models_dir=MODELS_DIR,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    metric_name='cka'  # or 'pwcca', 'geometry', 'rsa', 'relational', 'jaccard_knn'\n",
        ")\n",
        "\n",
        "# Multi-metric evaluation (recommended)\n",
        "multi_results = evaluate_with_multiple_metrics(\n",
        "    models_dir=MODELS_DIR,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    metric_names=['cka', 'pwcca', 'geometry', 'rsa', 'relational', 'jaccard_knn']\n",
        ")\n",
        "```\n",
        "\n",
        "### Memory Optimization:\n",
        "\n",
        "- **CKA**: Streaming mode (2-pass covariance computation)\n",
        "- **Other metrics**: Extract features once, reuse across metrics\n",
        "- **Sampling**: Geometry/RSA use sampling for large datasets (max 2000 samples)\n",
        "- **Batch-wise KNN**: Relational/Jaccard compute KNN in batches\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "- `evaluation_results_cka.json` - Single CKA metric results\n",
        "- `evaluation_results_multi_metric.json` - All metrics results\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "**Best quantization method depends on your priority:**\n",
        "- **Highest fidelity**: Look for high CKA + PWCCA + RSA\n",
        "- **Geometric preservation**: Look for high Geometry Score\n",
        "- **Neighbor structure**: Look for high Jaccard KNN + low Relational Loss\n",
        "- **Overall balance**: Check average ranking across all metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start: Run All Metrics Evaluation\n",
        "\n",
        "Готовый код для запуска полной оценки всех моделей со всеми метриками\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ГОТОВЫЙ КОД ДЛЯ ЗАПУСКА ВСЕХ МЕТРИК\n",
        "# ============================================================================\n",
        "\n",
        "# Список всех доступных метрик\n",
        "ALL_METRICS = ['cka', 'pwcca', 'geometry', 'rsa', 'relational', 'jaccard_knn']\n",
        "\n",
        "print(\"🚀 Запуск multi-metric evaluation\")\n",
        "print(f\"📊 Метрики: {', '.join(ALL_METRICS)}\")\n",
        "print(f\"📦 Модели: VQ-EMA, FSQ, LFQ, ResidualVQ\")\n",
        "print(f\"📂 Путь к моделям: {MODELS_DIR}\")\n",
        "print(f\"🎯 Устройство: {device}\")\n",
        "print(f\"📈 Размер датасета: {len(test_loader.dataset)} изображений\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Запуск оценки\n",
        "results = evaluate_with_multiple_metrics(\n",
        "    models_dir=MODELS_DIR,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    metric_names=ALL_METRICS\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Оценка завершена!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вывод результатов в виде таблицы\n",
        "print_multi_metric_summary(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация результатов (heatmap, bar chart, radar chart)\n",
        "visualize_multi_metric_results(results, save_path='./multi_metric_evaluation.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сохранение результатов в JSON\n",
        "output_file = './evaluation_results_all_metrics.json'\n",
        "\n",
        "output_data = {\n",
        "    'metrics_evaluated': ALL_METRICS,\n",
        "    'dataset_size': len(test_loader.dataset),\n",
        "    'batch_size': test_loader.batch_size,\n",
        "    'models': {}\n",
        "}\n",
        "\n",
        "for model_name, model_data in results.items():\n",
        "    output_data['models'][model_name] = {\n",
        "        'scores': model_data['scores'],\n",
        "        'checkpoint_info': model_data['checkpoint_info']\n",
        "    }\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(output_data, f, indent=2)\n",
        "\n",
        "print(f\"✅ Результаты сохранены в {output_file}\")\n",
        "print(f\"✅ Визуализация сохранена в ./multi_metric_evaluation.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Memory Optimizations & Progress Improvements\n",
        "\n",
        "### 🎯 Changes Made:\n",
        "\n",
        "#### 1. **Memory-Efficient Pooled Extraction** (Ячейка 19)\n",
        "- ✅ Added `extract_original_features_pooled()` and `extract_quantized_features_pooled()`\n",
        "- ✅ Performs spatial pooling **on GPU before CPU transfer**\n",
        "- ✅ Memory reduction: **30 GB → 30 MB** (1000× improvement!)\n",
        "- ✅ Results are **identical** to non-pooled version (pooling was already in metrics)\n",
        "\n",
        "#### 2. **Updated Multi-Metric Evaluation** (Ячейка 23)\n",
        "- ✅ Now uses pooled extraction functions\n",
        "- ✅ **Prevents CUDA OutOfMemoryError** on V100 (32 GB)\n",
        "- ✅ Clean tqdm progress bars instead of print spam\n",
        "\n",
        "#### 3. **Defensive Pooling in Metrics** (Ячейка 15)\n",
        "- ✅ Updated PWCCAMetric, RSAMetric, RelationalKnowledgeLossMetric, JaccardKNNMetric\n",
        "- ✅ Pools features **before** `.to(device)` for extra safety\n",
        "- ✅ Prevents OOM even if pooled extraction is not used\n",
        "\n",
        "#### 4. **Improved Progress Display** (Ячейка 21)\n",
        "- ✅ Replaced verbose prints with `tqdm.notebook` progress bars\n",
        "- ✅ Nested progress bars: Models → Metrics → Batches\n",
        "- ✅ Much cleaner output during evaluation\n",
        "\n",
        "#### 5. **Removed Redundant Feature Extraction** (Ячейка 23)\n",
        "- ✅ CKA now uses pre-extracted pooled features instead of streaming\n",
        "- ✅ **No redundant extraction**: features extracted ONCE per model\n",
        "- ✅ **Speedup: 3.4× faster** (5 dataset passes instead of 17!)\n",
        "- ✅ Results are identical to streaming CKA (same pooling logic)\n",
        "\n",
        "### 📊 Before vs After:\n",
        "\n",
        "**Before:**\n",
        "```\n",
        "Extracting original features...\n",
        "  Processing batch 0/460\n",
        "  ...\n",
        "✓ Extracted: [3680, 2048, 32, 32]  ← 30 GB!\n",
        "OutOfMemoryError: CUDA out of memory...\n",
        "\n",
        "For each model:\n",
        "  - CKA: extracts features 4× (Pass 1 orig, Pass 1 quant, Pass 2 orig, Pass 2 quant)\n",
        "  - Other metrics: use pre-extracted features\n",
        "Total: 17 dataset passes for 4 models × 6 metrics\n",
        "```\n",
        "\n",
        "**After:**\n",
        "```\n",
        "📊 Extracting original features (pooled, memory-efficient)...\n",
        "[Progress bar with tqdm]\n",
        "✓ Pooled features: [3680, 2048] (~30 MB)  ← 1000× smaller!\n",
        "\n",
        "For each model:\n",
        "  - Extract quantized features: 1×\n",
        "  - ALL metrics use pre-extracted features: 0× redundant extraction!\n",
        "Total: 5 dataset passes for 4 models × 6 metrics ← 3.4× faster!\n",
        "✅ All metrics complete without OOM\n",
        "```\n",
        "\n",
        "### 🚀 Ready to Run!\n",
        "\n",
        "Запустите **ячейку 32** для полной оценки всех моделей по всем метрикам.\n",
        "поч"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 🚀 Performance Optimization Details\n",
        "\n",
        "### Feature Extraction Efficiency\n",
        "\n",
        "**Smart Caching Strategy:**\n",
        "\n",
        "1. **Original features** → extracted ONCE, used by all 4 models × 6 metrics = 24 times reuse ✅\n",
        "2. **Quantized features** → extracted ONCE per model, used by 6 metrics each ✅\n",
        "3. **CKA metric** → now uses cached pooled features (previously extracted 4× per model) ✅\n",
        "\n",
        "### Dataset Pass Count:\n",
        "\n",
        "| Operation | Before | After | Improvement |\n",
        "|-----------|--------|-------|-------------|\n",
        "| Original features extraction | 5× (1 + 4 for CKA) | **1×** | 5× fewer |\n",
        "| Quantized features per model | 3× (1 + 2 for CKA) | **1×** | 3× fewer |\n",
        "| **Total for 4 models** | **17 passes** | **5 passes** | **3.4× faster** ⚡ |\n",
        "\n",
        "### Why This Works:\n",
        "\n",
        "1. ✅ All metrics perform spatial pooling `[N, C, H, W] → [N, C]` anyway\n",
        "2. ✅ Pooled features are small (30 MB vs 30 GB) → fit in memory\n",
        "3. ✅ CKA with pooled features gives **identical results** to streaming CKA\n",
        "4. ✅ No OOM risk (defensive pooling in metrics + pooled extraction)\n",
        "\n",
        "### Time Savings Example:\n",
        "\n",
        "For **3680 images** on **V100 GPU**:\n",
        "- Feature extraction: ~45 seconds per pass\n",
        "- **Before**: 17 passes × 45s = **12.75 minutes**\n",
        "- **After**: 5 passes × 45s = **3.75 minutes**\n",
        "- **Saved: 9 minutes per full evaluation!** ⏱️\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ✅ Все готово к запуску!\n",
        "\n",
        "### Что было оптимизировано:\n",
        "\n",
        "1. ✅ **Память**: 30 GB → 30 MB (1000× экономия)\n",
        "2. ✅ **Скорость**: 17 проходов → 5 проходов (3.4× быстрее)\n",
        "3. ✅ **Прогресс**: Clean tqdm bars вместо print spam\n",
        "4. ✅ **Надёжность**: Defensive pooling, no OOM errors\n",
        "\n",
        "### Запустите ячейку 32 для полной оценки! 🚀\n",
        "\n",
        "**Результат:**\n",
        "- 4 модели (VQ-EMA, FSQ, LFQ, ResidualVQ)\n",
        "- 6 метрик (CKA, PWCCA, Geometry, RSA, Relational, Jaccard KNN)\n",
        "- Время: ~4-8 минут на V100\n",
        "- Память: ~30 MB features\n",
        "- Визуализации + JSON результаты\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Опционально: Запуск отдельных метрик\n",
        "\n",
        "Если нужно протестировать только одну метрику\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример: запуск только одной метрики (быстрее)\n",
        "# Раскомментируйте нужную метрику:\n",
        "\n",
        "# SINGLE_METRIC = 'cka'           # Centered Kernel Alignment (streaming, быстро)\n",
        "# SINGLE_METRIC = 'pwcca'         # Projection Weighted CCA\n",
        "# SINGLE_METRIC = 'geometry'      # Geometry Score (Spearman correlation)\n",
        "# SINGLE_METRIC = 'rsa'           # Representational Similarity Analysis\n",
        "# SINGLE_METRIC = 'relational'    # Relational Knowledge Loss (lower is better)\n",
        "SINGLE_METRIC = 'jaccard_knn'   # Jaccard KNN similarity\n",
        "\n",
        "single_results = evaluate_all_models(\n",
        "    models_dir=MODELS_DIR,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    metric_name=SINGLE_METRIC\n",
        ")\n",
        "\n",
        "# Вывод результатов\n",
        "print(f\"\\n{SINGLE_METRIC.upper()} Results:\")\n",
        "print(\"=\"*50)\n",
        "for model, data in single_results.items():\n",
        "    print(f\"{model:<15}: {data['score']:.4f}\")\n",
        "    \n",
        "# Визуализация\n",
        "visualize_results(single_results, metric_name=SINGLE_METRIC.upper())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 📖 Краткая инструкция по запуску\n",
        "\n",
        "### Вариант 1: Все метрики сразу (рекомендуется)\n",
        "\n",
        "Запустите **ячейку 32** - она автоматически:\n",
        "- ✅ Загрузит все 4 модели (VQ-EMA, FSQ, LFQ, ResidualVQ)\n",
        "- ✅ Посчитает все 6 метрик для каждой модели\n",
        "- ✅ Выведет таблицу с результатами (ячейка 33)\n",
        "- ✅ Построит графики: heatmap, bar chart, radar chart (ячейка 34)\n",
        "- ✅ Сохранит результаты в JSON (ячейка 35)\n",
        "\n",
        "**Время выполнения**: ~15-30 минут (зависит от размера датасета и GPU)\n",
        "\n",
        "### Вариант 2: Одна метрика\n",
        "\n",
        "Если нужно быстро протестировать:\n",
        "- Запустите **ячейку 37** с нужной метрикой\n",
        "- Раскомментируйте строку с выбранной метрикой\n",
        "\n",
        "**Время выполнения**: ~3-5 минут на одну метрику\n",
        "\n",
        "### Интерпретация результатов\n",
        "\n",
        "| Метрика | Диапазон | Что показывает | Лучше |\n",
        "|---------|----------|----------------|-------|\n",
        "| CKA | 0-1 | Линейная схожесть представлений | Выше |\n",
        "| PWCCA | 0-1 | Взвешенная CCA по объясненной дисперсии | Выше |\n",
        "| Geometry | -1 to 1 | Сохранение попарных расстояний | Выше |\n",
        "| RSA | -1 to 1 | Корреляция матриц несхожести | Выше |\n",
        "| Relational | 0-1 | **LOSS** - потеря k-NN соседей | **Ниже** |\n",
        "| Jaccard KNN | 0-1 | Jaccard сходство k-NN множеств | Выше |\n",
        "\n",
        "### Файлы с результатами\n",
        "\n",
        "После запуска будут созданы:\n",
        "- `evaluation_results_all_metrics.json` - все результаты в JSON\n",
        "- `multi_metric_evaluation.png` - визуализация\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run multi-metric evaluation\n",
        "METRICS_TO_EVALUATE = ['cka', 'pwcca', 'geometry', 'rsa', 'relational', 'jaccard_knn']\n",
        "\n",
        "multi_results = evaluate_with_multiple_metrics(\n",
        "    models_dir=MODELS_DIR,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    metric_names=METRICS_TO_EVALUATE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation with CKA metric\n",
        "METRIC_NAME = 'cka'  # Change to 'cca' or 'geometric' for other metrics\n",
        "\n",
        "results = evaluate_all_models(\n",
        "    models_dir=MODELS_DIR,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    metric_name=METRIC_NAME\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print summary\n",
        "print_summary(results, metric_name=METRIC_NAME.upper())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "visualize_results(results, metric_name=METRIC_NAME.upper())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON\n",
        "output_json_path = f'./evaluation_results_{METRIC_NAME}.json'\n",
        "save_results_to_json(results, output_json_path, metric_name=METRIC_NAME.upper())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Using Different Metrics (Optional)\n",
        "\n",
        "To switch to a different similarity metric, simply change the `METRIC_NAME` variable above:\n",
        "\n",
        "```python\n",
        "# For CCA (Canonical Correlation Analysis)\n",
        "METRIC_NAME = 'cca'\n",
        "\n",
        "# For Geometric Score\n",
        "METRIC_NAME = 'geometric'\n",
        "```\n",
        "\n",
        "Then re-run the evaluation cells.\n",
        "\n",
        "### Implementation Notes:\n",
        "\n",
        "1. **CKA (Centered Kernel Alignment)**: Currently implemented using `torch-cka` library\n",
        "2. **CCA (Canonical Correlation Analysis)**: Placeholder - needs implementation\n",
        "3. **Geometric Score**: Placeholder - needs implementation\n",
        "\n",
        "The framework is designed to make adding new metrics straightforward:\n",
        "- Inherit from `SimilarityMetric` class\n",
        "- Implement the `compute()` method\n",
        "- Add to `METRIC_REGISTRY`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a comprehensive framework for evaluating quantized DeepLabV3 models using similarity metrics.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "1. **Modular Architecture**: All model components (Encoder, Decoder, Quantizers) are properly abstracted\n",
        "2. **Flexible Model Loading**: Unified function to load any quantized model from `models/` directory\n",
        "3. **Extensible Metrics**: Easy-to-extend framework supporting multiple similarity metrics\n",
        "4. **CKA Implementation**: Full CKA (Centered Kernel Alignment) metric implementation\n",
        "5. **Comprehensive Visualization**: Bar charts, ranking tables, and statistical summaries\n",
        "6. **Results Export**: JSON export for further analysis\n",
        "\n",
        "### Models Evaluated:\n",
        "\n",
        "- **VQ-EMA**: Vector Quantization with Exponential Moving Average\n",
        "- **FSQ**: Finite Scalar Quantization\n",
        "- **LFQ**: Lookup-Free Quantization\n",
        "- **ResidualVQ**: Residual Vector Quantization\n",
        "\n",
        "### Metrics:\n",
        "\n",
        "- **CKA (Implemented)**: Measures linear similarity between feature representations\n",
        "- **CCA (Placeholder)**: Canonical Correlation Analysis\n",
        "- **Geometric Score (Placeholder)**: Geometric similarity measure\n",
        "\n",
        "### How to Use:\n",
        "\n",
        "1. Run all cells sequentially\n",
        "2. Results will show CKA similarity scores comparing original backbone features vs quantized features\n",
        "3. Higher CKA score = quantized features are more similar to original features\n",
        "4. To change metrics, modify `METRIC_NAME` and re-run evaluation cells\n",
        "\n",
        "### Expected Outputs:\n",
        "\n",
        "- Console output with detailed progress and scores\n",
        "- Visualization with bar charts and ranking tables\n",
        "- JSON file with complete results (`evaluation_results_cka.json`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
