{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/","title":"LatentQuant Documentation","text":"<p>\u0414\u043e\u0431\u0440\u043e \u043f\u043e\u0436\u0430\u043b\u043e\u0432\u0430\u0442\u044c \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u043f\u0430\u043a\u0435\u0442\u0430 <code>embeddings_squeeze</code>.</p>"},{"location":"api/#_1","title":"\u0411\u044b\u0441\u0442\u0440\u044b\u0439 \u0434\u043e\u0441\u0442\u0443\u043f","text":"<ul> <li>\u0412\u0441\u0435 \u043c\u043e\u0434\u0443\u043b\u0438</li> <li>\u041c\u043e\u0434\u0435\u043b\u0438 </li> <li>\u041b\u043e\u0433\u0433\u0435\u0440\u044b</li> <li>\u0423\u0442\u0438\u043b\u0438\u0442\u044b</li> <li>\u041f\u0440\u0438\u043c\u0435\u0440\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f</li> <li>\u0418\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u044f \u043f\u043e \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0435</li> </ul>"},{"location":"api/#_2","title":"\u041e \u043f\u0440\u043e\u0435\u043a\u0442\u0435","text":"<p><code>embeddings_squeeze</code> - \u044d\u0442\u043e \u043f\u0430\u043a\u0435\u0442 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430\u043c\u0438 \u0438 \u0438\u0445 \u0441\u0436\u0430\u0442\u0438\u0435\u043c.</p>"},{"location":"api/all/","title":"\u041f\u043e\u043b\u043d\u0430\u044f \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043f\u0430\u043a\u0435\u0442\u0430","text":"<p>embeddings_squeeze: Vector Quantization for Segmentation Model Compression</p>"},{"location":"api/all/#embeddings_squeeze-classes","title":"Classes","text":""},{"location":"api/all/#embeddings_squeeze.VQWithProjection","title":"VQWithProjection","text":"<pre><code>VQWithProjection(\n    input_dim,\n    codebook_size=512,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Vector Quantization (VQ-VAE) with projections</p> <p>Uses EMA for codebook updates (no gradients needed for codebook) ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection (e.g., 2048 -&gt; 64)\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Vector Quantization\n    self.vq = VectorQuantize(\n        dim=bottleneck_dim,\n        codebook_size=codebook_size,\n        decay=decay,  # EMA decay for codebook\n        commitment_weight=commitment_weight  # Commitment loss weight\n    )\n\n    # Up projection (64 -&gt; 2048)\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.FSQWithProjection","title":"FSQWithProjection","text":"<pre><code>FSQWithProjection(input_dim, levels=None)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Finite Scalar Quantization (FSQ)</p> <p>Quantization without codebook - each dimension quantized independently ~10 bits per vector at levels=[8,5,5,5]</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int, levels: list = None):\n    super().__init__(input_dim)\n    if levels is None:\n        levels = [8, 5, 5, 5]  # 8*5*5*5 = 1000 codes \u2248 2^10\n\n    self.num_levels = len(levels)\n\n    # Projection to quantization space\n    self.project_in = nn.Linear(input_dim, self.num_levels)\n\n    # FSQ quantization\n    self.fsq = FSQ(levels=levels, dim=self.num_levels)\n\n    # Projection back\n    self.project_out = nn.Linear(self.num_levels, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.LFQWithProjection","title":"LFQWithProjection","text":"<pre><code>LFQWithProjection(\n    input_dim,\n    codebook_size=512,\n    entropy_loss_weight=0.1,\n    diversity_gamma=0.1,\n    spherical=False,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Lookup-Free Quantization (LFQ)</p> <p>Uses entropy loss for code diversity ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512,\n    entropy_loss_weight: float = 0.1, \n    diversity_gamma: float = 0.1, \n    spherical: bool = False\n):\n    super().__init__(input_dim)\n    # Quantization dimension = log2(codebook_size)\n    self.quant_dim = int(math.log2(codebook_size))\n\n    # Projection with normalization\n    self.project_in = nn.Sequential(\n        nn.Linear(input_dim, self.quant_dim),\n        nn.LayerNorm(self.quant_dim)\n    )\n\n    # LFQ quantization\n    self.lfq = LFQ(\n        dim=self.quant_dim,\n        codebook_size=codebook_size,\n        entropy_loss_weight=entropy_loss_weight,\n        diversity_gamma=diversity_gamma,\n        spherical=spherical\n    )\n\n    # Projection back\n    self.project_out = nn.Linear(self.quant_dim, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ResidualVQWithProjection","title":"ResidualVQWithProjection","text":"<pre><code>ResidualVQWithProjection(\n    input_dim,\n    num_quantizers=4,\n    codebook_size=256,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Residual Vector Quantization (RVQ)</p> <p>Multi-level quantization - each level quantizes the residual of the previous 32 bits per vector at num_quantizers=4, codebook_size=256 (4*8 bits)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    num_quantizers: int = 4,\n    codebook_size: int = 256, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Residual VQ\n    self.residual_vq = ResidualVQ(\n        dim=bottleneck_dim,\n        num_quantizers=num_quantizers,  # Number of levels\n        codebook_size=codebook_size,\n        decay=decay,\n        commitment_weight=commitment_weight\n    )\n\n    # Up projection\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaseQuantizer","title":"BaseQuantizer","text":"<pre><code>BaseQuantizer(input_dim)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for all quantizers</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int):\n    super().__init__()\n    self.input_dim = input_dim\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaseQuantizer-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.BaseQuantizer.quantize_spatial","title":"quantize_spatial","text":"<pre><code>quantize_spatial(features)\n</code></pre> <p>Quantize spatial features [B, C, H, W]</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>quantized</code> <p>Quantized features [B, C, H, W]</p> <code>loss</code> <p>Quantization loss (scalar)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def quantize_spatial(self, features: torch.Tensor):\n    \"\"\"\n    Quantize spatial features [B, C, H, W]\n\n    Args:\n        features: Tensor of shape [B, C, H, W]\n\n    Returns:\n        quantized: Quantized features [B, C, H, W]\n        loss: Quantization loss (scalar)\n    \"\"\"\n    B, C, H, W = features.shape\n    # Transform [B, C, H, W] -&gt; [B, H*W, C]\n    seq = features.permute(0, 2, 3, 1).reshape(B, H * W, C)\n\n    # Quantize\n    quantized, indices, loss = self.forward(seq)\n\n    # Transform back [B, H*W, C] -&gt; [B, C, H, W]\n    quantized = quantized.reshape(B, H, W, C).permute(0, 3, 1, 2)\n\n    # Handle loss (may be tensor with multiple elements)\n    if isinstance(loss, torch.Tensor) and loss.numel() &gt; 1:\n        loss = loss.mean()\n\n    return quantized, loss\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.SegmentationBackbone","title":"SegmentationBackbone","text":"<pre><code>SegmentationBackbone()\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation backbones.</p> <p>All segmentation backbones should inherit from this class and implement the required methods for feature extraction and full segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.SegmentationBackbone-attributes","title":"Attributes","text":""},{"location":"api/all/#embeddings_squeeze.SegmentationBackbone.feature_dim","title":"feature_dim  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>feature_dim\n</code></pre> <p>Return the feature dimension.</p>"},{"location":"api/all/#embeddings_squeeze.SegmentationBackbone.num_classes","title":"num_classes  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>num_classes\n</code></pre> <p>Return the number of output classes.</p>"},{"location":"api/all/#embeddings_squeeze.SegmentationBackbone-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.SegmentationBackbone.extract_features","title":"extract_features  <code>abstractmethod</code>","text":"<pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract features from input images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, feature_dim, H', W']</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef extract_features(self, images, detach=True):\n    \"\"\"\n    Extract features from input images.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, feature_dim, H', W']\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.SegmentationBackbone.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(images)\n</code></pre> <p>Full forward pass for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef forward(self, images):\n    \"\"\"\n    Full forward pass for segmentation.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule","title":"VQSqueezeModule","text":"<pre><code>VQSqueezeModule(\n    backbone,\n    quantizer=None,\n    num_classes=21,\n    learning_rate=0.0001,\n    vq_loss_weight=0.1,\n    loss_type=\"ce\",\n    class_weights=None,\n    add_adapter=False,\n    feature_dim=2048,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for VQ compression training.</p> <p>Features: - Multiple quantizer support (VQ, FSQ, LFQ, RVQ) - Adapter layers for fine-tuning frozen backbones - Advanced loss functions (CE, Dice, Focal, Combined) - Embedding extraction and saving</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    quantizer: Optional[nn.Module] = None,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    vq_loss_weight: float = 0.1,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    add_adapter: bool = False,\n    feature_dim: int = 2048,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'quantizer', 'clearml_logger'])\n\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n    self.vq_loss_weight = vq_loss_weight\n    self.loss_type = loss_type\n    self.add_adapter = add_adapter\n    self.feature_dim = feature_dim\n\n    # Setup backbone with optional adapters\n    self.backbone = backbone\n    self._setup_backbone_with_adapters(feature_dim, add_adapter)\n\n    # Quantizer (optional)\n    self.quantizer = quantizer\n\n    # Loss function\n    self.criterion = self._init_loss(loss_type, class_weights)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n\n    # Embedding storage (per-epoch, first batch only)\n    self.embedding_dir = \"embeddings\"\n    os.makedirs(self.embedding_dir, exist_ok=True)\n    self._first_val_batch_features = None\n\n    # UMAP visualization storage\n    self._val_backbone_embeddings = []\n    self._val_quantized_embeddings = []\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule.forward","title":"forward","text":"<pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone + optional quantizer + decoder.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> <code>quant_loss</code> <p>Quantization loss (0 if no quantizer)</p> <code>original_features</code> <p>Extracted features (before quantization)</p> <code>quantized_features</code> <p>Features after quantization (same as original if no quantizer)</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone + optional quantizer + decoder.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n        quant_loss: Quantization loss (0 if no quantizer)\n        original_features: Extracted features (before quantization)\n        quantized_features: Features after quantization (same as original if no quantizer)\n    \"\"\"\n    # Extract features\n    features = self.backbone.extract_features(images, detach=self.feature_adapter is not None)\n\n    # Apply adapter if present\n    if self.feature_adapter is not None:\n        features = features + self.feature_adapter(features)\n\n    # Store original features for embedding extraction\n    original_features = features\n\n    # Quantize if quantizer is present\n    quant_loss = torch.tensor(0.0, device=images.device)\n    quantized_features = original_features  # Default to original if no quantizer\n    if self.quantizer is not None:\n        features, quant_loss = self.quantizer.quantize_spatial(features)\n        quantized_features = features\n\n    # Decode to segmentation logits\n    output = self.backbone.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear', align_corners=False)\n\n    return output, quant_loss, original_features, quantized_features\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, _, _ = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return loss\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, backbone_features, quantized_features = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    # Accumulate embeddings for UMAP visualization\n    self._val_backbone_embeddings.append(backbone_features.detach().cpu())\n    self._val_quantized_embeddings.append(quantized_features.detach().cpu())\n\n    # Save only first batch features for this epoch\n    if batch_idx == 0:\n        self._first_val_batch_features = backbone_features.detach().cpu()\n\n    return loss\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Clear accumulated embeddings at the start of each validation epoch.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_start(self):\n    \"\"\"Clear accumulated embeddings at the start of each validation epoch.\"\"\"\n    self._val_backbone_embeddings.clear()\n    self._val_quantized_embeddings.clear()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations and save embeddings.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations and save embeddings.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    keys = [\n        \"train/loss\", \"val/loss\", \"train/iou\", \"val/iou\",\n        \"train/precision\", \"val/precision\", \"train/recall\", \"val/recall\",\n        \"train/f1\", \"val/f1\"\n    ]\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n\n    # Generate UMAP visualizations on even epochs\n    if self.current_epoch % 2 == 0:\n        try:\n            import umap.umap_ as umap_module\n\n            # Only proceed if we have embeddings\n            if len(self._val_backbone_embeddings) &gt; 0 and len(self._val_quantized_embeddings) &gt; 0:\n                # Concatenate all accumulated embeddings\n                backbone_emb_flat = torch.cat(self._val_backbone_embeddings, dim=0)\n                quantized_emb_flat = torch.cat(self._val_quantized_embeddings, dim=0)\n\n                # Flatten spatial dimensions: [B, C, H, W] -&gt; [B*H*W, C]\n                backbone_emb_flat = backbone_emb_flat.permute(0, 2, 3, 1).reshape(-1, backbone_emb_flat.shape[1])\n                quantized_emb_flat = quantized_emb_flat.permute(0, 2, 3, 1).reshape(-1, quantized_emb_flat.shape[1])\n\n                # Convert to numpy\n                backbone_emb_np = backbone_emb_flat.numpy()\n                quantized_emb_np = quantized_emb_flat.numpy()\n\n                # Limit samples for performance (take subset if too large)\n                max_samples = 10000\n                if len(backbone_emb_np) &gt; max_samples:\n                    indices = np.random.choice(len(backbone_emb_np), max_samples, replace=False)\n                    backbone_emb_np = backbone_emb_np[indices]\n                    quantized_emb_np = quantized_emb_np[indices]\n\n                # Generate 2D UMAP\n                fig_2d, axs_2d = plt.subplots(1, 2, figsize=(12, 6))\n\n                proj_2d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(backbone_emb_np)\n                axs_2d[0].scatter(proj_2d_backbone[:, 0], proj_2d_backbone[:, 1], alpha=0.3)\n                axs_2d[0].set_title('2D UMAP: Backbone Embeddings')\n\n                proj_2d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(quantized_emb_np)\n                axs_2d[1].scatter(proj_2d_quantized[:, 0], proj_2d_quantized[:, 1], alpha=0.3)\n                axs_2d[1].set_title('2D UMAP: Quantized Embeddings')\n\n                # Convert 2D plot to image and log\n                fig_2d.canvas.draw()\n                img_2d = np.frombuffer(fig_2d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_2d = img_2d.reshape(fig_2d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_2d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"2d_embeddings_epoch_{self.current_epoch}\", \n                        img_2d, \n                        iteration=self.current_epoch\n                    )\n\n                # Generate 3D UMAP\n                fig_3d = plt.figure(figsize=(12, 6))\n                ax1 = fig_3d.add_subplot(121, projection='3d')\n                ax2 = fig_3d.add_subplot(122, projection='3d')\n\n                proj_3d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(backbone_emb_np)\n                ax1.scatter(proj_3d_backbone[:, 0], proj_3d_backbone[:, 1], proj_3d_backbone[:, 2], alpha=0.3)\n                ax1.set_title('3D UMAP: Backbone Embeddings')\n\n                proj_3d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(quantized_emb_np)\n                ax2.scatter(proj_3d_quantized[:, 0], proj_3d_quantized[:, 1], proj_3d_quantized[:, 2], alpha=0.3)\n                ax2.set_title('3D UMAP: Quantized Embeddings')\n\n                # Convert 3D plot to image and log\n                fig_3d.canvas.draw()\n                img_3d = np.frombuffer(fig_3d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_3d = img_3d.reshape(fig_3d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_3d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"3d_embeddings_epoch_{self.current_epoch}\", \n                        img_3d, \n                        iteration=self.current_epoch\n                    )\n\n            # Clear accumulated embeddings after logging\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n        except Exception as e:\n            if self.clearml_logger:\n                self.clearml_logger.report_text(\n                    f\"UMAP visualization failed at epoch {self.current_epoch}: {e}\"\n                )\n            # Clear embeddings even if visualization failed\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n    # Save per-epoch embedding (first validation batch only)\n    try:\n        if self._first_val_batch_features is not None:\n            emb_path = os.path.join(\n                self.embedding_dir,\n                f\"val_embedding_epoch{self.current_epoch}.pt\"\n            )\n            torch.save(self._first_val_batch_features, emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Saved small embedding: {emb_path}\")\n            # Reset for next epoch\n            self._first_val_batch_features = None\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed saving epoch embedding: {e}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer - only trainable parameters.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer - only trainable parameters.\"\"\"\n    params = []\n\n    # Add adapter parameters if present\n    if self.feature_adapter is not None:\n        params += list(self.feature_adapter.parameters())\n\n    # Add quantizer parameters if present\n    if self.quantizer is not None:\n        params += list(self.quantizer.parameters())\n\n    # Add backbone parameters if not frozen\n    if self.feature_adapter is None:\n        params += [p for p in self.backbone.parameters() if p.requires_grad]\n\n    # Remove duplicates\n    params = list({id(p): p for p in params}.values())\n\n    if not params:\n        raise ValueError(\"No trainable parameters found!\")\n\n    return torch.optim.AdamW(params, lr=self.learning_rate)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.VQSqueezeModule.on_train_start","title":"on_train_start","text":"<pre><code>on_train_start()\n</code></pre> <p>Ensure frozen backbone stays in eval mode.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_train_start(self):\n    \"\"\"Ensure frozen backbone stays in eval mode.\"\"\"\n    if self.feature_adapter is not None:\n        self.backbone.eval()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule","title":"BaselineSegmentationModule","text":"<pre><code>BaselineSegmentationModule(\n    backbone,\n    num_classes=21,\n    learning_rate=0.0001,\n    loss_type=\"ce\",\n    class_weights=None,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for baseline segmentation training.</p> <p>Wraps segmentation backbone without Vector Quantization for comparison.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'clearml_logger'])\n\n    self.backbone = backbone\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n\n    # Segmentation loss\n    if class_weights is not None:\n        weight = torch.tensor(class_weights, dtype=torch.float32)\n        self.seg_criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=255)\n    else:\n        self.seg_criterion = nn.CrossEntropyLoss(ignore_index=255)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule.forward","title":"forward","text":"<pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    output = self.backbone(images)\n    # Handle both dict and tensor returns\n    if isinstance(output, dict):\n        return output['out']\n    return output\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', seg_loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer.\"\"\"\n    # Optimize only trainable params\n    params = [p for p in self.parameters() if p.requires_grad]\n    return torch.optim.Adam(params, lr=self.learning_rate)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule.predict","title":"predict","text":"<pre><code>predict(images)\n</code></pre> <p>Predict segmentation masks.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <p>Segmentation predictions [B, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict(self, images):\n    \"\"\"\n    Predict segmentation masks.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        predictions: Segmentation predictions [B, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        output = self(images)\n        predictions = output.argmax(dim=1)\n    return predictions\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaselineSegmentationModule.predict_logits","title":"predict_logits","text":"<pre><code>predict_logits(images)\n</code></pre> <p>Predict segmentation logits.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>logits</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict_logits(self, images):\n    \"\"\"\n    Predict segmentation logits.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        logits: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        logits = self(images)\n    return logits\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaseDataModule","title":"BaseDataModule","text":"<pre><code>BaseDataModule(\n    data_path,\n    batch_size=4,\n    num_workers=0,\n    pin_memory=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningDataModule</code>, <code>ABC</code></p> <p>Abstract base class for data modules.</p> <p>All dataset-specific data modules should inherit from this class.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    pin_memory: bool = True,\n    **kwargs\n):\n    super().__init__()\n    self.data_path = data_path\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    self.pin_memory = pin_memory\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaseDataModule-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.BaseDataModule.setup","title":"setup  <code>abstractmethod</code>","text":"<pre><code>setup(stage=None)\n</code></pre> <p>Setup datasets for training/validation/testing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>'fit', 'validate', 'test', or None</p> <code>None</code> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef setup(self, stage: str = None):\n    \"\"\"\n    Setup datasets for training/validation/testing.\n\n    Args:\n        stage: 'fit', 'validate', 'test', or None\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaseDataModule.train_dataloader","title":"train_dataloader  <code>abstractmethod</code>","text":"<pre><code>train_dataloader(max_batches=None)\n</code></pre> <p>Return training dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef train_dataloader(self, max_batches: int = None):\n    \"\"\"Return training dataloader.\"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaseDataModule.val_dataloader","title":"val_dataloader  <code>abstractmethod</code>","text":"<pre><code>val_dataloader(max_batches=None)\n</code></pre> <p>Return validation dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef val_dataloader(self, max_batches: int = None):\n    \"\"\"Return validation dataloader.\"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.BaseDataModule.test_dataloader","title":"test_dataloader  <code>abstractmethod</code>","text":"<pre><code>test_dataloader(max_batches=None)\n</code></pre> <p>Return test dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef test_dataloader(self, max_batches: int = None):\n    \"\"\"Return test dataloader.\"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger","title":"ClearMLLogger","text":"<pre><code>ClearMLLogger(task)\n</code></pre> <p>Wrapper for ClearML logging compatible with PyTorch Lightning. Supports scalar metrics, plots, images, and text logging.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def __init__(self, task: Task):\n    self.task = task\n    self.logger = task.get_logger() if task else None\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.log_metrics","title":"log_metrics","text":"<pre><code>log_metrics(metrics, step=None)\n</code></pre> <p>Log metrics to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: int = None):\n    \"\"\"Log metrics to ClearML.\"\"\"\n    if self.logger is None:\n        return\n\n    for key, value in metrics.items():\n        # Split key into title and series (e.g., \"train/loss\" -&gt; title=\"train\", series=\"loss\")\n        if '/' in key:\n            title, series = key.split('/', 1)\n        else:\n            title = 'metrics'\n            series = key\n\n        self.logger.report_scalar(\n            title=title,\n            series=series,\n            value=value,\n            iteration=step\n        )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.log_scalar","title":"log_scalar","text":"<pre><code>log_scalar(title, series, value, iteration)\n</code></pre> <p>Log a single scalar value to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Graph title (e.g., \"loss\", \"accuracy\")</p> required <code>series</code> <code>str</code> <p>Series name within the graph (e.g., \"train\", \"val\")</p> required <code>value</code> <code>float</code> <p>Scalar value to log</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100) logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_scalar(self, title: str, series: str, value: float, iteration: int):\n    \"\"\"\n    Log a single scalar value to ClearML.\n\n    Args:\n        title: Graph title (e.g., \"loss\", \"accuracy\")\n        series: Series name within the graph (e.g., \"train\", \"val\")\n        value: Scalar value to log\n        iteration: Iteration/step number\n\n    Example:\n        logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100)\n        logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_scalar(\n        title=title,\n        series=series,\n        value=value,\n        iteration=iteration\n    )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.log_image","title":"log_image","text":"<pre><code>log_image(title, series, image, iteration)\n</code></pre> <p>Log an image to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name (e.g., \"predictions\", \"ground_truth\")</p> required <code>image</code> <p>Image as numpy array (H, W) or (H, W, C) for grayscale/RGB    Supports uint8 (0-255) or float (0-1)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_image(self, title: str, series: str, image, iteration: int):\n    \"\"\"\n    Log an image to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name (e.g., \"predictions\", \"ground_truth\")\n        image: Image as numpy array (H, W) or (H, W, C) for grayscale/RGB\n               Supports uint8 (0-255) or float (0-1)\n        iteration: Iteration/step number\n\n    Example:\n        # Grayscale image\n        img = np.eye(256, 256, dtype=np.uint8) * 255\n        logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)\n\n        # RGB image\n        img_rgb = np.zeros((256, 256, 3), dtype=np.uint8)\n        img_rgb[:, :, 0] = 255  # Red channel\n        logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_image(\n        title=title,\n        series=series,\n        iteration=iteration,\n        image=image\n    )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.log_image--grayscale-image","title":"Grayscale image","text":"<p>img = np.eye(256, 256, dtype=np.uint8) * 255 logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.log_image--rgb-image","title":"RGB image","text":"<p>img_rgb = np.zeros((256, 256, 3), dtype=np.uint8) img_rgb[:, :, 0] = 255  # Red channel logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.log_images_batch","title":"log_images_batch","text":"<pre><code>log_images_batch(title, series, images, iteration)\n</code></pre> <p>Log multiple images to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>images</code> <code>list</code> <p>List of images (numpy arrays)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>images = [img1, img2, img3] logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_images_batch(self, title: str, series: str, images: list, iteration: int):\n    \"\"\"\n    Log multiple images to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name\n        images: List of images (numpy arrays)\n        iteration: Iteration/step number\n\n    Example:\n        images = [img1, img2, img3]\n        logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    for idx, image in enumerate(images):\n        self.logger.report_image(\n            title=title,\n            series=f\"{series}_img_{idx}\",\n            iteration=iteration,\n            image=image\n        )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.log_text","title":"log_text","text":"<pre><code>log_text(text, title='Info')\n</code></pre> <p>Log text to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_text(self, text: str, title: str = \"Info\"):\n    \"\"\"Log text to ClearML.\"\"\"\n    if self.logger is None:\n        return\n    self.logger.report_text(text, print_console=True)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.report_text","title":"report_text","text":"<pre><code>report_text(text)\n</code></pre> <p>Report text to ClearML (alias for log_text with default title).</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_text(self, text: str):\n    \"\"\"Report text to ClearML (alias for log_text with default title).\"\"\"\n    self.log_text(text)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.report_plotly","title":"report_plotly","text":"<pre><code>report_plotly(title, series, iteration, figure)\n</code></pre> <p>Report a Plotly figure to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Plot title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required <code>figure</code> <p>Plotly figure object</p> required Example <p>import plotly.graph_objects as go fig = go.Figure() fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data')) fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\") logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_plotly(self, title: str, series: str, iteration: int, figure):\n    \"\"\"\n    Report a Plotly figure to ClearML.\n\n    Args:\n        title: Plot title/group\n        series: Series name\n        iteration: Iteration/step number\n        figure: Plotly figure object\n\n    Example:\n        import plotly.graph_objects as go\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data'))\n        fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\")\n        logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_plotly(\n        title=title,\n        series=series,\n        iteration=iteration,\n        figure=figure\n    )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.ClearMLLogger.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize logging and close task.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def finalize(self):\n    \"\"\"Finalize logging and close task.\"\"\"\n    if self.task:\n        self.task.close()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.setup_clearml","title":"setup_clearml","text":"<pre><code>setup_clearml(project_name, task_name, auto_connect=True)\n</code></pre> <p>Setup ClearML with credentials from config file.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>ClearML project name</p> required <code>task_name</code> <code>str</code> <p>ClearML task name</p> required <code>auto_connect</code> <code>bool</code> <p>If True, automatically connect frameworks</p> <code>True</code> <p>Returns:</p> Type Description <p>Task object</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def setup_clearml(project_name: str, task_name: str, auto_connect: bool = True):\n    \"\"\"\n    Setup ClearML with credentials from config file.\n\n    Args:\n        project_name: ClearML project name\n        task_name: ClearML task name\n        auto_connect: If True, automatically connect frameworks\n\n    Returns:\n        Task object\n    \"\"\"\n    # Load credentials\n    config_dir = Path(__file__).parent.parent / 'configs'\n\n    try:\n        creds = load_credentials(config_dir)\n\n        # Set credentials\n        clearml.Task.set_credentials(\n            api_host=creds.get('api_host', 'https://api.clear.ml'),\n            web_host=creds.get('web_host', 'https://app.clear.ml'),\n            files_host=creds.get('files_host', 'https://files.clear.ml'),\n            key=creds['api_key'],\n            secret=creds['api_secret']\n        )\n\n        # Initialize task\n        task = Task.init(\n            project_name=project_name,\n            task_name=task_name,\n            auto_connect_frameworks=auto_connect\n        )\n        return task\n    except FileNotFoundError as e:\n        print(f\"Warning: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n    except Exception as e:\n        print(f\"Warning: Failed to setup ClearML: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n</code></pre>"},{"location":"api/all/#embeddings_squeeze-modules","title":"Modules","text":""},{"location":"api/all/#embeddings_squeeze.cli","title":"cli","text":"<p>CLI module for embeddings_squeeze package.</p>"},{"location":"api/all/#embeddings_squeeze.cli-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.cli.squeeze","title":"squeeze","text":"<pre><code>squeeze()\n</code></pre> <p>Entry point for squeeze command.</p> Source code in <code>embeddings_squeeze\\cli.py</code> <pre><code>def squeeze():\n    \"\"\"Entry point for squeeze command.\"\"\"\n    squeeze_main()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.configs","title":"configs","text":"<p>Configuration management for the package.</p>"},{"location":"api/all/#embeddings_squeeze.configs-classes","title":"Classes","text":""},{"location":"api/all/#embeddings_squeeze.configs.ModelConfig","title":"ModelConfig  <code>dataclass</code>","text":"<pre><code>ModelConfig(\n    backbone=\"vit\",\n    num_classes=21,\n    freeze_backbone=True,\n    vit_weights=\"IMAGENET1K_V1\",\n    deeplab_weights=\"COCO_WITH_VOC_LABELS_V1\",\n    add_adapter=False,\n    feature_dim=768,\n    loss_type=\"ce\",\n    class_weights=None,\n)\n</code></pre> <p>Model architecture configuration.</p>"},{"location":"api/all/#embeddings_squeeze.configs.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    epochs=10,\n    batch_size=4,\n    max_batches=None,\n    learning_rate=0.0001,\n    vq_loss_weight=0.1,\n    num_workers=4,\n    pin_memory=True,\n    optimizer=\"adam\",\n    weight_decay=0.0,\n    log_every_n_steps=50,\n    val_check_interval=1.0,\n    save_top_k=3,\n    monitor=\"val/loss\",\n    mode=\"min\",\n)\n</code></pre> <p>Training configuration.</p>"},{"location":"api/all/#embeddings_squeeze.configs.DataConfig","title":"DataConfig  <code>dataclass</code>","text":"<pre><code>DataConfig(\n    dataset=\"oxford_pet\",\n    data_path=\"./data\",\n    image_size=224,\n    subset_size=None,\n    normalize_mean=(0.485, 0.456, 0.406),\n    normalize_std=(0.229, 0.224, 0.225),\n)\n</code></pre> <p>Data configuration.</p>"},{"location":"api/all/#embeddings_squeeze.configs.ExperimentConfig","title":"ExperimentConfig  <code>dataclass</code>","text":"<pre><code>ExperimentConfig(\n    model=ModelConfig(),\n    training=TrainingConfig(),\n    data=DataConfig(),\n    quantizer=QuantizerConfig(),\n    logger=LoggerConfig(),\n    experiment_name=\"vq_squeeze\",\n    output_dir=\"./outputs\",\n    seed=42,\n    initialize_codebook=True,\n    max_init_samples=50000,\n)\n</code></pre> <p>Complete experiment configuration.</p>"},{"location":"api/all/#embeddings_squeeze.configs-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.configs.get_default_config","title":"get_default_config","text":"<pre><code>get_default_config()\n</code></pre> <p>Get default configuration.</p> Source code in <code>embeddings_squeeze\\configs\\default.py</code> <pre><code>def get_default_config() -&gt; ExperimentConfig:\n    \"\"\"Get default configuration.\"\"\"\n    return ExperimentConfig()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.configs-modules","title":"Modules","text":""},{"location":"api/all/#embeddings_squeeze.configs.default","title":"default","text":"<p>Default configuration classes and settings.</p>"},{"location":"api/all/#embeddings_squeeze.configs.default-classes","title":"Classes","text":"QuantizerConfig <code>dataclass</code> \u00b6 <pre><code>QuantizerConfig(\n    enabled=True,\n    type=\"vq\",\n    codebook_size=512,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n    levels=(lambda: [8, 5, 5, 5])(),\n    entropy_loss_weight=0.1,\n    diversity_gamma=0.1,\n    spherical=False,\n    num_quantizers=4,\n)\n</code></pre> <p>Quantizer configuration.</p> LoggerConfig <code>dataclass</code> \u00b6 <pre><code>LoggerConfig(\n    use_clearml=True,\n    use_tensorboard=False,\n    project_name=\"embeddings_squeeze\",\n    task_name=\"vq_compression\",\n    credentials_file=\"clearml_credentials.yaml\",\n)\n</code></pre> <p>Logger configuration.</p> ModelConfig <code>dataclass</code> \u00b6 <pre><code>ModelConfig(\n    backbone=\"vit\",\n    num_classes=21,\n    freeze_backbone=True,\n    vit_weights=\"IMAGENET1K_V1\",\n    deeplab_weights=\"COCO_WITH_VOC_LABELS_V1\",\n    add_adapter=False,\n    feature_dim=768,\n    loss_type=\"ce\",\n    class_weights=None,\n)\n</code></pre> <p>Model architecture configuration.</p> TrainingConfig <code>dataclass</code> \u00b6 <pre><code>TrainingConfig(\n    epochs=10,\n    batch_size=4,\n    max_batches=None,\n    learning_rate=0.0001,\n    vq_loss_weight=0.1,\n    num_workers=4,\n    pin_memory=True,\n    optimizer=\"adam\",\n    weight_decay=0.0,\n    log_every_n_steps=50,\n    val_check_interval=1.0,\n    save_top_k=3,\n    monitor=\"val/loss\",\n    mode=\"min\",\n)\n</code></pre> <p>Training configuration.</p> DataConfig <code>dataclass</code> \u00b6 <pre><code>DataConfig(\n    dataset=\"oxford_pet\",\n    data_path=\"./data\",\n    image_size=224,\n    subset_size=None,\n    normalize_mean=(0.485, 0.456, 0.406),\n    normalize_std=(0.229, 0.224, 0.225),\n)\n</code></pre> <p>Data configuration.</p> ExperimentConfig <code>dataclass</code> \u00b6 <pre><code>ExperimentConfig(\n    model=ModelConfig(),\n    training=TrainingConfig(),\n    data=DataConfig(),\n    quantizer=QuantizerConfig(),\n    logger=LoggerConfig(),\n    experiment_name=\"vq_squeeze\",\n    output_dir=\"./outputs\",\n    seed=42,\n    initialize_codebook=True,\n    max_init_samples=50000,\n)\n</code></pre> <p>Complete experiment configuration.</p>"},{"location":"api/all/#embeddings_squeeze.configs.default-functions","title":"Functions","text":"get_default_config \u00b6 <pre><code>get_default_config()\n</code></pre> <p>Get default configuration.</p> Source code in <code>embeddings_squeeze\\configs\\default.py</code> <pre><code>def get_default_config() -&gt; ExperimentConfig:\n    \"\"\"Get default configuration.\"\"\"\n    return ExperimentConfig()\n</code></pre> update_config_from_args \u00b6 <pre><code>update_config_from_args(config, args)\n</code></pre> <p>Update configuration from command line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ExperimentConfig</code> <p>Base configuration</p> required <code>args</code> <code>Dict[str, Any]</code> <p>Command line arguments</p> required <p>Returns:</p> Type Description <code>ExperimentConfig</code> <p>Updated configuration</p> Source code in <code>embeddings_squeeze\\configs\\default.py</code> <pre><code>def update_config_from_args(config: ExperimentConfig, args: Dict[str, Any]) -&gt; ExperimentConfig:\n    \"\"\"\n    Update configuration from command line arguments.\n\n    Args:\n        config: Base configuration\n        args: Command line arguments\n\n    Returns:\n        Updated configuration\n    \"\"\"\n    # Model config\n    if \"model\" in args:\n        config.model.backbone = args[\"model\"]\n    if \"num_classes\" in args:\n        config.model.num_classes = args[\"num_classes\"]\n    if \"add_adapter\" in args:\n        config.model.add_adapter = args[\"add_adapter\"]\n    if \"feature_dim\" in args and args[\"feature_dim\"] is not None:\n        config.model.feature_dim = args[\"feature_dim\"]\n    if \"loss_type\" in args:\n        config.model.loss_type = args[\"loss_type\"]\n    if \"class_weights\" in args:\n        config.model.class_weights = args[\"class_weights\"]\n\n    # Quantizer config\n    if \"quantizer_type\" in args:\n        config.quantizer.type = args[\"quantizer_type\"]\n    if \"quantizer_enabled\" in args:\n        config.quantizer.enabled = args[\"quantizer_enabled\"]\n    if \"codebook_size\" in args:\n        config.quantizer.codebook_size = args[\"codebook_size\"]\n    if \"bottleneck_dim\" in args:\n        config.quantizer.bottleneck_dim = args[\"bottleneck_dim\"]\n    if \"num_quantizers\" in args:\n        config.quantizer.num_quantizers = args[\"num_quantizers\"]\n\n    # Logger config\n    if \"use_clearml\" in args:\n        config.logger.use_clearml = args[\"use_clearml\"]\n    if \"project_name\" in args:\n        config.logger.project_name = args[\"project_name\"]\n    if \"task_name\" in args:\n        config.logger.task_name = args[\"task_name\"]\n\n    # Training config\n    if \"epochs\" in args:\n        config.training.epochs = args[\"epochs\"]\n    if \"batch_size\" in args:\n        config.training.batch_size = args[\"batch_size\"]\n    if \"max_batches\" in args:\n        config.training.max_batches = args[\"max_batches\"]\n    if \"lr\" in args:\n        config.training.learning_rate = args[\"lr\"]\n    if \"vq_loss_weight\" in args:\n        config.training.vq_loss_weight = args[\"vq_loss_weight\"]\n\n    # Data config\n    if \"dataset\" in args:\n        config.data.dataset = args[\"dataset\"]\n    if \"data_path\" in args:\n        config.data.data_path = args[\"data_path\"]\n    if \"subset_size\" in args:\n        config.data.subset_size = args[\"subset_size\"]\n\n    # Experiment config\n    if \"output_dir\" in args:\n        config.output_dir = args[\"output_dir\"]\n    if \"experiment_name\" in args:\n        config.experiment_name = args[\"experiment_name\"]\n    if \"seed\" in args:\n        config.seed = args[\"seed\"]\n    if \"initialize_codebook\" in args:\n        # argparse provides this key with a boolean even if the flag is not passed\n        config.initialize_codebook = args[\"initialize_codebook\"]\n    if \"max_init_samples\" in args and args[\"max_init_samples\"] is not None:\n        config.max_init_samples = args[\"max_init_samples\"]\n\n    return config\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.data","title":"data","text":"<p>Data modules for different datasets.</p>"},{"location":"api/all/#embeddings_squeeze.data-classes","title":"Classes","text":""},{"location":"api/all/#embeddings_squeeze.data.BaseDataModule","title":"BaseDataModule","text":"<pre><code>BaseDataModule(\n    data_path,\n    batch_size=4,\n    num_workers=0,\n    pin_memory=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningDataModule</code>, <code>ABC</code></p> <p>Abstract base class for data modules.</p> <p>All dataset-specific data modules should inherit from this class.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    pin_memory: bool = True,\n    **kwargs\n):\n    super().__init__()\n    self.data_path = data_path\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    self.pin_memory = pin_memory\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.data.BaseDataModule-functions","title":"Functions","text":"setup <code>abstractmethod</code> \u00b6 <pre><code>setup(stage=None)\n</code></pre> <p>Setup datasets for training/validation/testing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>'fit', 'validate', 'test', or None</p> <code>None</code> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef setup(self, stage: str = None):\n    \"\"\"\n    Setup datasets for training/validation/testing.\n\n    Args:\n        stage: 'fit', 'validate', 'test', or None\n    \"\"\"\n    pass\n</code></pre> train_dataloader <code>abstractmethod</code> \u00b6 <pre><code>train_dataloader(max_batches=None)\n</code></pre> <p>Return training dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef train_dataloader(self, max_batches: int = None):\n    \"\"\"Return training dataloader.\"\"\"\n    pass\n</code></pre> val_dataloader <code>abstractmethod</code> \u00b6 <pre><code>val_dataloader(max_batches=None)\n</code></pre> <p>Return validation dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef val_dataloader(self, max_batches: int = None):\n    \"\"\"Return validation dataloader.\"\"\"\n    pass\n</code></pre> test_dataloader <code>abstractmethod</code> \u00b6 <pre><code>test_dataloader(max_batches=None)\n</code></pre> <p>Return test dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef test_dataloader(self, max_batches: int = None):\n    \"\"\"Return test dataloader.\"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.data.OxfordPetDataModule","title":"OxfordPetDataModule","text":"<pre><code>OxfordPetDataModule(\n    data_path=\"./data\",\n    batch_size=4,\n    num_workers=4,\n    pin_memory=True,\n    image_size=224,\n    subset_size=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseDataModule</code></p> <p>Data module for Oxford-IIIT Pet segmentation dataset.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def __init__(\n    self,\n    data_path: str = './data',\n    batch_size: int = 4,\n    num_workers: int = 4,\n    pin_memory: bool = True,\n    image_size: int = 224,\n    subset_size: int = None,\n    **kwargs\n):\n    super().__init__(data_path, batch_size, num_workers, pin_memory, **kwargs)\n\n    self.image_size = image_size\n    self.subset_size = subset_size\n\n    # Define transforms\n    self.transform_image = transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.CenterCrop(image_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    self.transform_mask = transforms.Compose([\n        transforms.Resize(image_size, interpolation=transforms.InterpolationMode.NEAREST),\n        transforms.CenterCrop(image_size),\n        transforms.PILToTensor()\n    ])\n\n    # Dataset attributes\n    self.train_dataset = None\n    self.val_dataset = None\n    self.test_dataset = None\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.data.OxfordPetDataModule-functions","title":"Functions","text":"setup \u00b6 <pre><code>setup(stage=None)\n</code></pre> <p>Setup datasets.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def setup(self, stage: str = None):\n    \"\"\"Setup datasets.\"\"\"\n    if stage == 'fit' or stage is None:\n        # Check if dataset exists\n        pet_path = os.path.join(self.data_path, 'oxford-iiit-pet')\n        need_download = not os.path.exists(pet_path)\n\n        # Load full dataset\n        pet_dataset = OxfordIIITPet(\n            root=self.data_path,\n            split='trainval',\n            target_types='segmentation',\n            download=need_download\n        )\n\n        # Wrap with transforms\n        wrapped_dataset = PetSegmentationDataset(\n            pet_dataset, self.transform_image, self.transform_mask\n        )\n\n        # Create subset if specified\n        if self.subset_size is not None:\n            wrapped_dataset = Subset(wrapped_dataset, range(self.subset_size))\n\n        # Split into train/val (80/20)\n        total_size = len(wrapped_dataset)\n        train_size = int(0.8 * total_size)\n\n        self.train_dataset = Subset(wrapped_dataset, range(train_size))\n        self.val_dataset = Subset(wrapped_dataset, range(train_size, total_size))\n\n    if stage == 'test' or stage is None:\n        # Load test dataset\n        pet_dataset = OxfordIIITPet(\n            root=self.data_path,\n            split='test',\n            target_types='segmentation',\n            download=False\n        )\n\n        wrapped_dataset = PetSegmentationDataset(\n            pet_dataset, self.transform_image, self.transform_mask\n        )\n\n        if self.subset_size is not None:\n            wrapped_dataset = Subset(wrapped_dataset, range(min(self.subset_size, len(wrapped_dataset))))\n\n        self.test_dataset = wrapped_dataset\n</code></pre> train_dataloader \u00b6 <pre><code>train_dataloader(max_batches=None)\n</code></pre> <p>Return training dataloader.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def train_dataloader(self, max_batches: int = None):\n    \"\"\"Return training dataloader.\"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=max_batches is not None\n    )\n</code></pre> val_dataloader \u00b6 <pre><code>val_dataloader(max_batches=None)\n</code></pre> <p>Return validation dataloader.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def val_dataloader(self, max_batches: int = None):\n    \"\"\"Return validation dataloader.\"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=max_batches is not None\n    )\n</code></pre> test_dataloader \u00b6 <pre><code>test_dataloader(max_batches=None)\n</code></pre> <p>Return test dataloader.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def test_dataloader(self, max_batches: int = None):\n    \"\"\"Return test dataloader.\"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=max_batches is not None\n    )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.data-modules","title":"Modules","text":""},{"location":"api/all/#embeddings_squeeze.data.base","title":"base","text":"<p>Base data module for PyTorch Lightning.</p>"},{"location":"api/all/#embeddings_squeeze.data.base-classes","title":"Classes","text":"BaseDataModule \u00b6 <pre><code>BaseDataModule(\n    data_path,\n    batch_size=4,\n    num_workers=0,\n    pin_memory=True,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningDataModule</code>, <code>ABC</code></p> <p>Abstract base class for data modules.</p> <p>All dataset-specific data modules should inherit from this class.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    pin_memory: bool = True,\n    **kwargs\n):\n    super().__init__()\n    self.data_path = data_path\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    self.pin_memory = pin_memory\n</code></pre> Functions\u00b6 setup <code>abstractmethod</code> \u00b6 <pre><code>setup(stage=None)\n</code></pre> <p>Setup datasets for training/validation/testing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>'fit', 'validate', 'test', or None</p> <code>None</code> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef setup(self, stage: str = None):\n    \"\"\"\n    Setup datasets for training/validation/testing.\n\n    Args:\n        stage: 'fit', 'validate', 'test', or None\n    \"\"\"\n    pass\n</code></pre> train_dataloader <code>abstractmethod</code> \u00b6 <pre><code>train_dataloader(max_batches=None)\n</code></pre> <p>Return training dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef train_dataloader(self, max_batches: int = None):\n    \"\"\"Return training dataloader.\"\"\"\n    pass\n</code></pre> val_dataloader <code>abstractmethod</code> \u00b6 <pre><code>val_dataloader(max_batches=None)\n</code></pre> <p>Return validation dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef val_dataloader(self, max_batches: int = None):\n    \"\"\"Return validation dataloader.\"\"\"\n    pass\n</code></pre> test_dataloader <code>abstractmethod</code> \u00b6 <pre><code>test_dataloader(max_batches=None)\n</code></pre> <p>Return test dataloader.</p> Source code in <code>embeddings_squeeze\\data\\base.py</code> <pre><code>@abstractmethod\ndef test_dataloader(self, max_batches: int = None):\n    \"\"\"Return test dataloader.\"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.data.oxford_pet","title":"oxford_pet","text":"<p>Oxford-IIIT Pet dataset data module.</p>"},{"location":"api/all/#embeddings_squeeze.data.oxford_pet-classes","title":"Classes","text":"PetSegmentationDataset \u00b6 <pre><code>PetSegmentationDataset(\n    pet_dataset, transform_image, transform_mask\n)\n</code></pre> <p>Wrapper for Oxford-IIIT Pet dataset with proper transforms.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def __init__(self, pet_dataset, transform_image, transform_mask):\n    self.dataset = pet_dataset\n    self.transform_image = transform_image\n    self.transform_mask = transform_mask\n</code></pre> OxfordPetDataModule \u00b6 <pre><code>OxfordPetDataModule(\n    data_path=\"./data\",\n    batch_size=4,\n    num_workers=4,\n    pin_memory=True,\n    image_size=224,\n    subset_size=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseDataModule</code></p> <p>Data module for Oxford-IIIT Pet segmentation dataset.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def __init__(\n    self,\n    data_path: str = './data',\n    batch_size: int = 4,\n    num_workers: int = 4,\n    pin_memory: bool = True,\n    image_size: int = 224,\n    subset_size: int = None,\n    **kwargs\n):\n    super().__init__(data_path, batch_size, num_workers, pin_memory, **kwargs)\n\n    self.image_size = image_size\n    self.subset_size = subset_size\n\n    # Define transforms\n    self.transform_image = transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.CenterCrop(image_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    self.transform_mask = transforms.Compose([\n        transforms.Resize(image_size, interpolation=transforms.InterpolationMode.NEAREST),\n        transforms.CenterCrop(image_size),\n        transforms.PILToTensor()\n    ])\n\n    # Dataset attributes\n    self.train_dataset = None\n    self.val_dataset = None\n    self.test_dataset = None\n</code></pre> Functions\u00b6 setup \u00b6 <pre><code>setup(stage=None)\n</code></pre> <p>Setup datasets.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def setup(self, stage: str = None):\n    \"\"\"Setup datasets.\"\"\"\n    if stage == 'fit' or stage is None:\n        # Check if dataset exists\n        pet_path = os.path.join(self.data_path, 'oxford-iiit-pet')\n        need_download = not os.path.exists(pet_path)\n\n        # Load full dataset\n        pet_dataset = OxfordIIITPet(\n            root=self.data_path,\n            split='trainval',\n            target_types='segmentation',\n            download=need_download\n        )\n\n        # Wrap with transforms\n        wrapped_dataset = PetSegmentationDataset(\n            pet_dataset, self.transform_image, self.transform_mask\n        )\n\n        # Create subset if specified\n        if self.subset_size is not None:\n            wrapped_dataset = Subset(wrapped_dataset, range(self.subset_size))\n\n        # Split into train/val (80/20)\n        total_size = len(wrapped_dataset)\n        train_size = int(0.8 * total_size)\n\n        self.train_dataset = Subset(wrapped_dataset, range(train_size))\n        self.val_dataset = Subset(wrapped_dataset, range(train_size, total_size))\n\n    if stage == 'test' or stage is None:\n        # Load test dataset\n        pet_dataset = OxfordIIITPet(\n            root=self.data_path,\n            split='test',\n            target_types='segmentation',\n            download=False\n        )\n\n        wrapped_dataset = PetSegmentationDataset(\n            pet_dataset, self.transform_image, self.transform_mask\n        )\n\n        if self.subset_size is not None:\n            wrapped_dataset = Subset(wrapped_dataset, range(min(self.subset_size, len(wrapped_dataset))))\n\n        self.test_dataset = wrapped_dataset\n</code></pre> train_dataloader \u00b6 <pre><code>train_dataloader(max_batches=None)\n</code></pre> <p>Return training dataloader.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def train_dataloader(self, max_batches: int = None):\n    \"\"\"Return training dataloader.\"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=max_batches is not None\n    )\n</code></pre> val_dataloader \u00b6 <pre><code>val_dataloader(max_batches=None)\n</code></pre> <p>Return validation dataloader.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def val_dataloader(self, max_batches: int = None):\n    \"\"\"Return validation dataloader.\"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=max_batches is not None\n    )\n</code></pre> test_dataloader \u00b6 <pre><code>test_dataloader(max_batches=None)\n</code></pre> <p>Return test dataloader.</p> Source code in <code>embeddings_squeeze\\data\\oxford_pet.py</code> <pre><code>def test_dataloader(self, max_batches: int = None):\n    \"\"\"Return test dataloader.\"\"\"\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n        drop_last=max_batches is not None\n    )\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.loggers","title":"loggers","text":"<p>Logger integrations for embeddings_squeeze.</p>"},{"location":"api/all/#embeddings_squeeze.loggers-classes","title":"Classes","text":""},{"location":"api/all/#embeddings_squeeze.loggers.ClearMLLogger","title":"ClearMLLogger","text":"<pre><code>ClearMLLogger(task)\n</code></pre> <p>Wrapper for ClearML logging compatible with PyTorch Lightning. Supports scalar metrics, plots, images, and text logging.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def __init__(self, task: Task):\n    self.task = task\n    self.logger = task.get_logger() if task else None\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.loggers.ClearMLLogger-functions","title":"Functions","text":"log_metrics \u00b6 <pre><code>log_metrics(metrics, step=None)\n</code></pre> <p>Log metrics to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: int = None):\n    \"\"\"Log metrics to ClearML.\"\"\"\n    if self.logger is None:\n        return\n\n    for key, value in metrics.items():\n        # Split key into title and series (e.g., \"train/loss\" -&gt; title=\"train\", series=\"loss\")\n        if '/' in key:\n            title, series = key.split('/', 1)\n        else:\n            title = 'metrics'\n            series = key\n\n        self.logger.report_scalar(\n            title=title,\n            series=series,\n            value=value,\n            iteration=step\n        )\n</code></pre> log_scalar \u00b6 <pre><code>log_scalar(title, series, value, iteration)\n</code></pre> <p>Log a single scalar value to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Graph title (e.g., \"loss\", \"accuracy\")</p> required <code>series</code> <code>str</code> <p>Series name within the graph (e.g., \"train\", \"val\")</p> required <code>value</code> <code>float</code> <p>Scalar value to log</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100) logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_scalar(self, title: str, series: str, value: float, iteration: int):\n    \"\"\"\n    Log a single scalar value to ClearML.\n\n    Args:\n        title: Graph title (e.g., \"loss\", \"accuracy\")\n        series: Series name within the graph (e.g., \"train\", \"val\")\n        value: Scalar value to log\n        iteration: Iteration/step number\n\n    Example:\n        logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100)\n        logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_scalar(\n        title=title,\n        series=series,\n        value=value,\n        iteration=iteration\n    )\n</code></pre> log_image \u00b6 <pre><code>log_image(title, series, image, iteration)\n</code></pre> <p>Log an image to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name (e.g., \"predictions\", \"ground_truth\")</p> required <code>image</code> <p>Image as numpy array (H, W) or (H, W, C) for grayscale/RGB    Supports uint8 (0-255) or float (0-1)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_image(self, title: str, series: str, image, iteration: int):\n    \"\"\"\n    Log an image to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name (e.g., \"predictions\", \"ground_truth\")\n        image: Image as numpy array (H, W) or (H, W, C) for grayscale/RGB\n               Supports uint8 (0-255) or float (0-1)\n        iteration: Iteration/step number\n\n    Example:\n        # Grayscale image\n        img = np.eye(256, 256, dtype=np.uint8) * 255\n        logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)\n\n        # RGB image\n        img_rgb = np.zeros((256, 256, 3), dtype=np.uint8)\n        img_rgb[:, :, 0] = 255  # Red channel\n        logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_image(\n        title=title,\n        series=series,\n        iteration=iteration,\n        image=image\n    )\n</code></pre> log_images_batch \u00b6 <pre><code>log_images_batch(title, series, images, iteration)\n</code></pre> <p>Log multiple images to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>images</code> <code>list</code> <p>List of images (numpy arrays)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>images = [img1, img2, img3] logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_images_batch(self, title: str, series: str, images: list, iteration: int):\n    \"\"\"\n    Log multiple images to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name\n        images: List of images (numpy arrays)\n        iteration: Iteration/step number\n\n    Example:\n        images = [img1, img2, img3]\n        logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    for idx, image in enumerate(images):\n        self.logger.report_image(\n            title=title,\n            series=f\"{series}_img_{idx}\",\n            iteration=iteration,\n            image=image\n        )\n</code></pre> log_text \u00b6 <pre><code>log_text(text, title='Info')\n</code></pre> <p>Log text to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_text(self, text: str, title: str = \"Info\"):\n    \"\"\"Log text to ClearML.\"\"\"\n    if self.logger is None:\n        return\n    self.logger.report_text(text, print_console=True)\n</code></pre> report_text \u00b6 <pre><code>report_text(text)\n</code></pre> <p>Report text to ClearML (alias for log_text with default title).</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_text(self, text: str):\n    \"\"\"Report text to ClearML (alias for log_text with default title).\"\"\"\n    self.log_text(text)\n</code></pre> report_plotly \u00b6 <pre><code>report_plotly(title, series, iteration, figure)\n</code></pre> <p>Report a Plotly figure to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Plot title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required <code>figure</code> <p>Plotly figure object</p> required Example <p>import plotly.graph_objects as go fig = go.Figure() fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data')) fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\") logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_plotly(self, title: str, series: str, iteration: int, figure):\n    \"\"\"\n    Report a Plotly figure to ClearML.\n\n    Args:\n        title: Plot title/group\n        series: Series name\n        iteration: Iteration/step number\n        figure: Plotly figure object\n\n    Example:\n        import plotly.graph_objects as go\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data'))\n        fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\")\n        logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_plotly(\n        title=title,\n        series=series,\n        iteration=iteration,\n        figure=figure\n    )\n</code></pre> finalize \u00b6 <pre><code>finalize()\n</code></pre> <p>Finalize logging and close task.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def finalize(self):\n    \"\"\"Finalize logging and close task.\"\"\"\n    if self.task:\n        self.task.close()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.loggers.ClearMLLogger.log_image--grayscale-image","title":"Grayscale image","text":"<p>img = np.eye(256, 256, dtype=np.uint8) * 255 logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.ClearMLLogger.log_image--rgb-image","title":"RGB image","text":"<p>img_rgb = np.zeros((256, 256, 3), dtype=np.uint8) img_rgb[:, :, 0] = 255  # Red channel logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.ClearMLUploadCallback","title":"ClearMLUploadCallback","text":"<pre><code>ClearMLUploadCallback(\n    task,\n    clearml_logger=None,\n    checkpoint_dir=\"checkpoints\",\n    embedding_dir=\"embeddings\",\n)\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>PyTorch Lightning callback for logging checkpoint and embedding paths to ClearML.</p> <p>Automatically logs local file paths for: - Latest checkpoint after each validation epoch - Per-epoch validation embeddings</p> Usage <p>from pytorch_lightning import Trainer from embeddings_squeeze.loggers import ClearMLUploadCallback, setup_clearml</p> <p>task = setup_clearml(project_name=\"my_project\", task_name=\"experiment_1\") logger = ClearMLLogger(task) if task else None callback = ClearMLUploadCallback(task, logger, checkpoint_dir=\"checkpoints\")</p> <p>trainer = Trainer(callbacks=[callback], ...)</p> <p>Initialize ClearML path logging callback.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>ClearML Task object</p> required <code>clearml_logger</code> <code>ClearMLLogger</code> <p>ClearML logger for text reporting (optional)</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>Directory containing checkpoints</p> <code>'checkpoints'</code> <code>embedding_dir</code> <code>str</code> <p>Directory containing embeddings</p> <code>'embeddings'</code> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def __init__(self, task: Task, clearml_logger: ClearMLLogger = None, \n             checkpoint_dir: str = \"checkpoints\", embedding_dir: str = \"embeddings\"):\n    \"\"\"\n    Initialize ClearML path logging callback.\n\n    Args:\n        task: ClearML Task object\n        clearml_logger: ClearML logger for text reporting (optional)\n        checkpoint_dir: Directory containing checkpoints\n        embedding_dir: Directory containing embeddings\n    \"\"\"\n    super().__init__()\n    self.task = task\n    self.clearml_logger = clearml_logger\n    self.checkpoint_dir = checkpoint_dir\n    self.embedding_dir = embedding_dir\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.loggers.ClearMLUploadCallback-functions","title":"Functions","text":"on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end(trainer, pl_module)\n</code></pre> <p>Called after validation epoch ends.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n    \"\"\"Called after validation epoch ends.\"\"\"\n    if self.task is None:\n        return\n\n    # Log checkpoint path\n    try:\n        ckpt_path = self._find_latest_checkpoint()\n        if ckpt_path:\n            abs_path = os.path.abspath(ckpt_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Checkpoint saved: {abs_path}\")\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed finding checkpoint: {e}\")\n\n    # Log embedding path\n    try:\n        emb_path = os.path.join(\n            self.embedding_dir, \n            f\"val_embedding_epoch{pl_module.current_epoch}.pt\"\n        )\n        if os.path.exists(emb_path):\n            abs_path = os.path.abspath(emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Embedding saved: {abs_path}\")\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed logging embedding path: {e}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.loggers-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.loggers.setup_clearml","title":"setup_clearml","text":"<pre><code>setup_clearml(project_name, task_name, auto_connect=True)\n</code></pre> <p>Setup ClearML with credentials from config file.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>ClearML project name</p> required <code>task_name</code> <code>str</code> <p>ClearML task name</p> required <code>auto_connect</code> <code>bool</code> <p>If True, automatically connect frameworks</p> <code>True</code> <p>Returns:</p> Type Description <p>Task object</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def setup_clearml(project_name: str, task_name: str, auto_connect: bool = True):\n    \"\"\"\n    Setup ClearML with credentials from config file.\n\n    Args:\n        project_name: ClearML project name\n        task_name: ClearML task name\n        auto_connect: If True, automatically connect frameworks\n\n    Returns:\n        Task object\n    \"\"\"\n    # Load credentials\n    config_dir = Path(__file__).parent.parent / 'configs'\n\n    try:\n        creds = load_credentials(config_dir)\n\n        # Set credentials\n        clearml.Task.set_credentials(\n            api_host=creds.get('api_host', 'https://api.clear.ml'),\n            web_host=creds.get('web_host', 'https://app.clear.ml'),\n            files_host=creds.get('files_host', 'https://files.clear.ml'),\n            key=creds['api_key'],\n            secret=creds['api_secret']\n        )\n\n        # Initialize task\n        task = Task.init(\n            project_name=project_name,\n            task_name=task_name,\n            auto_connect_frameworks=auto_connect\n        )\n        return task\n    except FileNotFoundError as e:\n        print(f\"Warning: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n    except Exception as e:\n        print(f\"Warning: Failed to setup ClearML: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.loggers-modules","title":"Modules","text":""},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger","title":"clearml_logger","text":"<p>ClearML logger integration with credentials management.</p> Usage Examples"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger--setup-clearml","title":"Setup ClearML","text":"<p>task = setup_clearml(project_name=\"my_project\", task_name=\"experiment_1\") logger = ClearMLLogger(task)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger--log-scalar-metrics-creates-unified-graphs","title":"Log scalar metrics (creates unified graphs)","text":"<p>for i in range(100):     logger.log_scalar(\"loss\", \"train\", 1.0/(i+1), iteration=i)     logger.log_scalar(\"loss\", \"val\", 0.5/(i+1), iteration=i)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger--log-images-grayscale","title":"Log images (grayscale)","text":"<p>import numpy as np img = np.eye(256, 256, dtype=np.uint8) * 255 logger.log_image(\"predictions\", \"sample_1\", img, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger--log-rgb-images","title":"Log RGB images","text":"<p>img_rgb = np.zeros((256, 256, 3), dtype=np.uint8) img_rgb[:, :, 0] = 255  # Red channel logger.log_image(\"predictions\", \"sample_rgb\", img_rgb, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger--log-multiple-images-at-once","title":"Log multiple images at once","text":"<p>images = [img1, img2, img3] logger.log_images_batch(\"batch_samples\", \"epoch_1\", images, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger--log-text","title":"Log text","text":"<p>logger.log_text(\"Training started successfully!\")</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger--finalize","title":"Finalize","text":"<p>logger.finalize()</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger-classes","title":"Classes","text":"ClearMLLogger \u00b6 <pre><code>ClearMLLogger(task)\n</code></pre> <p>Wrapper for ClearML logging compatible with PyTorch Lightning. Supports scalar metrics, plots, images, and text logging.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def __init__(self, task: Task):\n    self.task = task\n    self.logger = task.get_logger() if task else None\n</code></pre> Functions\u00b6 log_metrics \u00b6 <pre><code>log_metrics(metrics, step=None)\n</code></pre> <p>Log metrics to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: int = None):\n    \"\"\"Log metrics to ClearML.\"\"\"\n    if self.logger is None:\n        return\n\n    for key, value in metrics.items():\n        # Split key into title and series (e.g., \"train/loss\" -&gt; title=\"train\", series=\"loss\")\n        if '/' in key:\n            title, series = key.split('/', 1)\n        else:\n            title = 'metrics'\n            series = key\n\n        self.logger.report_scalar(\n            title=title,\n            series=series,\n            value=value,\n            iteration=step\n        )\n</code></pre> log_scalar \u00b6 <pre><code>log_scalar(title, series, value, iteration)\n</code></pre> <p>Log a single scalar value to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Graph title (e.g., \"loss\", \"accuracy\")</p> required <code>series</code> <code>str</code> <p>Series name within the graph (e.g., \"train\", \"val\")</p> required <code>value</code> <code>float</code> <p>Scalar value to log</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100) logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_scalar(self, title: str, series: str, value: float, iteration: int):\n    \"\"\"\n    Log a single scalar value to ClearML.\n\n    Args:\n        title: Graph title (e.g., \"loss\", \"accuracy\")\n        series: Series name within the graph (e.g., \"train\", \"val\")\n        value: Scalar value to log\n        iteration: Iteration/step number\n\n    Example:\n        logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100)\n        logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_scalar(\n        title=title,\n        series=series,\n        value=value,\n        iteration=iteration\n    )\n</code></pre> log_image \u00b6 <pre><code>log_image(title, series, image, iteration)\n</code></pre> <p>Log an image to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name (e.g., \"predictions\", \"ground_truth\")</p> required <code>image</code> <p>Image as numpy array (H, W) or (H, W, C) for grayscale/RGB    Supports uint8 (0-255) or float (0-1)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_image(self, title: str, series: str, image, iteration: int):\n    \"\"\"\n    Log an image to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name (e.g., \"predictions\", \"ground_truth\")\n        image: Image as numpy array (H, W) or (H, W, C) for grayscale/RGB\n               Supports uint8 (0-255) or float (0-1)\n        iteration: Iteration/step number\n\n    Example:\n        # Grayscale image\n        img = np.eye(256, 256, dtype=np.uint8) * 255\n        logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)\n\n        # RGB image\n        img_rgb = np.zeros((256, 256, 3), dtype=np.uint8)\n        img_rgb[:, :, 0] = 255  # Red channel\n        logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_image(\n        title=title,\n        series=series,\n        iteration=iteration,\n        image=image\n    )\n</code></pre> log_images_batch \u00b6 <pre><code>log_images_batch(title, series, images, iteration)\n</code></pre> <p>Log multiple images to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>images</code> <code>list</code> <p>List of images (numpy arrays)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>images = [img1, img2, img3] logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_images_batch(self, title: str, series: str, images: list, iteration: int):\n    \"\"\"\n    Log multiple images to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name\n        images: List of images (numpy arrays)\n        iteration: Iteration/step number\n\n    Example:\n        images = [img1, img2, img3]\n        logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    for idx, image in enumerate(images):\n        self.logger.report_image(\n            title=title,\n            series=f\"{series}_img_{idx}\",\n            iteration=iteration,\n            image=image\n        )\n</code></pre> log_text \u00b6 <pre><code>log_text(text, title='Info')\n</code></pre> <p>Log text to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_text(self, text: str, title: str = \"Info\"):\n    \"\"\"Log text to ClearML.\"\"\"\n    if self.logger is None:\n        return\n    self.logger.report_text(text, print_console=True)\n</code></pre> report_text \u00b6 <pre><code>report_text(text)\n</code></pre> <p>Report text to ClearML (alias for log_text with default title).</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_text(self, text: str):\n    \"\"\"Report text to ClearML (alias for log_text with default title).\"\"\"\n    self.log_text(text)\n</code></pre> report_plotly \u00b6 <pre><code>report_plotly(title, series, iteration, figure)\n</code></pre> <p>Report a Plotly figure to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Plot title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required <code>figure</code> <p>Plotly figure object</p> required Example <p>import plotly.graph_objects as go fig = go.Figure() fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data')) fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\") logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_plotly(self, title: str, series: str, iteration: int, figure):\n    \"\"\"\n    Report a Plotly figure to ClearML.\n\n    Args:\n        title: Plot title/group\n        series: Series name\n        iteration: Iteration/step number\n        figure: Plotly figure object\n\n    Example:\n        import plotly.graph_objects as go\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data'))\n        fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\")\n        logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_plotly(\n        title=title,\n        series=series,\n        iteration=iteration,\n        figure=figure\n    )\n</code></pre> finalize \u00b6 <pre><code>finalize()\n</code></pre> <p>Finalize logging and close task.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def finalize(self):\n    \"\"\"Finalize logging and close task.\"\"\"\n    if self.task:\n        self.task.close()\n</code></pre> ClearMLUploadCallback \u00b6 <pre><code>ClearMLUploadCallback(\n    task,\n    clearml_logger=None,\n    checkpoint_dir=\"checkpoints\",\n    embedding_dir=\"embeddings\",\n)\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>PyTorch Lightning callback for logging checkpoint and embedding paths to ClearML.</p> <p>Automatically logs local file paths for: - Latest checkpoint after each validation epoch - Per-epoch validation embeddings</p> Usage <p>from pytorch_lightning import Trainer from embeddings_squeeze.loggers import ClearMLUploadCallback, setup_clearml</p> <p>task = setup_clearml(project_name=\"my_project\", task_name=\"experiment_1\") logger = ClearMLLogger(task) if task else None callback = ClearMLUploadCallback(task, logger, checkpoint_dir=\"checkpoints\")</p> <p>trainer = Trainer(callbacks=[callback], ...)</p> <p>Initialize ClearML path logging callback.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>ClearML Task object</p> required <code>clearml_logger</code> <code>ClearMLLogger</code> <p>ClearML logger for text reporting (optional)</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>Directory containing checkpoints</p> <code>'checkpoints'</code> <code>embedding_dir</code> <code>str</code> <p>Directory containing embeddings</p> <code>'embeddings'</code> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def __init__(self, task: Task, clearml_logger: ClearMLLogger = None, \n             checkpoint_dir: str = \"checkpoints\", embedding_dir: str = \"embeddings\"):\n    \"\"\"\n    Initialize ClearML path logging callback.\n\n    Args:\n        task: ClearML Task object\n        clearml_logger: ClearML logger for text reporting (optional)\n        checkpoint_dir: Directory containing checkpoints\n        embedding_dir: Directory containing embeddings\n    \"\"\"\n    super().__init__()\n    self.task = task\n    self.clearml_logger = clearml_logger\n    self.checkpoint_dir = checkpoint_dir\n    self.embedding_dir = embedding_dir\n</code></pre> Functions\u00b6 on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end(trainer, pl_module)\n</code></pre> <p>Called after validation epoch ends.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n    \"\"\"Called after validation epoch ends.\"\"\"\n    if self.task is None:\n        return\n\n    # Log checkpoint path\n    try:\n        ckpt_path = self._find_latest_checkpoint()\n        if ckpt_path:\n            abs_path = os.path.abspath(ckpt_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Checkpoint saved: {abs_path}\")\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed finding checkpoint: {e}\")\n\n    # Log embedding path\n    try:\n        emb_path = os.path.join(\n            self.embedding_dir, \n            f\"val_embedding_epoch{pl_module.current_epoch}.pt\"\n        )\n        if os.path.exists(emb_path):\n            abs_path = os.path.abspath(emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Embedding saved: {abs_path}\")\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed logging embedding path: {e}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger.ClearMLLogger.log_image--grayscale-image","title":"Grayscale image","text":"<p>img = np.eye(256, 256, dtype=np.uint8) * 255 logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger.ClearMLLogger.log_image--rgb-image","title":"RGB image","text":"<p>img_rgb = np.zeros((256, 256, 3), dtype=np.uint8) img_rgb[:, :, 0] = 255  # Red channel logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)</p>"},{"location":"api/all/#embeddings_squeeze.loggers.clearml_logger-functions","title":"Functions","text":"load_credentials \u00b6 <pre><code>load_credentials(config_dir=None)\n</code></pre> <p>Load ClearML credentials from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config_dir</code> <code>str</code> <p>Directory containing clearml_credentials.yaml        If None, uses the configs directory in embeddings_squeeze</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Credentials dictionary</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def load_credentials(config_dir: str = None):\n    \"\"\"\n    Load ClearML credentials from YAML file.\n\n    Args:\n        config_dir: Directory containing clearml_credentials.yaml\n                   If None, uses the configs directory in embeddings_squeeze\n\n    Returns:\n        dict: Credentials dictionary\n    \"\"\"\n    if config_dir is None:\n        # Default to embeddings_squeeze/configs\n        current_file = Path(__file__)\n        config_dir = current_file.parent.parent / 'configs'\n    else:\n        config_dir = Path(config_dir)\n\n    creds_file = config_dir / 'clearml_credentials.yaml'\n\n    if not creds_file.exists():\n        raise FileNotFoundError(\n            f\"ClearML credentials file not found: {creds_file}\\n\"\n            f\"Please copy clearml_credentials.yaml.example to clearml_credentials.yaml \"\n            f\"and fill in your credentials.\"\n        )\n\n    creds = OmegaConf.load(creds_file)\n    return OmegaConf.to_container(creds, resolve=True)\n</code></pre> setup_clearml \u00b6 <pre><code>setup_clearml(project_name, task_name, auto_connect=True)\n</code></pre> <p>Setup ClearML with credentials from config file.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>ClearML project name</p> required <code>task_name</code> <code>str</code> <p>ClearML task name</p> required <code>auto_connect</code> <code>bool</code> <p>If True, automatically connect frameworks</p> <code>True</code> <p>Returns:</p> Type Description <p>Task object</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def setup_clearml(project_name: str, task_name: str, auto_connect: bool = True):\n    \"\"\"\n    Setup ClearML with credentials from config file.\n\n    Args:\n        project_name: ClearML project name\n        task_name: ClearML task name\n        auto_connect: If True, automatically connect frameworks\n\n    Returns:\n        Task object\n    \"\"\"\n    # Load credentials\n    config_dir = Path(__file__).parent.parent / 'configs'\n\n    try:\n        creds = load_credentials(config_dir)\n\n        # Set credentials\n        clearml.Task.set_credentials(\n            api_host=creds.get('api_host', 'https://api.clear.ml'),\n            web_host=creds.get('web_host', 'https://app.clear.ml'),\n            files_host=creds.get('files_host', 'https://files.clear.ml'),\n            key=creds['api_key'],\n            secret=creds['api_secret']\n        )\n\n        # Initialize task\n        task = Task.init(\n            project_name=project_name,\n            task_name=task_name,\n            auto_connect_frameworks=auto_connect\n        )\n        return task\n    except FileNotFoundError as e:\n        print(f\"Warning: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n    except Exception as e:\n        print(f\"Warning: Failed to setup ClearML: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models","title":"models","text":"<p>Model architectures and components.</p>"},{"location":"api/all/#embeddings_squeeze.models-classes","title":"Classes","text":""},{"location":"api/all/#embeddings_squeeze.models.VQWithProjection","title":"VQWithProjection","text":"<pre><code>VQWithProjection(\n    input_dim,\n    codebook_size=512,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Vector Quantization (VQ-VAE) with projections</p> <p>Uses EMA for codebook updates (no gradients needed for codebook) ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection (e.g., 2048 -&gt; 64)\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Vector Quantization\n    self.vq = VectorQuantize(\n        dim=bottleneck_dim,\n        codebook_size=codebook_size,\n        decay=decay,  # EMA decay for codebook\n        commitment_weight=commitment_weight  # Commitment loss weight\n    )\n\n    # Up projection (64 -&gt; 2048)\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.FSQWithProjection","title":"FSQWithProjection","text":"<pre><code>FSQWithProjection(input_dim, levels=None)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Finite Scalar Quantization (FSQ)</p> <p>Quantization without codebook - each dimension quantized independently ~10 bits per vector at levels=[8,5,5,5]</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int, levels: list = None):\n    super().__init__(input_dim)\n    if levels is None:\n        levels = [8, 5, 5, 5]  # 8*5*5*5 = 1000 codes \u2248 2^10\n\n    self.num_levels = len(levels)\n\n    # Projection to quantization space\n    self.project_in = nn.Linear(input_dim, self.num_levels)\n\n    # FSQ quantization\n    self.fsq = FSQ(levels=levels, dim=self.num_levels)\n\n    # Projection back\n    self.project_out = nn.Linear(self.num_levels, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.LFQWithProjection","title":"LFQWithProjection","text":"<pre><code>LFQWithProjection(\n    input_dim,\n    codebook_size=512,\n    entropy_loss_weight=0.1,\n    diversity_gamma=0.1,\n    spherical=False,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Lookup-Free Quantization (LFQ)</p> <p>Uses entropy loss for code diversity ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512,\n    entropy_loss_weight: float = 0.1, \n    diversity_gamma: float = 0.1, \n    spherical: bool = False\n):\n    super().__init__(input_dim)\n    # Quantization dimension = log2(codebook_size)\n    self.quant_dim = int(math.log2(codebook_size))\n\n    # Projection with normalization\n    self.project_in = nn.Sequential(\n        nn.Linear(input_dim, self.quant_dim),\n        nn.LayerNorm(self.quant_dim)\n    )\n\n    # LFQ quantization\n    self.lfq = LFQ(\n        dim=self.quant_dim,\n        codebook_size=codebook_size,\n        entropy_loss_weight=entropy_loss_weight,\n        diversity_gamma=diversity_gamma,\n        spherical=spherical\n    )\n\n    # Projection back\n    self.project_out = nn.Linear(self.quant_dim, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.ResidualVQWithProjection","title":"ResidualVQWithProjection","text":"<pre><code>ResidualVQWithProjection(\n    input_dim,\n    num_quantizers=4,\n    codebook_size=256,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Residual Vector Quantization (RVQ)</p> <p>Multi-level quantization - each level quantizes the residual of the previous 32 bits per vector at num_quantizers=4, codebook_size=256 (4*8 bits)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    num_quantizers: int = 4,\n    codebook_size: int = 256, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Residual VQ\n    self.residual_vq = ResidualVQ(\n        dim=bottleneck_dim,\n        num_quantizers=num_quantizers,  # Number of levels\n        codebook_size=codebook_size,\n        decay=decay,\n        commitment_weight=commitment_weight\n    )\n\n    # Up projection\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.BaseQuantizer","title":"BaseQuantizer","text":"<pre><code>BaseQuantizer(input_dim)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for all quantizers</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int):\n    super().__init__()\n    self.input_dim = input_dim\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.BaseQuantizer-functions","title":"Functions","text":"quantize_spatial \u00b6 <pre><code>quantize_spatial(features)\n</code></pre> <p>Quantize spatial features [B, C, H, W]</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>quantized</code> <p>Quantized features [B, C, H, W]</p> <code>loss</code> <p>Quantization loss (scalar)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def quantize_spatial(self, features: torch.Tensor):\n    \"\"\"\n    Quantize spatial features [B, C, H, W]\n\n    Args:\n        features: Tensor of shape [B, C, H, W]\n\n    Returns:\n        quantized: Quantized features [B, C, H, W]\n        loss: Quantization loss (scalar)\n    \"\"\"\n    B, C, H, W = features.shape\n    # Transform [B, C, H, W] -&gt; [B, H*W, C]\n    seq = features.permute(0, 2, 3, 1).reshape(B, H * W, C)\n\n    # Quantize\n    quantized, indices, loss = self.forward(seq)\n\n    # Transform back [B, H*W, C] -&gt; [B, C, H, W]\n    quantized = quantized.reshape(B, H, W, C).permute(0, 3, 1, 2)\n\n    # Handle loss (may be tensor with multiple elements)\n    if isinstance(loss, torch.Tensor) and loss.numel() &gt; 1:\n        loss = loss.mean()\n\n    return quantized, loss\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.DiceLoss","title":"DiceLoss","text":"<pre><code>DiceLoss(smooth=1.0)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Dice Loss for multi-class segmentation</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, smooth: float = 1.0):\n    super().__init__()\n    self.smooth = smooth\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.FocalLoss","title":"FocalLoss","text":"<pre><code>FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Focal Loss for handling class imbalance (multi-class via CE per-pixel)</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):\n    super().__init__()\n    self.alpha = alpha\n    self.gamma = gamma\n    self.reduction = reduction\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.CombinedLoss","title":"CombinedLoss","text":"<pre><code>CombinedLoss(\n    ce_weight=1.0,\n    dice_weight=1.0,\n    focal_weight=0.5,\n    class_weights=None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Combined loss: CE + Dice + Focal. Returns (total, ce, dice, focal).</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(\n    self, \n    ce_weight: float = 1.0, \n    dice_weight: float = 1.0, \n    focal_weight: float = 0.5, \n    class_weights=None\n):\n    super().__init__()\n    # class_weights can be None or a tensor/list\n    if class_weights is not None:\n        # Leave tensor creation to forward (to place on correct device) but store raw\n        self._class_weights = class_weights\n    else:\n        self._class_weights = None\n\n    self.ce_weight = ce_weight\n    self.dice_weight = dice_weight\n    self.focal_weight = focal_weight\n\n    # Instantiate component losses\n    self.dice_loss = DiceLoss()\n    self.focal_loss = FocalLoss()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.SegmentationBackbone","title":"SegmentationBackbone","text":"<pre><code>SegmentationBackbone()\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation backbones.</p> <p>All segmentation backbones should inherit from this class and implement the required methods for feature extraction and full segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.SegmentationBackbone-attributes","title":"Attributes","text":"feature_dim <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return the feature dimension.</p> num_classes <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return the number of output classes.</p>"},{"location":"api/all/#embeddings_squeeze.models.SegmentationBackbone-functions","title":"Functions","text":"extract_features <code>abstractmethod</code> \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract features from input images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, feature_dim, H', W']</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef extract_features(self, images, detach=True):\n    \"\"\"\n    Extract features from input images.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, feature_dim, H', W']\n    \"\"\"\n    pass\n</code></pre> forward <code>abstractmethod</code> \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full forward pass for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef forward(self, images):\n    \"\"\"\n    Full forward pass for segmentation.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.VQSqueezeModule","title":"VQSqueezeModule","text":"<pre><code>VQSqueezeModule(\n    backbone,\n    quantizer=None,\n    num_classes=21,\n    learning_rate=0.0001,\n    vq_loss_weight=0.1,\n    loss_type=\"ce\",\n    class_weights=None,\n    add_adapter=False,\n    feature_dim=2048,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for VQ compression training.</p> <p>Features: - Multiple quantizer support (VQ, FSQ, LFQ, RVQ) - Adapter layers for fine-tuning frozen backbones - Advanced loss functions (CE, Dice, Focal, Combined) - Embedding extraction and saving</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    quantizer: Optional[nn.Module] = None,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    vq_loss_weight: float = 0.1,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    add_adapter: bool = False,\n    feature_dim: int = 2048,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'quantizer', 'clearml_logger'])\n\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n    self.vq_loss_weight = vq_loss_weight\n    self.loss_type = loss_type\n    self.add_adapter = add_adapter\n    self.feature_dim = feature_dim\n\n    # Setup backbone with optional adapters\n    self.backbone = backbone\n    self._setup_backbone_with_adapters(feature_dim, add_adapter)\n\n    # Quantizer (optional)\n    self.quantizer = quantizer\n\n    # Loss function\n    self.criterion = self._init_loss(loss_type, class_weights)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n\n    # Embedding storage (per-epoch, first batch only)\n    self.embedding_dir = \"embeddings\"\n    os.makedirs(self.embedding_dir, exist_ok=True)\n    self._first_val_batch_features = None\n\n    # UMAP visualization storage\n    self._val_backbone_embeddings = []\n    self._val_quantized_embeddings = []\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.VQSqueezeModule-functions","title":"Functions","text":"forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone + optional quantizer + decoder.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> <code>quant_loss</code> <p>Quantization loss (0 if no quantizer)</p> <code>original_features</code> <p>Extracted features (before quantization)</p> <code>quantized_features</code> <p>Features after quantization (same as original if no quantizer)</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone + optional quantizer + decoder.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n        quant_loss: Quantization loss (0 if no quantizer)\n        original_features: Extracted features (before quantization)\n        quantized_features: Features after quantization (same as original if no quantizer)\n    \"\"\"\n    # Extract features\n    features = self.backbone.extract_features(images, detach=self.feature_adapter is not None)\n\n    # Apply adapter if present\n    if self.feature_adapter is not None:\n        features = features + self.feature_adapter(features)\n\n    # Store original features for embedding extraction\n    original_features = features\n\n    # Quantize if quantizer is present\n    quant_loss = torch.tensor(0.0, device=images.device)\n    quantized_features = original_features  # Default to original if no quantizer\n    if self.quantizer is not None:\n        features, quant_loss = self.quantizer.quantize_spatial(features)\n        quantized_features = features\n\n    # Decode to segmentation logits\n    output = self.backbone.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear', align_corners=False)\n\n    return output, quant_loss, original_features, quantized_features\n</code></pre> training_step \u00b6 <pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, _, _ = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return loss\n</code></pre> validation_step \u00b6 <pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, backbone_features, quantized_features = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    # Accumulate embeddings for UMAP visualization\n    self._val_backbone_embeddings.append(backbone_features.detach().cpu())\n    self._val_quantized_embeddings.append(quantized_features.detach().cpu())\n\n    # Save only first batch features for this epoch\n    if batch_idx == 0:\n        self._first_val_batch_features = backbone_features.detach().cpu()\n\n    return loss\n</code></pre> on_validation_epoch_start \u00b6 <pre><code>on_validation_epoch_start()\n</code></pre> <p>Clear accumulated embeddings at the start of each validation epoch.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_start(self):\n    \"\"\"Clear accumulated embeddings at the start of each validation epoch.\"\"\"\n    self._val_backbone_embeddings.clear()\n    self._val_quantized_embeddings.clear()\n</code></pre> on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations and save embeddings.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations and save embeddings.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    keys = [\n        \"train/loss\", \"val/loss\", \"train/iou\", \"val/iou\",\n        \"train/precision\", \"val/precision\", \"train/recall\", \"val/recall\",\n        \"train/f1\", \"val/f1\"\n    ]\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n\n    # Generate UMAP visualizations on even epochs\n    if self.current_epoch % 2 == 0:\n        try:\n            import umap.umap_ as umap_module\n\n            # Only proceed if we have embeddings\n            if len(self._val_backbone_embeddings) &gt; 0 and len(self._val_quantized_embeddings) &gt; 0:\n                # Concatenate all accumulated embeddings\n                backbone_emb_flat = torch.cat(self._val_backbone_embeddings, dim=0)\n                quantized_emb_flat = torch.cat(self._val_quantized_embeddings, dim=0)\n\n                # Flatten spatial dimensions: [B, C, H, W] -&gt; [B*H*W, C]\n                backbone_emb_flat = backbone_emb_flat.permute(0, 2, 3, 1).reshape(-1, backbone_emb_flat.shape[1])\n                quantized_emb_flat = quantized_emb_flat.permute(0, 2, 3, 1).reshape(-1, quantized_emb_flat.shape[1])\n\n                # Convert to numpy\n                backbone_emb_np = backbone_emb_flat.numpy()\n                quantized_emb_np = quantized_emb_flat.numpy()\n\n                # Limit samples for performance (take subset if too large)\n                max_samples = 10000\n                if len(backbone_emb_np) &gt; max_samples:\n                    indices = np.random.choice(len(backbone_emb_np), max_samples, replace=False)\n                    backbone_emb_np = backbone_emb_np[indices]\n                    quantized_emb_np = quantized_emb_np[indices]\n\n                # Generate 2D UMAP\n                fig_2d, axs_2d = plt.subplots(1, 2, figsize=(12, 6))\n\n                proj_2d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(backbone_emb_np)\n                axs_2d[0].scatter(proj_2d_backbone[:, 0], proj_2d_backbone[:, 1], alpha=0.3)\n                axs_2d[0].set_title('2D UMAP: Backbone Embeddings')\n\n                proj_2d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(quantized_emb_np)\n                axs_2d[1].scatter(proj_2d_quantized[:, 0], proj_2d_quantized[:, 1], alpha=0.3)\n                axs_2d[1].set_title('2D UMAP: Quantized Embeddings')\n\n                # Convert 2D plot to image and log\n                fig_2d.canvas.draw()\n                img_2d = np.frombuffer(fig_2d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_2d = img_2d.reshape(fig_2d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_2d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"2d_embeddings_epoch_{self.current_epoch}\", \n                        img_2d, \n                        iteration=self.current_epoch\n                    )\n\n                # Generate 3D UMAP\n                fig_3d = plt.figure(figsize=(12, 6))\n                ax1 = fig_3d.add_subplot(121, projection='3d')\n                ax2 = fig_3d.add_subplot(122, projection='3d')\n\n                proj_3d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(backbone_emb_np)\n                ax1.scatter(proj_3d_backbone[:, 0], proj_3d_backbone[:, 1], proj_3d_backbone[:, 2], alpha=0.3)\n                ax1.set_title('3D UMAP: Backbone Embeddings')\n\n                proj_3d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(quantized_emb_np)\n                ax2.scatter(proj_3d_quantized[:, 0], proj_3d_quantized[:, 1], proj_3d_quantized[:, 2], alpha=0.3)\n                ax2.set_title('3D UMAP: Quantized Embeddings')\n\n                # Convert 3D plot to image and log\n                fig_3d.canvas.draw()\n                img_3d = np.frombuffer(fig_3d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_3d = img_3d.reshape(fig_3d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_3d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"3d_embeddings_epoch_{self.current_epoch}\", \n                        img_3d, \n                        iteration=self.current_epoch\n                    )\n\n            # Clear accumulated embeddings after logging\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n        except Exception as e:\n            if self.clearml_logger:\n                self.clearml_logger.report_text(\n                    f\"UMAP visualization failed at epoch {self.current_epoch}: {e}\"\n                )\n            # Clear embeddings even if visualization failed\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n    # Save per-epoch embedding (first validation batch only)\n    try:\n        if self._first_val_batch_features is not None:\n            emb_path = os.path.join(\n                self.embedding_dir,\n                f\"val_embedding_epoch{self.current_epoch}.pt\"\n            )\n            torch.save(self._first_val_batch_features, emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Saved small embedding: {emb_path}\")\n            # Reset for next epoch\n            self._first_val_batch_features = None\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed saving epoch embedding: {e}\")\n</code></pre> configure_optimizers \u00b6 <pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer - only trainable parameters.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer - only trainable parameters.\"\"\"\n    params = []\n\n    # Add adapter parameters if present\n    if self.feature_adapter is not None:\n        params += list(self.feature_adapter.parameters())\n\n    # Add quantizer parameters if present\n    if self.quantizer is not None:\n        params += list(self.quantizer.parameters())\n\n    # Add backbone parameters if not frozen\n    if self.feature_adapter is None:\n        params += [p for p in self.backbone.parameters() if p.requires_grad]\n\n    # Remove duplicates\n    params = list({id(p): p for p in params}.values())\n\n    if not params:\n        raise ValueError(\"No trainable parameters found!\")\n\n    return torch.optim.AdamW(params, lr=self.learning_rate)\n</code></pre> on_train_start \u00b6 <pre><code>on_train_start()\n</code></pre> <p>Ensure frozen backbone stays in eval mode.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_train_start(self):\n    \"\"\"Ensure frozen backbone stays in eval mode.\"\"\"\n    if self.feature_adapter is not None:\n        self.backbone.eval()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.BaselineSegmentationModule","title":"BaselineSegmentationModule","text":"<pre><code>BaselineSegmentationModule(\n    backbone,\n    num_classes=21,\n    learning_rate=0.0001,\n    loss_type=\"ce\",\n    class_weights=None,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for baseline segmentation training.</p> <p>Wraps segmentation backbone without Vector Quantization for comparison.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'clearml_logger'])\n\n    self.backbone = backbone\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n\n    # Segmentation loss\n    if class_weights is not None:\n        weight = torch.tensor(class_weights, dtype=torch.float32)\n        self.seg_criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=255)\n    else:\n        self.seg_criterion = nn.CrossEntropyLoss(ignore_index=255)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.BaselineSegmentationModule-functions","title":"Functions","text":"forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    output = self.backbone(images)\n    # Handle both dict and tensor returns\n    if isinstance(output, dict):\n        return output['out']\n    return output\n</code></pre> training_step \u00b6 <pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', seg_loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre> validation_step \u00b6 <pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre> on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n</code></pre> configure_optimizers \u00b6 <pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer.\"\"\"\n    # Optimize only trainable params\n    params = [p for p in self.parameters() if p.requires_grad]\n    return torch.optim.Adam(params, lr=self.learning_rate)\n</code></pre> predict \u00b6 <pre><code>predict(images)\n</code></pre> <p>Predict segmentation masks.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <p>Segmentation predictions [B, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict(self, images):\n    \"\"\"\n    Predict segmentation masks.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        predictions: Segmentation predictions [B, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        output = self(images)\n        predictions = output.argmax(dim=1)\n    return predictions\n</code></pre> predict_logits \u00b6 <pre><code>predict_logits(images)\n</code></pre> <p>Predict segmentation logits.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>logits</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict_logits(self, images):\n    \"\"\"\n    Predict segmentation logits.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        logits: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        logits = self(images)\n    return logits\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models-modules","title":"Modules","text":""},{"location":"api/all/#embeddings_squeeze.models.backbones","title":"backbones","text":"<p>Segmentation backbone implementations.</p>"},{"location":"api/all/#embeddings_squeeze.models.backbones-classes","title":"Classes","text":"SegmentationBackbone \u00b6 <pre><code>SegmentationBackbone()\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation backbones.</p> <p>All segmentation backbones should inherit from this class and implement the required methods for feature extraction and full segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Attributes\u00b6 feature_dim <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return the feature dimension.</p> num_classes <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return the number of output classes.</p> Functions\u00b6 extract_features <code>abstractmethod</code> \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract features from input images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, feature_dim, H', W']</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef extract_features(self, images, detach=True):\n    \"\"\"\n    Extract features from input images.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, feature_dim, H', W']\n    \"\"\"\n    pass\n</code></pre> forward <code>abstractmethod</code> \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full forward pass for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef forward(self, images):\n    \"\"\"\n    Full forward pass for segmentation.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    pass\n</code></pre> ViTSegmentationBackbone \u00b6 <pre><code>ViTSegmentationBackbone(\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>ViT-based segmentation backbone.</p> <p>Uses ViT-B/32 as backbone with custom segmentation head.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def __init__(\n    self,\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n    base_vit = model_fn(weights=weights)\n    self.backbone = _ViTBackboneWrapper(base_vit, freeze=freeze_backbone)\n    self.classifier = _ViTSegmentationHead(self.backbone.hidden_dim, num_classes)\n\n    self._num_classes = num_classes\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return ViT hidden dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract ViT backbone feature maps.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, hidden_dim, H/patch, W/patch]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract ViT backbone feature maps.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, hidden_dim, H/patch, W/patch]\n    \"\"\"\n    feats = self.backbone(images)['out']\n    return feats.detach() if detach else feats\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full ViT segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full ViT segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    logits = self.classifier(features)\n    logits = F.interpolate(logits, size=images.shape[-2:], mode='bilinear', align_corners=False)\n    return {'out': logits}\n</code></pre> DeepLabV3SegmentationBackbone \u00b6 <pre><code>DeepLabV3SegmentationBackbone(\n    weights_name=\"COCO_WITH_VOC_LABELS_V1\",\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>DeepLabV3-ResNet50 segmentation backbone.</p> <p>Uses pre-trained DeepLabV3-ResNet50 for segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def __init__(\n    self,\n    weights_name='COCO_WITH_VOC_LABELS_V1',\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n\n    weights = getattr(DeepLabV3_ResNet50_Weights, weights_name)\n    model = deeplabv3_resnet50(weights=weights)\n\n    self.backbone = model.backbone\n    self.classifier = model.classifier\n\n    self._num_classes = num_classes\n\n    if freeze_backbone:\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        self.backbone.eval()\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return DeepLabV3 feature dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract DeepLabV3 backbone features.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, 2048, H/8, W/8]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract DeepLabV3 backbone features.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, 2048, H/8, W/8]\n    \"\"\"\n    with torch.set_grad_enabled(not detach):\n        features = self.backbone(images)['out']\n    return features\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full DeepLabV3 segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full DeepLabV3 segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    output = self.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear')\n    return {'out': output}\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.backbones-modules","title":"Modules","text":"base \u00b6 <p>Abstract base class for segmentation backbones.</p> Classes\u00b6 SegmentationBackbone \u00b6 <pre><code>SegmentationBackbone()\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation backbones.</p> <p>All segmentation backbones should inherit from this class and implement the required methods for feature extraction and full segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Attributes\u00b6 feature_dim <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return the feature dimension.</p> num_classes <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return the number of output classes.</p> Functions\u00b6 extract_features <code>abstractmethod</code> \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract features from input images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, feature_dim, H', W']</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef extract_features(self, images, detach=True):\n    \"\"\"\n    Extract features from input images.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, feature_dim, H', W']\n    \"\"\"\n    pass\n</code></pre> forward <code>abstractmethod</code> \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full forward pass for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef forward(self, images):\n    \"\"\"\n    Full forward pass for segmentation.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    pass\n</code></pre> deeplab \u00b6 <p>DeepLabV3-ResNet50 segmentation backbone implementation.</p> Classes\u00b6 DeepLabV3SegmentationBackbone \u00b6 <pre><code>DeepLabV3SegmentationBackbone(\n    weights_name=\"COCO_WITH_VOC_LABELS_V1\",\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>DeepLabV3-ResNet50 segmentation backbone.</p> <p>Uses pre-trained DeepLabV3-ResNet50 for segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def __init__(\n    self,\n    weights_name='COCO_WITH_VOC_LABELS_V1',\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n\n    weights = getattr(DeepLabV3_ResNet50_Weights, weights_name)\n    model = deeplabv3_resnet50(weights=weights)\n\n    self.backbone = model.backbone\n    self.classifier = model.classifier\n\n    self._num_classes = num_classes\n\n    if freeze_backbone:\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        self.backbone.eval()\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return DeepLabV3 feature dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract DeepLabV3 backbone features.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, 2048, H/8, W/8]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract DeepLabV3 backbone features.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, 2048, H/8, W/8]\n    \"\"\"\n    with torch.set_grad_enabled(not detach):\n        features = self.backbone(images)['out']\n    return features\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full DeepLabV3 segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full DeepLabV3 segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    output = self.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear')\n    return {'out': output}\n</code></pre> vit \u00b6 <p>ViT-based segmentation backbone implementation.</p> Classes\u00b6 ViTSegmentationBackbone \u00b6 <pre><code>ViTSegmentationBackbone(\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>ViT-based segmentation backbone.</p> <p>Uses ViT-B/32 as backbone with custom segmentation head.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def __init__(\n    self,\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n    base_vit = model_fn(weights=weights)\n    self.backbone = _ViTBackboneWrapper(base_vit, freeze=freeze_backbone)\n    self.classifier = _ViTSegmentationHead(self.backbone.hidden_dim, num_classes)\n\n    self._num_classes = num_classes\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return ViT hidden dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract ViT backbone feature maps.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, hidden_dim, H/patch, W/patch]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract ViT backbone feature maps.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, hidden_dim, H/patch, W/patch]\n    \"\"\"\n    feats = self.backbone(images)['out']\n    return feats.detach() if detach else feats\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full ViT segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full ViT segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    logits = self.classifier(features)\n    logits = F.interpolate(logits, size=images.shape[-2:], mode='bilinear', align_corners=False)\n    return {'out': logits}\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.baseline_module","title":"baseline_module","text":"<p>PyTorch Lightning module for baseline segmentation training without VQ.</p>"},{"location":"api/all/#embeddings_squeeze.models.baseline_module-classes","title":"Classes","text":"BaselineSegmentationModule \u00b6 <pre><code>BaselineSegmentationModule(\n    backbone,\n    num_classes=21,\n    learning_rate=0.0001,\n    loss_type=\"ce\",\n    class_weights=None,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for baseline segmentation training.</p> <p>Wraps segmentation backbone without Vector Quantization for comparison.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'clearml_logger'])\n\n    self.backbone = backbone\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n\n    # Segmentation loss\n    if class_weights is not None:\n        weight = torch.tensor(class_weights, dtype=torch.float32)\n        self.seg_criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=255)\n    else:\n        self.seg_criterion = nn.CrossEntropyLoss(ignore_index=255)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n</code></pre> Functions\u00b6 forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    output = self.backbone(images)\n    # Handle both dict and tensor returns\n    if isinstance(output, dict):\n        return output['out']\n    return output\n</code></pre> training_step \u00b6 <pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', seg_loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre> validation_step \u00b6 <pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre> on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n</code></pre> configure_optimizers \u00b6 <pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer.\"\"\"\n    # Optimize only trainable params\n    params = [p for p in self.parameters() if p.requires_grad]\n    return torch.optim.Adam(params, lr=self.learning_rate)\n</code></pre> predict \u00b6 <pre><code>predict(images)\n</code></pre> <p>Predict segmentation masks.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <p>Segmentation predictions [B, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict(self, images):\n    \"\"\"\n    Predict segmentation masks.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        predictions: Segmentation predictions [B, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        output = self(images)\n        predictions = output.argmax(dim=1)\n    return predictions\n</code></pre> predict_logits \u00b6 <pre><code>predict_logits(images)\n</code></pre> <p>Predict segmentation logits.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>logits</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict_logits(self, images):\n    \"\"\"\n    Predict segmentation logits.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        logits: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        logits = self(images)\n    return logits\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.lightning_module","title":"lightning_module","text":"<p>PyTorch Lightning module for VQ compression training with advanced features. Supports multiple quantizers (VQ, FSQ, LFQ, RVQ), adapters, and loss functions.</p>"},{"location":"api/all/#embeddings_squeeze.models.lightning_module-classes","title":"Classes","text":"VQSqueezeModule \u00b6 <pre><code>VQSqueezeModule(\n    backbone,\n    quantizer=None,\n    num_classes=21,\n    learning_rate=0.0001,\n    vq_loss_weight=0.1,\n    loss_type=\"ce\",\n    class_weights=None,\n    add_adapter=False,\n    feature_dim=2048,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for VQ compression training.</p> <p>Features: - Multiple quantizer support (VQ, FSQ, LFQ, RVQ) - Adapter layers for fine-tuning frozen backbones - Advanced loss functions (CE, Dice, Focal, Combined) - Embedding extraction and saving</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    quantizer: Optional[nn.Module] = None,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    vq_loss_weight: float = 0.1,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    add_adapter: bool = False,\n    feature_dim: int = 2048,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'quantizer', 'clearml_logger'])\n\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n    self.vq_loss_weight = vq_loss_weight\n    self.loss_type = loss_type\n    self.add_adapter = add_adapter\n    self.feature_dim = feature_dim\n\n    # Setup backbone with optional adapters\n    self.backbone = backbone\n    self._setup_backbone_with_adapters(feature_dim, add_adapter)\n\n    # Quantizer (optional)\n    self.quantizer = quantizer\n\n    # Loss function\n    self.criterion = self._init_loss(loss_type, class_weights)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n\n    # Embedding storage (per-epoch, first batch only)\n    self.embedding_dir = \"embeddings\"\n    os.makedirs(self.embedding_dir, exist_ok=True)\n    self._first_val_batch_features = None\n\n    # UMAP visualization storage\n    self._val_backbone_embeddings = []\n    self._val_quantized_embeddings = []\n</code></pre> Functions\u00b6 forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone + optional quantizer + decoder.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> <code>quant_loss</code> <p>Quantization loss (0 if no quantizer)</p> <code>original_features</code> <p>Extracted features (before quantization)</p> <code>quantized_features</code> <p>Features after quantization (same as original if no quantizer)</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone + optional quantizer + decoder.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n        quant_loss: Quantization loss (0 if no quantizer)\n        original_features: Extracted features (before quantization)\n        quantized_features: Features after quantization (same as original if no quantizer)\n    \"\"\"\n    # Extract features\n    features = self.backbone.extract_features(images, detach=self.feature_adapter is not None)\n\n    # Apply adapter if present\n    if self.feature_adapter is not None:\n        features = features + self.feature_adapter(features)\n\n    # Store original features for embedding extraction\n    original_features = features\n\n    # Quantize if quantizer is present\n    quant_loss = torch.tensor(0.0, device=images.device)\n    quantized_features = original_features  # Default to original if no quantizer\n    if self.quantizer is not None:\n        features, quant_loss = self.quantizer.quantize_spatial(features)\n        quantized_features = features\n\n    # Decode to segmentation logits\n    output = self.backbone.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear', align_corners=False)\n\n    return output, quant_loss, original_features, quantized_features\n</code></pre> training_step \u00b6 <pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, _, _ = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return loss\n</code></pre> validation_step \u00b6 <pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, backbone_features, quantized_features = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    # Accumulate embeddings for UMAP visualization\n    self._val_backbone_embeddings.append(backbone_features.detach().cpu())\n    self._val_quantized_embeddings.append(quantized_features.detach().cpu())\n\n    # Save only first batch features for this epoch\n    if batch_idx == 0:\n        self._first_val_batch_features = backbone_features.detach().cpu()\n\n    return loss\n</code></pre> on_validation_epoch_start \u00b6 <pre><code>on_validation_epoch_start()\n</code></pre> <p>Clear accumulated embeddings at the start of each validation epoch.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_start(self):\n    \"\"\"Clear accumulated embeddings at the start of each validation epoch.\"\"\"\n    self._val_backbone_embeddings.clear()\n    self._val_quantized_embeddings.clear()\n</code></pre> on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations and save embeddings.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations and save embeddings.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    keys = [\n        \"train/loss\", \"val/loss\", \"train/iou\", \"val/iou\",\n        \"train/precision\", \"val/precision\", \"train/recall\", \"val/recall\",\n        \"train/f1\", \"val/f1\"\n    ]\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n\n    # Generate UMAP visualizations on even epochs\n    if self.current_epoch % 2 == 0:\n        try:\n            import umap.umap_ as umap_module\n\n            # Only proceed if we have embeddings\n            if len(self._val_backbone_embeddings) &gt; 0 and len(self._val_quantized_embeddings) &gt; 0:\n                # Concatenate all accumulated embeddings\n                backbone_emb_flat = torch.cat(self._val_backbone_embeddings, dim=0)\n                quantized_emb_flat = torch.cat(self._val_quantized_embeddings, dim=0)\n\n                # Flatten spatial dimensions: [B, C, H, W] -&gt; [B*H*W, C]\n                backbone_emb_flat = backbone_emb_flat.permute(0, 2, 3, 1).reshape(-1, backbone_emb_flat.shape[1])\n                quantized_emb_flat = quantized_emb_flat.permute(0, 2, 3, 1).reshape(-1, quantized_emb_flat.shape[1])\n\n                # Convert to numpy\n                backbone_emb_np = backbone_emb_flat.numpy()\n                quantized_emb_np = quantized_emb_flat.numpy()\n\n                # Limit samples for performance (take subset if too large)\n                max_samples = 10000\n                if len(backbone_emb_np) &gt; max_samples:\n                    indices = np.random.choice(len(backbone_emb_np), max_samples, replace=False)\n                    backbone_emb_np = backbone_emb_np[indices]\n                    quantized_emb_np = quantized_emb_np[indices]\n\n                # Generate 2D UMAP\n                fig_2d, axs_2d = plt.subplots(1, 2, figsize=(12, 6))\n\n                proj_2d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(backbone_emb_np)\n                axs_2d[0].scatter(proj_2d_backbone[:, 0], proj_2d_backbone[:, 1], alpha=0.3)\n                axs_2d[0].set_title('2D UMAP: Backbone Embeddings')\n\n                proj_2d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(quantized_emb_np)\n                axs_2d[1].scatter(proj_2d_quantized[:, 0], proj_2d_quantized[:, 1], alpha=0.3)\n                axs_2d[1].set_title('2D UMAP: Quantized Embeddings')\n\n                # Convert 2D plot to image and log\n                fig_2d.canvas.draw()\n                img_2d = np.frombuffer(fig_2d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_2d = img_2d.reshape(fig_2d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_2d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"2d_embeddings_epoch_{self.current_epoch}\", \n                        img_2d, \n                        iteration=self.current_epoch\n                    )\n\n                # Generate 3D UMAP\n                fig_3d = plt.figure(figsize=(12, 6))\n                ax1 = fig_3d.add_subplot(121, projection='3d')\n                ax2 = fig_3d.add_subplot(122, projection='3d')\n\n                proj_3d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(backbone_emb_np)\n                ax1.scatter(proj_3d_backbone[:, 0], proj_3d_backbone[:, 1], proj_3d_backbone[:, 2], alpha=0.3)\n                ax1.set_title('3D UMAP: Backbone Embeddings')\n\n                proj_3d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(quantized_emb_np)\n                ax2.scatter(proj_3d_quantized[:, 0], proj_3d_quantized[:, 1], proj_3d_quantized[:, 2], alpha=0.3)\n                ax2.set_title('3D UMAP: Quantized Embeddings')\n\n                # Convert 3D plot to image and log\n                fig_3d.canvas.draw()\n                img_3d = np.frombuffer(fig_3d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_3d = img_3d.reshape(fig_3d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_3d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"3d_embeddings_epoch_{self.current_epoch}\", \n                        img_3d, \n                        iteration=self.current_epoch\n                    )\n\n            # Clear accumulated embeddings after logging\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n        except Exception as e:\n            if self.clearml_logger:\n                self.clearml_logger.report_text(\n                    f\"UMAP visualization failed at epoch {self.current_epoch}: {e}\"\n                )\n            # Clear embeddings even if visualization failed\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n    # Save per-epoch embedding (first validation batch only)\n    try:\n        if self._first_val_batch_features is not None:\n            emb_path = os.path.join(\n                self.embedding_dir,\n                f\"val_embedding_epoch{self.current_epoch}.pt\"\n            )\n            torch.save(self._first_val_batch_features, emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Saved small embedding: {emb_path}\")\n            # Reset for next epoch\n            self._first_val_batch_features = None\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed saving epoch embedding: {e}\")\n</code></pre> configure_optimizers \u00b6 <pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer - only trainable parameters.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer - only trainable parameters.\"\"\"\n    params = []\n\n    # Add adapter parameters if present\n    if self.feature_adapter is not None:\n        params += list(self.feature_adapter.parameters())\n\n    # Add quantizer parameters if present\n    if self.quantizer is not None:\n        params += list(self.quantizer.parameters())\n\n    # Add backbone parameters if not frozen\n    if self.feature_adapter is None:\n        params += [p for p in self.backbone.parameters() if p.requires_grad]\n\n    # Remove duplicates\n    params = list({id(p): p for p in params}.values())\n\n    if not params:\n        raise ValueError(\"No trainable parameters found!\")\n\n    return torch.optim.AdamW(params, lr=self.learning_rate)\n</code></pre> on_train_start \u00b6 <pre><code>on_train_start()\n</code></pre> <p>Ensure frozen backbone stays in eval mode.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_train_start(self):\n    \"\"\"Ensure frozen backbone stays in eval mode.\"\"\"\n    if self.feature_adapter is not None:\n        self.backbone.eval()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.losses","title":"losses","text":"<p>Loss functions for segmentation tasks. Includes: Cross Entropy, Dice Loss, Focal Loss, and Combined Loss.</p>"},{"location":"api/all/#embeddings_squeeze.models.losses-classes","title":"Classes","text":"DiceLoss \u00b6 <pre><code>DiceLoss(smooth=1.0)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Dice Loss for multi-class segmentation</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, smooth: float = 1.0):\n    super().__init__()\n    self.smooth = smooth\n</code></pre> FocalLoss \u00b6 <pre><code>FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Focal Loss for handling class imbalance (multi-class via CE per-pixel)</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):\n    super().__init__()\n    self.alpha = alpha\n    self.gamma = gamma\n    self.reduction = reduction\n</code></pre> CombinedLoss \u00b6 <pre><code>CombinedLoss(\n    ce_weight=1.0,\n    dice_weight=1.0,\n    focal_weight=0.5,\n    class_weights=None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Combined loss: CE + Dice + Focal. Returns (total, ce, dice, focal).</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(\n    self, \n    ce_weight: float = 1.0, \n    dice_weight: float = 1.0, \n    focal_weight: float = 0.5, \n    class_weights=None\n):\n    super().__init__()\n    # class_weights can be None or a tensor/list\n    if class_weights is not None:\n        # Leave tensor creation to forward (to place on correct device) but store raw\n        self._class_weights = class_weights\n    else:\n        self._class_weights = None\n\n    self.ce_weight = ce_weight\n    self.dice_weight = dice_weight\n    self.focal_weight = focal_weight\n\n    # Instantiate component losses\n    self.dice_loss = DiceLoss()\n    self.focal_loss = FocalLoss()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.models.quantizers","title":"quantizers","text":"<p>Vector Quantization implementations using vector_quantize_pytorch library. Supports: VQ-VAE, FSQ, LFQ, and Residual VQ.</p>"},{"location":"api/all/#embeddings_squeeze.models.quantizers-classes","title":"Classes","text":"BaseQuantizer \u00b6 <pre><code>BaseQuantizer(input_dim)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for all quantizers</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int):\n    super().__init__()\n    self.input_dim = input_dim\n</code></pre> Functions\u00b6 quantize_spatial \u00b6 <pre><code>quantize_spatial(features)\n</code></pre> <p>Quantize spatial features [B, C, H, W]</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>quantized</code> <p>Quantized features [B, C, H, W]</p> <code>loss</code> <p>Quantization loss (scalar)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def quantize_spatial(self, features: torch.Tensor):\n    \"\"\"\n    Quantize spatial features [B, C, H, W]\n\n    Args:\n        features: Tensor of shape [B, C, H, W]\n\n    Returns:\n        quantized: Quantized features [B, C, H, W]\n        loss: Quantization loss (scalar)\n    \"\"\"\n    B, C, H, W = features.shape\n    # Transform [B, C, H, W] -&gt; [B, H*W, C]\n    seq = features.permute(0, 2, 3, 1).reshape(B, H * W, C)\n\n    # Quantize\n    quantized, indices, loss = self.forward(seq)\n\n    # Transform back [B, H*W, C] -&gt; [B, C, H, W]\n    quantized = quantized.reshape(B, H, W, C).permute(0, 3, 1, 2)\n\n    # Handle loss (may be tensor with multiple elements)\n    if isinstance(loss, torch.Tensor) and loss.numel() &gt; 1:\n        loss = loss.mean()\n\n    return quantized, loss\n</code></pre> VQWithProjection \u00b6 <pre><code>VQWithProjection(\n    input_dim,\n    codebook_size=512,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Vector Quantization (VQ-VAE) with projections</p> <p>Uses EMA for codebook updates (no gradients needed for codebook) ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection (e.g., 2048 -&gt; 64)\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Vector Quantization\n    self.vq = VectorQuantize(\n        dim=bottleneck_dim,\n        codebook_size=codebook_size,\n        decay=decay,  # EMA decay for codebook\n        commitment_weight=commitment_weight  # Commitment loss weight\n    )\n\n    # Up projection (64 -&gt; 2048)\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre> FSQWithProjection \u00b6 <pre><code>FSQWithProjection(input_dim, levels=None)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Finite Scalar Quantization (FSQ)</p> <p>Quantization without codebook - each dimension quantized independently ~10 bits per vector at levels=[8,5,5,5]</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int, levels: list = None):\n    super().__init__(input_dim)\n    if levels is None:\n        levels = [8, 5, 5, 5]  # 8*5*5*5 = 1000 codes \u2248 2^10\n\n    self.num_levels = len(levels)\n\n    # Projection to quantization space\n    self.project_in = nn.Linear(input_dim, self.num_levels)\n\n    # FSQ quantization\n    self.fsq = FSQ(levels=levels, dim=self.num_levels)\n\n    # Projection back\n    self.project_out = nn.Linear(self.num_levels, input_dim)\n</code></pre> LFQWithProjection \u00b6 <pre><code>LFQWithProjection(\n    input_dim,\n    codebook_size=512,\n    entropy_loss_weight=0.1,\n    diversity_gamma=0.1,\n    spherical=False,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Lookup-Free Quantization (LFQ)</p> <p>Uses entropy loss for code diversity ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512,\n    entropy_loss_weight: float = 0.1, \n    diversity_gamma: float = 0.1, \n    spherical: bool = False\n):\n    super().__init__(input_dim)\n    # Quantization dimension = log2(codebook_size)\n    self.quant_dim = int(math.log2(codebook_size))\n\n    # Projection with normalization\n    self.project_in = nn.Sequential(\n        nn.Linear(input_dim, self.quant_dim),\n        nn.LayerNorm(self.quant_dim)\n    )\n\n    # LFQ quantization\n    self.lfq = LFQ(\n        dim=self.quant_dim,\n        codebook_size=codebook_size,\n        entropy_loss_weight=entropy_loss_weight,\n        diversity_gamma=diversity_gamma,\n        spherical=spherical\n    )\n\n    # Projection back\n    self.project_out = nn.Linear(self.quant_dim, input_dim)\n</code></pre> ResidualVQWithProjection \u00b6 <pre><code>ResidualVQWithProjection(\n    input_dim,\n    num_quantizers=4,\n    codebook_size=256,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Residual Vector Quantization (RVQ)</p> <p>Multi-level quantization - each level quantizes the residual of the previous 32 bits per vector at num_quantizers=4, codebook_size=256 (4*8 bits)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    num_quantizers: int = 4,\n    codebook_size: int = 256, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Residual VQ\n    self.residual_vq = ResidualVQ(\n        dim=bottleneck_dim,\n        num_quantizers=num_quantizers,  # Number of levels\n        codebook_size=codebook_size,\n        decay=decay,\n        commitment_weight=commitment_weight\n    )\n\n    # Up projection\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.squeeze","title":"squeeze","text":"<p>CLI script for VQ compression training.</p> Usage <p>python squeeze.py --model vit --dataset oxford_pet --num_vectors 128 --epochs 3</p>"},{"location":"api/all/#embeddings_squeeze.squeeze-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.squeeze.create_quantizer","title":"create_quantizer","text":"<pre><code>create_quantizer(config)\n</code></pre> <p>Create quantizer based on config.</p> Source code in <code>embeddings_squeeze\\squeeze.py</code> <pre><code>def create_quantizer(config):\n    \"\"\"Create quantizer based on config.\"\"\"\n    if not config.quantizer.enabled:\n        return None\n\n    qtype = config.quantizer.type.lower()\n    feature_dim = config.model.feature_dim\n\n    if qtype == 'vq':\n        return VQWithProjection(\n            input_dim=feature_dim,\n            codebook_size=config.quantizer.codebook_size,\n            bottleneck_dim=config.quantizer.bottleneck_dim,\n            decay=config.quantizer.decay,\n            commitment_weight=config.quantizer.commitment_weight\n        )\n    elif qtype == 'fsq':\n        return FSQWithProjection(\n            input_dim=feature_dim,\n            levels=config.quantizer.levels\n        )\n    elif qtype == 'lfq':\n        return LFQWithProjection(\n            input_dim=feature_dim,\n            codebook_size=config.quantizer.codebook_size,\n            entropy_loss_weight=config.quantizer.entropy_loss_weight,\n            diversity_gamma=config.quantizer.diversity_gamma,\n            spherical=config.quantizer.spherical\n        )\n    elif qtype == 'rvq':\n        return ResidualVQWithProjection(\n            input_dim=feature_dim,\n            num_quantizers=config.quantizer.num_quantizers,\n            codebook_size=config.quantizer.codebook_size,\n            bottleneck_dim=config.quantizer.bottleneck_dim,\n            decay=config.quantizer.decay,\n            commitment_weight=config.quantizer.commitment_weight\n        )\n    elif qtype == 'none':\n        return None\n    else:\n        raise ValueError(f\"Unknown quantizer type: {qtype}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.squeeze.create_backbone","title":"create_backbone","text":"<pre><code>create_backbone(config)\n</code></pre> <p>Create segmentation backbone based on config.</p> Source code in <code>embeddings_squeeze\\squeeze.py</code> <pre><code>def create_backbone(config):\n    \"\"\"Create segmentation backbone based on config.\"\"\"\n    # Auto-detect feature_dim based on backbone if not set or invalid\n    if config.model.backbone.lower() == \"vit\":\n        # ViT uses 768-dim features\n        if config.model.feature_dim is None or config.model.feature_dim == 2048:\n            config.model.feature_dim = 768\n        backbone = ViTSegmentationBackbone(\n            num_classes=config.model.num_classes,\n            freeze_backbone=config.model.freeze_backbone\n        )\n    elif config.model.backbone.lower() == \"deeplab\":\n        # DeepLab uses 2048-dim features\n        if config.model.feature_dim is None:\n            config.model.feature_dim = 2048\n        backbone = DeepLabV3SegmentationBackbone(\n            weights_name=config.model.deeplab_weights,\n            num_classes=config.model.num_classes,\n            freeze_backbone=config.model.freeze_backbone\n        )\n    else:\n        raise ValueError(f\"Unknown backbone: {config.model.backbone}\")\n\n    return backbone\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.squeeze.create_data_module","title":"create_data_module","text":"<pre><code>create_data_module(config)\n</code></pre> <p>Create data module based on config.</p> Source code in <code>embeddings_squeeze\\squeeze.py</code> <pre><code>def create_data_module(config):\n    \"\"\"Create data module based on config.\"\"\"\n    if config.data.dataset.lower() == \"oxford_pet\":\n        data_module = OxfordPetDataModule(\n            data_path=config.data.data_path,\n            batch_size=config.training.batch_size,\n            num_workers=config.training.num_workers,\n            pin_memory=config.training.pin_memory,\n            image_size=config.data.image_size,\n            subset_size=config.data.subset_size\n        )\n    else:\n        raise ValueError(f\"Unknown dataset: {config.data.dataset}\")\n\n    return data_module\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.squeeze.setup_logging_and_callbacks","title":"setup_logging_and_callbacks","text":"<pre><code>setup_logging_and_callbacks(config)\n</code></pre> <p>Setup logging and callbacks.</p> Source code in <code>embeddings_squeeze\\squeeze.py</code> <pre><code>def setup_logging_and_callbacks(config):\n    \"\"\"Setup logging and callbacks.\"\"\"\n    # Create output directory\n    os.makedirs(config.output_dir, exist_ok=True)\n\n    # Setup ClearML\n    clearml_task = setup_clearml(\n        project_name=config.logger.project_name,\n        task_name=config.logger.task_name\n    )\n\n    # Create ClearML logger wrapper\n    clearml_logger = ClearMLLogger(clearml_task) if clearml_task else None\n\n    # TensorBoard logger for ClearML auto-logging\n    pl_logger = TensorBoardLogger(\n        save_dir=config.output_dir,\n        name=config.experiment_name,\n        version=None\n    )\n\n    # Callbacks\n    checkpoint_dir = os.path.join(config.output_dir, config.experiment_name)\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=checkpoint_dir,\n        filename='{epoch:02d}-{val/loss:.2f}',\n        monitor=config.training.monitor,\n        mode=config.training.mode,\n        save_top_k=config.training.save_top_k,\n        save_last=True\n    )\n\n    early_stop_callback = EarlyStopping(\n        monitor=config.training.monitor,\n        mode=config.training.mode,\n        patience=5,\n        verbose=True\n    )\n\n    callbacks = [checkpoint_callback, early_stop_callback]\n\n    # Add ClearML upload callback if task exists\n    if clearml_task:\n        clearml_upload_callback = ClearMLUploadCallback(\n            task=clearml_task,\n            clearml_logger=clearml_logger,\n            checkpoint_dir=checkpoint_dir,\n            embedding_dir=\"embeddings\"\n        )\n        callbacks.append(clearml_upload_callback)\n        print(\"ClearML logging and upload enabled\")\n\n    return pl_logger, clearml_logger, callbacks\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.squeeze.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main training function.</p> Source code in <code>embeddings_squeeze\\squeeze.py</code> <pre><code>def main():\n    \"\"\"Main training function.\"\"\"\n    parser = argparse.ArgumentParser(description=\"VQ Compression Training\")\n\n    # Model arguments\n    parser.add_argument(\"--model\", type=str, default=\"vit\", \n                       choices=[\"vit\", \"deeplab\"], help=\"Backbone model\")\n    parser.add_argument(\"--num_classes\", type=int, default=21,\n                       help=\"Number of classes\")\n    parser.add_argument(\"--add_adapter\", action=\"store_true\",\n                       help=\"Add adapter layers to frozen backbone\")\n    parser.add_argument(\"--feature_dim\", type=int, default=None,\n                       help=\"Feature dimension (auto-detected if not set)\")\n    parser.add_argument(\"--loss_type\", type=str, default=\"ce\",\n                       choices=[\"ce\", \"dice\", \"focal\", \"combined\"], help=\"Loss function type\")\n\n    # Quantizer arguments\n    parser.add_argument(\"--quantizer_type\", type=str, default=\"vq\",\n                       choices=[\"vq\", \"fsq\", \"lfq\", \"rvq\", \"none\"], help=\"Quantizer type\")\n    parser.add_argument(\"--quantizer_enabled\", action=\"store_true\", default=True,\n                       help=\"Enable quantization\")\n    parser.add_argument(\"--codebook_size\", type=int, default=512,\n                       help=\"Codebook size for VQ/LFQ/RVQ\")\n    parser.add_argument(\"--bottleneck_dim\", type=int, default=64,\n                       help=\"Bottleneck dimension for VQ/RVQ\")\n    parser.add_argument(\"--num_quantizers\", type=int, default=4,\n                       help=\"Number of quantizers for RVQ\")\n\n    # Logger arguments\n    parser.add_argument(\"--use_clearml\", action=\"store_true\",\n                       help=\"Use ClearML for logging\")\n    parser.add_argument(\"--project_name\", type=str, default=\"embeddings_squeeze\",\n                       help=\"Project name for logging\")\n    parser.add_argument(\"--task_name\", type=str, default=None,\n                       help=\"Task name for logging (defaults to experiment_name)\")\n\n    # Training arguments\n    parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs\")\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size\")\n    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n    parser.add_argument(\"--vq_loss_weight\", type=float, default=0.1,\n                       help=\"VQ loss weight\")\n    parser.add_argument(\"--max_batches\", type=int, default=None,\n                       help=\"Limit number of batches per epoch for train/val/test\")\n\n    # Data arguments\n    parser.add_argument(\"--dataset\", type=str, default=\"oxford_pet\",\n                       choices=[\"oxford_pet\"], help=\"Dataset name\")\n    parser.add_argument(\"--data_path\", type=str, default=\"./data\",\n                       help=\"Path to dataset\")\n    parser.add_argument(\"--subset_size\", type=int, default=None,\n                       help=\"Subset size for quick testing\")\n\n    # Experiment arguments\n    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs\",\n                       help=\"Output directory\")\n    parser.add_argument(\"--experiment_name\", type=str, default=\"vq_squeeze\",\n                       help=\"Experiment name\")\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n\n    # Other arguments\n    parser.add_argument(\"--initialize_codebook\", action=\"store_true\",\n                       help=\"Initialize codebook with k-means\")\n    parser.add_argument(\"--max_init_samples\", type=int, default=50000,\n                       help=\"Max samples for codebook initialization\")\n\n    args = parser.parse_args()\n\n    # Set random seeds\n    random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    pl.seed_everything(args.seed)\n\n    # Create configuration\n    config = get_default_config()\n\n    # Set task_name from experiment_name if not provided\n    args_dict = vars(args)\n    if args_dict.get('task_name') is None:\n        args_dict['task_name'] = args_dict.get('experiment_name', 'vq_squeeze')\n\n    config = update_config_from_args(config, args_dict)\n\n    print(f\"Starting experiment: {config.experiment_name}\")\n    print(f\"Model: {config.model.backbone}\")\n    print(f\"Dataset: {config.data.dataset}\")\n    print(f\"Quantizer: {config.quantizer.type if config.quantizer.enabled else 'None'}\")\n    print(f\"Loss type: {config.model.loss_type}\")\n    print(f\"Epochs: {config.training.epochs}\")\n\n    # Create components\n    # IMPORTANT: Create backbone first to auto-detect feature_dim\n    backbone = create_backbone(config)\n\n    # Now create quantizer with correct feature_dim\n    quantizer = create_quantizer(config)\n\n    data_module = create_data_module(config)\n\n    # Setup logging and callbacks (do this before creating model to get clearml_logger)\n    pl_logger, clearml_logger, callbacks = setup_logging_and_callbacks(config)\n\n    model = VQSqueezeModule(\n        backbone=backbone,\n        quantizer=quantizer,\n        num_classes=config.model.num_classes,\n        learning_rate=config.training.learning_rate,\n        vq_loss_weight=config.training.vq_loss_weight,\n        loss_type=config.model.loss_type,\n        class_weights=config.model.class_weights,\n        add_adapter=config.model.add_adapter,\n        feature_dim=config.model.feature_dim,\n        clearml_logger=clearml_logger\n    )\n\n    # Setup data\n    data_module.setup('fit')\n\n    # Initialize codebook if requested (only for VQ-based quantizers)\n    # Note: Codebook initialization is currently disabled in this version\n    # if config.initialize_codebook and quantizer is not None:\n    #     print(\"Initializing codebook with k-means...\")\n    #     initialize_codebook_from_data(\n    #         quantizer,\n    #         backbone,\n    #         data_module.train_dataloader(max_batches=config.training.max_batches),\n    #         model.device,\n    #         max_samples=config.max_init_samples\n    #     )\n\n    # Create trainer\n    trainer = pl.Trainer(\n        max_epochs=config.training.epochs,\n        logger=pl_logger,\n        callbacks=callbacks,\n        log_every_n_steps=config.training.log_every_n_steps,\n        val_check_interval=config.training.val_check_interval,\n        accelerator='auto', devices='auto',\n        precision=16 if torch.cuda.is_available() else 32,\n    )\n\n    # Train\n    print(\"Starting training...\")\n    trainer.fit(model, data_module)\n\n    # Finalize ClearML logging\n    if clearml_logger:\n        print(\"Finalizing ClearML task...\")\n        clearml_logger.finalize()\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_integration","title":"test_integration","text":"<p>Test script to verify the complete VQ visualization workflow.</p> <p>This script tests the integration of all components without running full training.</p>"},{"location":"api/all/#embeddings_squeeze.test_integration-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.test_integration.test_model_creation","title":"test_model_creation","text":"<pre><code>test_model_creation()\n</code></pre> <p>Test that models can be created successfully.</p> Source code in <code>embeddings_squeeze\\test_integration.py</code> <pre><code>def test_model_creation():\n    \"\"\"Test that models can be created successfully.\"\"\"\n    print(\"Testing model creation...\")\n\n    # Create backbone\n    backbone = ViTSegmentationBackbone(num_classes=21, freeze_backbone=True)\n    print(f\"\u2713 Backbone created: {type(backbone).__name__}\")\n\n    # Create VQ model\n    vq_model = VQSqueezeModule(\n        backbone=backbone,\n        num_vectors=128,\n        commitment_cost=0.25,\n        learning_rate=1e-4,\n        vq_loss_weight=0.1\n    )\n    print(f\"\u2713 VQ model created: {type(vq_model).__name__}\")\n\n    # Create baseline model\n    baseline_model = BaselineSegmentationModule(\n        backbone=backbone,\n        learning_rate=1e-4\n    )\n    print(f\"\u2713 Baseline model created: {type(baseline_model).__name__}\")\n\n    return vq_model, baseline_model\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_integration.test_data_loading","title":"test_data_loading","text":"<pre><code>test_data_loading()\n</code></pre> <p>Test that data can be loaded successfully.</p> Source code in <code>embeddings_squeeze\\test_integration.py</code> <pre><code>def test_data_loading():\n    \"\"\"Test that data can be loaded successfully.\"\"\"\n    print(\"\\nTesting data loading...\")\n\n    # Create data module with small subset\n    data_module = OxfordPetDataModule(\n        data_path=\"./data\",\n        batch_size=2,\n        num_workers=0,\n        pin_memory=False,\n        image_size=224,\n        subset_size=10  # Small subset for testing\n    )\n\n    try:\n        data_module.setup('test')\n        test_loader = data_module.test_dataloader()\n        print(f\"\u2713 Data module created with {len(test_loader.dataset)} samples\")\n\n        # Test loading a batch\n        batch = next(iter(test_loader))\n        images, masks = batch\n        print(f\"\u2713 Batch loaded: images {images.shape}, masks {masks.shape}\")\n\n        return test_loader\n    except Exception as e:\n        print(f\"\u2717 Data loading failed: {e}\")\n        print(\"Note: This is expected if Oxford-IIIT Pet dataset is not downloaded\")\n        return None\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_integration.test_model_inference","title":"test_model_inference","text":"<pre><code>test_model_inference(vq_model, baseline_model, test_loader)\n</code></pre> <p>Test that models can run inference.</p> Source code in <code>embeddings_squeeze\\test_integration.py</code> <pre><code>def test_model_inference(vq_model, baseline_model, test_loader):\n    \"\"\"Test that models can run inference.\"\"\"\n    print(\"\\nTesting model inference...\")\n\n    if test_loader is None:\n        print(\"Skipping inference test - no data available\")\n        return False\n\n    device = torch.device(\"cpu\")  # Use CPU for testing\n    vq_model.to(device)\n    baseline_model.to(device)\n\n    try:\n        # Get a batch\n        images, masks = next(iter(test_loader))\n        images = images.to(device)\n\n        # Test VQ model inference\n        with torch.no_grad():\n            vq_output, vq_loss = vq_model(images)\n            vq_preds = vq_model.predict_with_vq(images)\n        print(f\"\u2713 VQ model inference: output {vq_output.shape}, preds {vq_preds.shape}\")\n\n        # Test baseline model inference\n        with torch.no_grad():\n            baseline_output = baseline_model(images)\n            baseline_preds = baseline_model.predict(images)\n        print(f\"\u2713 Baseline model inference: output {baseline_output.shape}, preds {baseline_preds.shape}\")\n\n        return True\n    except Exception as e:\n        print(f\"\u2717 Model inference failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_integration.test_visualization_utilities","title":"test_visualization_utilities","text":"<pre><code>test_visualization_utilities()\n</code></pre> <p>Test visualization utilities.</p> Source code in <code>embeddings_squeeze\\test_integration.py</code> <pre><code>def test_visualization_utilities():\n    \"\"\"Test visualization utilities.\"\"\"\n    print(\"\\nTesting visualization utilities...\")\n\n    try:\n        from utils.comparison import compute_sample_iou, find_best_worst_samples\n\n        # Create dummy data\n        pred = torch.randint(0, 21, (224, 224))\n        target = torch.randint(0, 21, (224, 224))\n\n        # Test IoU computation\n        iou = compute_sample_iou(pred, target, num_classes=21)\n        print(f\"\u2713 IoU computation: {iou:.3f}\")\n\n        # Test sample ranking\n        dummy_results = [\n            (0, 0.8, torch.randn(3, 224, 224), torch.randint(0, 21, (224, 224)), torch.randint(0, 21, (224, 224))),\n            (1, 0.3, torch.randn(3, 224, 224), torch.randint(0, 21, (224, 224)), torch.randint(0, 21, (224, 224))),\n            (2, 0.9, torch.randn(3, 224, 224), torch.randint(0, 21, (224, 224)), torch.randint(0, 21, (224, 224))),\n        ]\n\n        best, worst = find_best_worst_samples(dummy_results, n_best=2, n_worst=1)\n        print(f\"\u2713 Sample ranking: {len(best)} best, {len(worst)} worst\")\n\n        return True\n    except Exception as e:\n        print(f\"\u2717 Visualization utilities failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_integration.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Run all tests.</p> Source code in <code>embeddings_squeeze\\test_integration.py</code> <pre><code>def main():\n    \"\"\"Run all tests.\"\"\"\n    print(\"=\"*60)\n    print(\"VQ VISUALIZATION SYSTEM - INTEGRATION TEST\")\n    print(\"=\"*60)\n\n    # Test model creation\n    vq_model, baseline_model = test_model_creation()\n\n    # Test data loading\n    test_loader = test_data_loading()\n\n    # Test model inference\n    inference_ok = test_model_inference(vq_model, baseline_model, test_loader)\n\n    # Test visualization utilities\n    utils_ok = test_visualization_utilities()\n\n    # Summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"TEST SUMMARY\")\n    print(\"=\"*60)\n    print(\"\u2713 Model creation: PASSED\")\n    print(\"\u2713 Data loading: PASSED\" if test_loader is not None else \"\u26a0 Data loading: SKIPPED (no dataset)\")\n    print(\"\u2713 Model inference: PASSED\" if inference_ok else \"\u2717 Model inference: FAILED\")\n    print(\"\u2713 Visualization utilities: PASSED\" if utils_ok else \"\u2717 Visualization utilities: FAILED\")\n\n    if inference_ok and utils_ok:\n        print(\"\\n\ud83c\udf89 All core components are working correctly!\")\n        print(\"\\nNext steps:\")\n        print(\"1. Train baseline model: python train_baseline.py --epochs 3 --subset_size 100\")\n        print(\"2. Train VQ model: python squeeze.py --epochs 3 --subset_size 100\")\n        print(\"3. Run visualization: python visualize.py --vq_checkpoint &lt;path&gt; --baseline_checkpoint &lt;path&gt;\")\n    else:\n        print(\"\\n\u274c Some components failed. Please check the errors above.\")\n\n    print(\"=\"*60)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_simplified_baseline","title":"test_simplified_baseline","text":"<p>Test script to verify the simplified baseline training approach.</p>"},{"location":"api/all/#embeddings_squeeze.test_simplified_baseline-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.test_simplified_baseline.test_backbone_classifier_training","title":"test_backbone_classifier_training","text":"<pre><code>test_backbone_classifier_training()\n</code></pre> <p>Test that backbone classifiers are trainable when backbone is frozen.</p> Source code in <code>embeddings_squeeze\\test_simplified_baseline.py</code> <pre><code>def test_backbone_classifier_training():\n    \"\"\"Test that backbone classifiers are trainable when backbone is frozen.\"\"\"\n    print(\"Testing backbone classifier training...\")\n\n    # Test ViT backbone\n    print(\"\\n--- ViT Backbone Test ---\")\n    vit_backbone = ViTSegmentationBackbone(num_classes=21, freeze_backbone=True)\n    vit_model = BaselineSegmentationModule(vit_backbone)\n\n    # Test DeepLab backbone\n    print(\"\\n--- DeepLab Backbone Test ---\")\n    deeplab_backbone = DeepLabV3SegmentationBackbone(num_classes=21, freeze_backbone=True)\n    deeplab_model = BaselineSegmentationModule(deeplab_backbone)\n\n    # Test inference\n    print(\"\\n--- Inference Test ---\")\n    dummy_input = torch.randn(1, 3, 224, 224)\n\n    try:\n        # Test ViT\n        vit_output = vit_model(dummy_input)\n        print(f\"\u2713 ViT inference: {vit_output['out'].shape if isinstance(vit_output, dict) else vit_output.shape}\")\n\n        # Test DeepLab\n        deeplab_output = deeplab_model(dummy_input)\n        print(f\"\u2713 DeepLab inference: {deeplab_output['out'].shape if isinstance(deeplab_output, dict) else deeplab_output.shape}\")\n\n        return True\n    except Exception as e:\n        print(f\"\u2717 Inference failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_simplified_baseline.test_optimizer_creation","title":"test_optimizer_creation","text":"<pre><code>test_optimizer_creation()\n</code></pre> <p>Test that optimizers can be created.</p> Source code in <code>embeddings_squeeze\\test_simplified_baseline.py</code> <pre><code>def test_optimizer_creation():\n    \"\"\"Test that optimizers can be created.\"\"\"\n    print(\"\\n--- Optimizer Test ---\")\n\n    try:\n        vit_backbone = ViTSegmentationBackbone(num_classes=21, freeze_backbone=True)\n        vit_model = BaselineSegmentationModule(vit_backbone)\n\n        optimizer = vit_model.configure_optimizers()\n        print(f\"\u2713 Optimizer created: {type(optimizer).__name__}\")\n\n        return True\n    except Exception as e:\n        print(f\"\u2717 Optimizer creation failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.test_simplified_baseline.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Run all tests.</p> Source code in <code>embeddings_squeeze\\test_simplified_baseline.py</code> <pre><code>def main():\n    \"\"\"Run all tests.\"\"\"\n    print(\"=\"*60)\n    print(\"SIMPLIFIED BASELINE TRAINING TEST\")\n    print(\"=\"*60)\n\n    # Test backbone classifier training\n    backbone_ok = test_backbone_classifier_training()\n\n    # Test optimizer creation\n    optimizer_ok = test_optimizer_creation()\n\n    # Summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"TEST SUMMARY\")\n    print(\"=\"*60)\n    print(\"\u2713 Backbone classifier training: PASSED\" if backbone_ok else \"\u2717 Backbone classifier training: FAILED\")\n    print(\"\u2713 Optimizer creation: PASSED\" if optimizer_ok else \"\u2717 Optimizer creation: FAILED\")\n\n    if backbone_ok and optimizer_ok:\n        print(\"\\n\ud83c\udf89 Simplified baseline training approach is working!\")\n        print(\"The backbone classifiers are trainable while backbones remain frozen.\")\n    else:\n        print(\"\\n\u274c Some tests failed. Check the errors above.\")\n\n    print(\"=\"*60)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.train_baseline","title":"train_baseline","text":"<p>CLI script for baseline segmentation training without VQ.</p> <p>This script directly uses the existing backbone's trainable classifier head while keeping the backbone frozen.</p> Usage <p>python train_baseline.py --model vit --dataset oxford_pet --epochs 3</p>"},{"location":"api/all/#embeddings_squeeze.train_baseline-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.train_baseline.create_backbone","title":"create_backbone","text":"<pre><code>create_backbone(config)\n</code></pre> <p>Create segmentation backbone based on config.</p> Source code in <code>embeddings_squeeze\\train_baseline.py</code> <pre><code>def create_backbone(config):\n    \"\"\"Create segmentation backbone based on config.\"\"\"\n    # Auto-detect feature_dim based on backbone if not set or invalid\n    if config.model.backbone.lower() == \"vit\":\n        # ViT uses 768-dim features\n        if config.model.feature_dim is None or config.model.feature_dim == 2048:\n            config.model.feature_dim = 768\n        backbone = ViTSegmentationBackbone(\n            num_classes=config.model.num_classes,\n            freeze_backbone=True  # Always freeze backbone, train only classifier\n        )\n    elif config.model.backbone.lower() == \"deeplab\":\n        # DeepLab uses 2048-dim features\n        if config.model.feature_dim is None:\n            config.model.feature_dim = 2048\n        backbone = DeepLabV3SegmentationBackbone(\n            weights_name=config.model.deeplab_weights,\n            num_classes=config.model.num_classes,\n            freeze_backbone=True  # Always freeze backbone, train only classifier\n        )\n    else:\n        raise ValueError(f\"Unknown backbone: {config.model.backbone}\")\n\n    return backbone\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.train_baseline.create_data_module","title":"create_data_module","text":"<pre><code>create_data_module(config)\n</code></pre> <p>Create data module based on config.</p> Source code in <code>embeddings_squeeze\\train_baseline.py</code> <pre><code>def create_data_module(config):\n    \"\"\"Create data module based on config.\"\"\"\n    if config.data.dataset.lower() == \"oxford_pet\":\n        data_module = OxfordPetDataModule(\n            data_path=config.data.data_path,\n            batch_size=config.training.batch_size,\n            num_workers=config.training.num_workers,\n            pin_memory=config.training.pin_memory,\n            image_size=config.data.image_size,\n            subset_size=config.data.subset_size\n        )\n    else:\n        raise ValueError(f\"Unknown dataset: {config.data.dataset}\")\n\n    return data_module\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.train_baseline.setup_logging_and_callbacks","title":"setup_logging_and_callbacks","text":"<pre><code>setup_logging_and_callbacks(config)\n</code></pre> <p>Setup logging and callbacks.</p> Source code in <code>embeddings_squeeze\\train_baseline.py</code> <pre><code>def setup_logging_and_callbacks(config):\n    \"\"\"Setup logging and callbacks.\"\"\"\n    # Create output directory\n    os.makedirs(config.output_dir, exist_ok=True)\n\n    # Setup ClearML\n    if config.logger.use_clearml:\n        clearml_task = setup_clearml(\n            project_name=config.logger.project_name,\n            task_name=config.logger.task_name\n        )\n    else:\n        clearml_task = None\n\n    # Create ClearML logger wrapper\n    clearml_logger = ClearMLLogger(clearml_task) if clearml_task else None\n\n    # PyTorch Lightning logger (None for ClearML auto-logging, TensorBoard otherwise)\n    if clearml_task:\n        pl_logger = None\n        print(\"Using ClearML for logging\")\n    else:\n        pl_logger = TensorBoardLogger(\n            save_dir=config.output_dir,\n            name=config.experiment_name,\n            version=None\n        )\n        print(\"Using TensorBoard for logging\")\n\n    # Callbacks\n    monitor_metric = 'val/loss'  # Unified metric name\n    checkpoint_dir = os.path.join(config.output_dir, config.experiment_name)\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=checkpoint_dir,\n        filename='{epoch:02d}-{val/loss:.2f}',\n        monitor=monitor_metric,\n        mode='min',  # Minimize loss\n        save_top_k=config.training.save_top_k,\n        save_last=True\n    )\n\n    early_stop_callback = EarlyStopping(\n        monitor=monitor_metric,\n        mode='min',  # Minimize loss\n        patience=5,\n        verbose=True\n    )\n\n    callbacks = [checkpoint_callback, early_stop_callback]\n\n    # Add ClearML upload callback if task exists\n    if clearml_task:\n        clearml_upload_callback = ClearMLUploadCallback(\n            task=clearml_task,\n            clearml_logger=clearml_logger,\n            checkpoint_dir=checkpoint_dir,\n            embedding_dir=\"embeddings\"\n        )\n        callbacks.append(clearml_upload_callback)\n        print(\"ClearML upload callback enabled\")\n\n    return pl_logger, clearml_logger, callbacks\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.train_baseline.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main training function.</p> Source code in <code>embeddings_squeeze\\train_baseline.py</code> <pre><code>def main():\n    \"\"\"Main training function.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Baseline Segmentation Training\")\n\n    # Model arguments\n    parser.add_argument(\"--model\", type=str, default=\"vit\", \n                       choices=[\"vit\", \"deeplab\"], help=\"Backbone model\")\n    parser.add_argument(\"--num_classes\", type=int, default=21,\n                       help=\"Number of classes\")\n    parser.add_argument(\"--loss_type\", type=str, default=\"ce\",\n                       choices=[\"ce\", \"dice\", \"focal\", \"combined\"], help=\"Loss function type\")\n\n    # Logger arguments\n    parser.add_argument(\"--use_clearml\", action=\"store_true\",\n                       help=\"Use ClearML for logging\")\n    parser.add_argument(\"--project_name\", type=str, default=\"embeddings_squeeze\",\n                       help=\"Project name for logging\")\n    parser.add_argument(\"--task_name\", type=str, default=None,\n                       help=\"Task name for logging (defaults to experiment_name)\")\n\n    # Training arguments\n    parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs\")\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size\")\n    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n    parser.add_argument(\"--max_batches\", type=int, default=None,\n                       help=\"Limit number of batches per epoch for train/val/test\")\n\n    # Data arguments\n    parser.add_argument(\"--dataset\", type=str, default=\"oxford_pet\",\n                       choices=[\"oxford_pet\"], help=\"Dataset name\")\n    parser.add_argument(\"--data_path\", type=str, default=\"./data\",\n                       help=\"Path to dataset\")\n    parser.add_argument(\"--subset_size\", type=int, default=None,\n                       help=\"Subset size for quick testing\")\n\n    # Experiment arguments\n    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs\",\n                       help=\"Output directory\")\n    parser.add_argument(\"--experiment_name\", type=str, default=\"segmentation_baseline\",\n                       help=\"Experiment name\")\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n\n    args = parser.parse_args()\n\n    # Set random seeds\n    random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    pl.seed_everything(args.seed)\n\n    # Create configuration\n    config = get_default_config()\n\n    # Set task_name from experiment_name if not provided\n    args_dict = vars(args)\n    if args_dict.get('task_name') is None:\n        args_dict['task_name'] = args_dict.get('experiment_name', 'segmentation_baseline')\n\n    config = update_config_from_args(config, args_dict)\n\n    # Override experiment name to indicate baseline\n    config.experiment_name = f\"{config.experiment_name}\"\n\n    print(f\"Starting baseline experiment: {config.experiment_name}\")\n    print(f\"Model: {config.model.backbone}\")\n    print(f\"Dataset: {config.data.dataset}\")\n    print(f\"Loss type: {config.model.loss_type}\")\n    print(f\"Epochs: {config.training.epochs}\")\n    print(\"Training strategy: Frozen backbone + trainable classifier\")\n\n    # Create components\n    backbone = create_backbone(config)\n    data_module = create_data_module(config)\n\n    # Setup logging and callbacks (do this before creating model to get clearml_logger)\n    pl_logger, clearml_logger, callbacks = setup_logging_and_callbacks(config)\n\n    model = BaselineSegmentationModule(\n        backbone=backbone,\n        num_classes=config.model.num_classes,\n        learning_rate=config.training.learning_rate,\n        loss_type=config.model.loss_type,\n        class_weights=config.model.class_weights,\n        clearml_logger=clearml_logger\n    )\n\n    # Setup data\n    data_module.setup('fit')\n\n    # Create trainer\n    trainer = pl.Trainer(\n        max_epochs=config.training.epochs,\n        logger=pl_logger,\n        callbacks=callbacks,\n        log_every_n_steps=config.training.log_every_n_steps,\n        val_check_interval=config.training.val_check_interval,\n        accelerator='auto', devices='auto',\n        precision=16 if torch.cuda.is_available() else 32,\n    )\n\n    # Train\n    print(\"Starting training...\")\n    trainer.fit(model, data_module)\n\n    # Finalize ClearML logging\n    if clearml_logger:\n        print(\"Finalizing ClearML task...\")\n        clearml_logger.finalize()\n\n    print(f\"Baseline training completed!\")\n    print(f\"Results saved to: {config.output_dir}/{config.experiment_name}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils","title":"utils","text":"<p>Utility functions for VQ compression.</p>"},{"location":"api/all/#embeddings_squeeze.utils-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.utils.measure_compression","title":"measure_compression","text":"<pre><code>measure_compression(\n    vq_model, backbone, test_loader, device\n)\n</code></pre> <p>Measure compression ratio achieved by VQ.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VectorQuantizer model</p> required <code>backbone</code> <p>Segmentation backbone</p> required <code>test_loader</code> <p>Test data loader</p> required <code>device</code> <p>Device to run on</p> required <p>Returns:</p> Name Type Description <code>compression_ratio</code> <p>Compression ratio achieved</p> Source code in <code>embeddings_squeeze\\utils\\compression.py</code> <pre><code>def measure_compression(vq_model, backbone, test_loader, device):\n    \"\"\"\n    Measure compression ratio achieved by VQ.\n\n    Args:\n        vq_model: VectorQuantizer model\n        backbone: Segmentation backbone\n        test_loader: Test data loader\n        device: Device to run on\n\n    Returns:\n        compression_ratio: Compression ratio achieved\n    \"\"\"\n    vq_model.eval()\n\n    total_original_bits = 0\n    total_compressed_bits = 0\n    num_features = 0\n\n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            features = backbone.extract_features(images)\n\n            B, C, H, W = features.shape\n            feat_flat = features.permute(0, 2, 3, 1).reshape(-1, C)\n\n            # Original size (float32 = 32 bits)\n            original_bits = feat_flat.numel() * 32\n\n            # Compressed size (only indices)\n            num_codes = vq_model.num_vectors\n            bits_per_index = np.ceil(np.log2(num_codes))\n            compressed_bits = feat_flat.shape[0] * bits_per_index\n\n            total_original_bits += original_bits\n            total_compressed_bits += compressed_bits\n            num_features += feat_flat.shape[0]\n\n    compression_ratio = total_original_bits / total_compressed_bits\n\n    print(\"=\"*70)\n    print(\"COMPRESSION ANALYSIS\")\n    print(\"=\"*70)\n    print(f\"Total features processed: {num_features}\")\n    print(f\"Feature dimension: {vq_model.vector_dim}\")\n    print(f\"Codebook size: {vq_model.num_vectors}\")\n    print()\n    print(f\"Original storage:\")\n    print(f\"  Per feature: {vq_model.vector_dim} \u00d7 32 bits = {vq_model.vector_dim * 32} bits = {vq_model.vector_dim * 4} bytes\")\n    print(f\"  Total: {total_original_bits / 8 / 1024 / 1024:.2f} MB\")\n    print()\n    print(f\"Compressed storage:\")\n    print(f\"  Per feature: {bits_per_index:.0f} bits (index)\")\n    print(f\"  Total: {total_compressed_bits / 8 / 1024:.2f} KB\")\n    print(f\"  + Codebook: {vq_model.num_vectors * vq_model.vector_dim * 4 / 1024:.2f} KB\")\n    print()\n    print(f\"Compression ratio: {compression_ratio:.1f}x\")\n    print(f\"Space savings: {(1 - 1/compression_ratio)*100:.1f}%\")\n    print(\"=\"*70)\n\n    return compression_ratio\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.compute_iou_metrics","title":"compute_iou_metrics","text":"<pre><code>compute_iou_metrics(\n    predictions, targets, num_classes, ignore_index=255\n)\n</code></pre> <p>Compute IoU metrics for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <p>Predicted masks [B, H, W]</p> required <code>targets</code> <p>Ground truth masks [B, H, W]</p> required <code>num_classes</code> <p>Number of classes</p> required <code>ignore_index</code> <p>Index to ignore in computation</p> <code>255</code> <p>Returns:</p> Name Type Description <code>metrics</code> <p>Dictionary with IoU metrics</p> Source code in <code>embeddings_squeeze\\utils\\compression.py</code> <pre><code>def compute_iou_metrics(predictions, targets, num_classes, ignore_index=255):\n    \"\"\"\n    Compute IoU metrics for segmentation.\n\n    Args:\n        predictions: Predicted masks [B, H, W]\n        targets: Ground truth masks [B, H, W]\n        num_classes: Number of classes\n        ignore_index: Index to ignore in computation\n\n    Returns:\n        metrics: Dictionary with IoU metrics\n    \"\"\"\n    predictions = predictions.cpu().numpy()\n    targets = targets.cpu().numpy()\n\n    # Flatten arrays\n    pred_flat = predictions.flatten()\n    target_flat = targets.flatten()\n\n    # Remove ignored pixels\n    valid_mask = target_flat != ignore_index\n    pred_flat = pred_flat[valid_mask]\n    target_flat = target_flat[valid_mask]\n\n    # Compute IoU for each class\n    ious = []\n    for cls in range(num_classes):\n        pred_cls = (pred_flat == cls)\n        target_cls = (target_flat == cls)\n\n        intersection = (pred_cls &amp; target_cls).sum()\n        union = pred_cls.sum() + target_cls.sum() - intersection\n\n        if union &gt; 0:\n            iou = intersection / union\n        else:\n            iou = 0.0\n\n        ious.append(iou)\n\n    # Compute mean IoU\n    mean_iou = np.mean(ious)\n\n    # Compute pixel accuracy\n    pixel_acc = (pred_flat == target_flat).mean()\n\n    metrics = {\n        'mean_iou': mean_iou,\n        'pixel_accuracy': pixel_acc,\n        'class_ious': ious\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.initialize_codebook_from_data","title":"initialize_codebook_from_data","text":"<pre><code>initialize_codebook_from_data(\n    vq_model,\n    backbone,\n    train_loader,\n    device,\n    max_samples=50000,\n)\n</code></pre> <p>Initialize codebook using k-means clustering on real data.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VectorQuantizer model</p> required <code>backbone</code> <p>Segmentation backbone</p> required <code>train_loader</code> <p>Training data loader</p> required <code>device</code> <p>Device to run on</p> required <code>max_samples</code> <code>int</code> <p>Maximum number of samples for k-means</p> <code>50000</code> Source code in <code>embeddings_squeeze\\utils\\initialization.py</code> <pre><code>def initialize_codebook_from_data(\n    vq_model, \n    backbone, \n    train_loader, \n    device, \n    max_samples: int = 50_000\n):\n    \"\"\"\n    Initialize codebook using k-means clustering on real data.\n\n    Args:\n        vq_model: VectorQuantizer model\n        backbone: Segmentation backbone\n        train_loader: Training data loader\n        device: Device to run on\n        max_samples: Maximum number of samples for k-means\n    \"\"\"\n    print(\"Collecting features for k-means initialization...\")\n    all_features = []\n\n    backbone.eval()\n    with torch.no_grad():\n        i = 0\n        for images, _ in train_loader:\n            print(f\"Processing batch {i}\")\n            features = backbone.extract_features(images.to(device))\n            i += 1\n\n            B, C, H, W = features.shape\n            feat_flat = features.permute(0, 2, 3, 1).reshape(-1, C)\n            all_features.append(feat_flat.cpu())\n\n            if len(all_features) * feat_flat.shape[0] &gt; max_samples:\n                break\n\n    all_features = torch.cat(all_features).numpy()\n\n    print(f\"Running k-means on {all_features.shape[0]} features...\")\n    kmeans = MiniBatchKMeans(\n        n_clusters=vq_model.num_vectors,\n        random_state=0,\n        batch_size=1000,\n        max_iter=100\n    )\n    kmeans.fit(all_features)\n\n    # Update codebook with cluster centers\n    vq_model.codebook.embeddings.data = torch.tensor(kmeans.cluster_centers_).to(device).float()\n\n    print(f\"Codebook initialized:\")\n    print(f\"  Mean: {vq_model.codebook.embeddings.mean():.2f}\")\n    print(f\"  Std: {vq_model.codebook.embeddings.std():.2f}\")\n    print(f\"  Norm: {vq_model.codebook.embeddings.norm(dim=1).mean():.2f}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.compute_sample_iou","title":"compute_sample_iou","text":"<pre><code>compute_sample_iou(\n    prediction, target, num_classes, ignore_index=255\n)\n</code></pre> <p>Compute IoU for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>Predicted mask [H, W]</p> required <code>target</code> <code>Tensor</code> <p>Ground truth mask [H, W]</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> required <code>ignore_index</code> <code>int</code> <p>Index to ignore in computation</p> <code>255</code> <p>Returns:</p> Name Type Description <code>iou</code> <code>float</code> <p>Mean IoU across all classes</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def compute_sample_iou(prediction: torch.Tensor, target: torch.Tensor, num_classes: int, ignore_index: int = 255) -&gt; float:\n    \"\"\"\n    Compute IoU for a single sample.\n\n    Args:\n        prediction: Predicted mask [H, W]\n        target: Ground truth mask [H, W]\n        num_classes: Number of classes\n        ignore_index: Index to ignore in computation\n\n    Returns:\n        iou: Mean IoU across all classes\n    \"\"\"\n    prediction = prediction.cpu().numpy()\n    target = target.cpu().numpy()\n\n    # Flatten arrays\n    pred_flat = prediction.flatten()\n    target_flat = target.flatten()\n\n    # Remove ignored pixels\n    valid_mask = target_flat != ignore_index\n    pred_flat = pred_flat[valid_mask]\n    target_flat = target_flat[valid_mask]\n\n    # Compute IoU for each class\n    ious = []\n    for cls in range(num_classes):\n        pred_cls = (pred_flat == cls)\n        target_cls = (target_flat == cls)\n\n        intersection = (pred_cls &amp; target_cls).sum()\n        union = pred_cls.sum() + target_cls.sum() - intersection\n\n        if union &gt; 0:\n            iou = intersection / union\n        else:\n            iou = 0.0\n\n        ious.append(iou)\n\n    # Return mean IoU\n    return np.mean(ious)\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(model, dataloader, device, num_classes=21)\n</code></pre> <p>Evaluate model on dataset and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to evaluate</p> required <code>dataloader</code> <p>Data loader</p> required <code>device</code> <p>Device to run on</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> <p>Returns:</p> Name Type Description <code>results</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor]]</code> <p>List of (sample_idx, iou, image, mask, prediction) tuples</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def evaluate_model(model, dataloader, device, num_classes: int = 21) -&gt; List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Evaluate model on dataset and collect results.\n\n    Args:\n        model: Model to evaluate\n        dataloader: Data loader\n        device: Device to run on\n        num_classes: Number of classes\n\n    Returns:\n        results: List of (sample_idx, iou, image, mask, prediction) tuples\n    \"\"\"\n    model.eval()\n    results = []\n\n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(dataloader):\n            images = images.to(device)\n            masks = masks.squeeze(1).long().to(device)\n\n            # Get predictions\n            if hasattr(model, 'predict'):\n                predictions = model.predict(images)\n            else:\n                # For VQ model\n                predictions = model.predict_with_vq(images)\n\n            # Process each sample in batch\n            for i in range(images.shape[0]):\n                image = images[i]\n                mask = masks[i]\n                pred = predictions[i]\n\n                # Compute IoU\n                iou = compute_sample_iou(pred, mask, num_classes)\n\n                # Store results\n                sample_idx = batch_idx * dataloader.batch_size + i\n                results.append((sample_idx, iou, image, mask, pred))\n\n    return results\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.find_best_worst_samples","title":"find_best_worst_samples","text":"<pre><code>find_best_worst_samples(results, n_best=5, n_worst=5)\n</code></pre> <p>Find best and worst samples based on IoU.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor]]</code> <p>List of (sample_idx, iou, image, mask, prediction) tuples</p> required <code>n_best</code> <code>int</code> <p>Number of best samples to return</p> <code>5</code> <code>n_worst</code> <code>int</code> <p>Number of worst samples to return</p> <code>5</code> <p>Returns:</p> Name Type Description <code>best_samples</code> <code>List</code> <p>List of best sample tuples</p> <code>worst_samples</code> <code>List</code> <p>List of worst sample tuples</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def find_best_worst_samples(results: List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor]], \n                           n_best: int = 5, n_worst: int = 5) -&gt; Tuple[List, List]:\n    \"\"\"\n    Find best and worst samples based on IoU.\n\n    Args:\n        results: List of (sample_idx, iou, image, mask, prediction) tuples\n        n_best: Number of best samples to return\n        n_worst: Number of worst samples to return\n\n    Returns:\n        best_samples: List of best sample tuples\n        worst_samples: List of worst sample tuples\n    \"\"\"\n    # Sort by IoU (descending)\n    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n\n    # Get best and worst\n    best_samples = sorted_results[:n_best]\n    worst_samples = sorted_results[-n_worst:]\n\n    return best_samples, worst_samples\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.prepare_visualization_data","title":"prepare_visualization_data","text":"<pre><code>prepare_visualization_data(\n    vq_model,\n    baseline_model,\n    dataloader,\n    device,\n    num_classes=21,\n    n_best=5,\n    n_worst=5,\n)\n</code></pre> <p>Prepare data for visualization by running both models and ranking results.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VQ model</p> required <code>baseline_model</code> <p>Baseline model</p> required <code>dataloader</code> <p>Data loader</p> required <code>device</code> <p>Device to run on</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> <code>n_best</code> <code>int</code> <p>Number of best samples</p> <code>5</code> <code>n_worst</code> <code>int</code> <p>Number of worst samples</p> <code>5</code> <p>Returns:</p> Name Type Description <code>best_samples</code> <p>List of best sample tuples with both predictions</p> <code>worst_samples</code> <p>List of worst sample tuples with both predictions</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def prepare_visualization_data(vq_model, baseline_model, dataloader, device, \n                              num_classes: int = 21, n_best: int = 5, n_worst: int = 5):\n    \"\"\"\n    Prepare data for visualization by running both models and ranking results.\n\n    Args:\n        vq_model: VQ model\n        baseline_model: Baseline model\n        dataloader: Data loader\n        device: Device to run on\n        num_classes: Number of classes\n        n_best: Number of best samples\n        n_worst: Number of worst samples\n\n    Returns:\n        best_samples: List of best sample tuples with both predictions\n        worst_samples: List of worst sample tuples with both predictions\n    \"\"\"\n    # Evaluate VQ model\n    print(\"Evaluating VQ model...\")\n    vq_results = evaluate_model(vq_model, dataloader, device, num_classes)\n\n    # Evaluate baseline model\n    print(\"Evaluating baseline model...\")\n    baseline_results = evaluate_model(baseline_model, dataloader, device, num_classes)\n\n    # Combine results (assuming same order)\n    combined_results = []\n    for (idx1, iou1, img1, mask1, pred_vq), (idx2, iou2, img2, mask2, pred_baseline) in zip(vq_results, baseline_results):\n        assert idx1 == idx2, \"Sample indices don't match\"\n        assert torch.equal(img1, img2), \"Images don't match\"\n        assert torch.equal(mask1, mask2), \"Masks don't match\"\n\n        combined_results.append((idx1, iou1, img1, mask1, pred_baseline, pred_vq))\n\n    # Find best and worst based on VQ IoU\n    best_samples, worst_samples = find_best_worst_samples(combined_results, n_best, n_worst)\n\n    return best_samples, worst_samples\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.visualize_comparison","title":"visualize_comparison","text":"<pre><code>visualize_comparison(\n    samples, title, output_path, num_classes=21\n)\n</code></pre> <p>Create visualization comparing baseline and VQ predictions.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor, Tensor]]</code> <p>List of (idx, iou, image, mask, pred_baseline, pred_vq) tuples</p> required <code>title</code> <code>str</code> <p>Figure title</p> required <code>output_path</code> <code>str</code> <p>Path to save figure</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def visualize_comparison(samples: List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]], \n                        title: str, output_path: str, num_classes: int = 21):\n    \"\"\"\n    Create visualization comparing baseline and VQ predictions.\n\n    Args:\n        samples: List of (idx, iou, image, mask, pred_baseline, pred_vq) tuples\n        title: Figure title\n        output_path: Path to save figure\n        num_classes: Number of classes\n    \"\"\"\n    n_samples = len(samples)\n    fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4 * n_samples))\n\n    if n_samples == 1:\n        axes = axes.reshape(1, -1)\n\n    colormap = create_segmentation_colormap(num_classes)\n\n    for i, (idx, iou, image, mask, pred_baseline, pred_vq) in enumerate(samples):\n        # Denormalize image\n        image_vis = denormalize_image(image)\n\n        # Convert to numpy for plotting\n        image_np = image_vis.permute(1, 2, 0).cpu().numpy()\n        mask_np = mask.cpu().numpy()\n        pred_baseline_np = pred_baseline.cpu().numpy()\n        pred_vq_np = pred_vq.cpu().numpy()\n\n        # Plot original image\n        axes[i, 0].imshow(image_np)\n        axes[i, 0].set_title(f\"Original Image\\nSample {idx}\")\n        axes[i, 0].axis('off')\n\n        # Plot ground truth\n        axes[i, 1].imshow(mask_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 1].set_title(\"Ground Truth\")\n        axes[i, 1].axis('off')\n\n        # Plot baseline prediction\n        axes[i, 2].imshow(pred_baseline_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 2].set_title(\"Baseline Prediction\")\n        axes[i, 2].axis('off')\n\n        # Plot VQ prediction\n        axes[i, 3].imshow(pred_vq_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 3].set_title(f\"VQ Prediction\\nIoU: {iou:.3f}\")\n        axes[i, 3].axis('off')\n\n    plt.suptitle(title, fontsize=16)\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"Visualization saved to: {output_path}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils-modules","title":"Modules","text":""},{"location":"api/all/#embeddings_squeeze.utils.comparison","title":"comparison","text":"<p>Comparison utilities for VQ vs baseline segmentation evaluation.</p>"},{"location":"api/all/#embeddings_squeeze.utils.comparison-functions","title":"Functions","text":"compute_sample_iou \u00b6 <pre><code>compute_sample_iou(\n    prediction, target, num_classes, ignore_index=255\n)\n</code></pre> <p>Compute IoU for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>Predicted mask [H, W]</p> required <code>target</code> <code>Tensor</code> <p>Ground truth mask [H, W]</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> required <code>ignore_index</code> <code>int</code> <p>Index to ignore in computation</p> <code>255</code> <p>Returns:</p> Name Type Description <code>iou</code> <code>float</code> <p>Mean IoU across all classes</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def compute_sample_iou(prediction: torch.Tensor, target: torch.Tensor, num_classes: int, ignore_index: int = 255) -&gt; float:\n    \"\"\"\n    Compute IoU for a single sample.\n\n    Args:\n        prediction: Predicted mask [H, W]\n        target: Ground truth mask [H, W]\n        num_classes: Number of classes\n        ignore_index: Index to ignore in computation\n\n    Returns:\n        iou: Mean IoU across all classes\n    \"\"\"\n    prediction = prediction.cpu().numpy()\n    target = target.cpu().numpy()\n\n    # Flatten arrays\n    pred_flat = prediction.flatten()\n    target_flat = target.flatten()\n\n    # Remove ignored pixels\n    valid_mask = target_flat != ignore_index\n    pred_flat = pred_flat[valid_mask]\n    target_flat = target_flat[valid_mask]\n\n    # Compute IoU for each class\n    ious = []\n    for cls in range(num_classes):\n        pred_cls = (pred_flat == cls)\n        target_cls = (target_flat == cls)\n\n        intersection = (pred_cls &amp; target_cls).sum()\n        union = pred_cls.sum() + target_cls.sum() - intersection\n\n        if union &gt; 0:\n            iou = intersection / union\n        else:\n            iou = 0.0\n\n        ious.append(iou)\n\n    # Return mean IoU\n    return np.mean(ious)\n</code></pre> evaluate_model \u00b6 <pre><code>evaluate_model(model, dataloader, device, num_classes=21)\n</code></pre> <p>Evaluate model on dataset and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to evaluate</p> required <code>dataloader</code> <p>Data loader</p> required <code>device</code> <p>Device to run on</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> <p>Returns:</p> Name Type Description <code>results</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor]]</code> <p>List of (sample_idx, iou, image, mask, prediction) tuples</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def evaluate_model(model, dataloader, device, num_classes: int = 21) -&gt; List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Evaluate model on dataset and collect results.\n\n    Args:\n        model: Model to evaluate\n        dataloader: Data loader\n        device: Device to run on\n        num_classes: Number of classes\n\n    Returns:\n        results: List of (sample_idx, iou, image, mask, prediction) tuples\n    \"\"\"\n    model.eval()\n    results = []\n\n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(dataloader):\n            images = images.to(device)\n            masks = masks.squeeze(1).long().to(device)\n\n            # Get predictions\n            if hasattr(model, 'predict'):\n                predictions = model.predict(images)\n            else:\n                # For VQ model\n                predictions = model.predict_with_vq(images)\n\n            # Process each sample in batch\n            for i in range(images.shape[0]):\n                image = images[i]\n                mask = masks[i]\n                pred = predictions[i]\n\n                # Compute IoU\n                iou = compute_sample_iou(pred, mask, num_classes)\n\n                # Store results\n                sample_idx = batch_idx * dataloader.batch_size + i\n                results.append((sample_idx, iou, image, mask, pred))\n\n    return results\n</code></pre> find_best_worst_samples \u00b6 <pre><code>find_best_worst_samples(results, n_best=5, n_worst=5)\n</code></pre> <p>Find best and worst samples based on IoU.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor]]</code> <p>List of (sample_idx, iou, image, mask, prediction) tuples</p> required <code>n_best</code> <code>int</code> <p>Number of best samples to return</p> <code>5</code> <code>n_worst</code> <code>int</code> <p>Number of worst samples to return</p> <code>5</code> <p>Returns:</p> Name Type Description <code>best_samples</code> <code>List</code> <p>List of best sample tuples</p> <code>worst_samples</code> <code>List</code> <p>List of worst sample tuples</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def find_best_worst_samples(results: List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor]], \n                           n_best: int = 5, n_worst: int = 5) -&gt; Tuple[List, List]:\n    \"\"\"\n    Find best and worst samples based on IoU.\n\n    Args:\n        results: List of (sample_idx, iou, image, mask, prediction) tuples\n        n_best: Number of best samples to return\n        n_worst: Number of worst samples to return\n\n    Returns:\n        best_samples: List of best sample tuples\n        worst_samples: List of worst sample tuples\n    \"\"\"\n    # Sort by IoU (descending)\n    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n\n    # Get best and worst\n    best_samples = sorted_results[:n_best]\n    worst_samples = sorted_results[-n_worst:]\n\n    return best_samples, worst_samples\n</code></pre> prepare_visualization_data \u00b6 <pre><code>prepare_visualization_data(\n    vq_model,\n    baseline_model,\n    dataloader,\n    device,\n    num_classes=21,\n    n_best=5,\n    n_worst=5,\n)\n</code></pre> <p>Prepare data for visualization by running both models and ranking results.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VQ model</p> required <code>baseline_model</code> <p>Baseline model</p> required <code>dataloader</code> <p>Data loader</p> required <code>device</code> <p>Device to run on</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> <code>n_best</code> <code>int</code> <p>Number of best samples</p> <code>5</code> <code>n_worst</code> <code>int</code> <p>Number of worst samples</p> <code>5</code> <p>Returns:</p> Name Type Description <code>best_samples</code> <p>List of best sample tuples with both predictions</p> <code>worst_samples</code> <p>List of worst sample tuples with both predictions</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def prepare_visualization_data(vq_model, baseline_model, dataloader, device, \n                              num_classes: int = 21, n_best: int = 5, n_worst: int = 5):\n    \"\"\"\n    Prepare data for visualization by running both models and ranking results.\n\n    Args:\n        vq_model: VQ model\n        baseline_model: Baseline model\n        dataloader: Data loader\n        device: Device to run on\n        num_classes: Number of classes\n        n_best: Number of best samples\n        n_worst: Number of worst samples\n\n    Returns:\n        best_samples: List of best sample tuples with both predictions\n        worst_samples: List of worst sample tuples with both predictions\n    \"\"\"\n    # Evaluate VQ model\n    print(\"Evaluating VQ model...\")\n    vq_results = evaluate_model(vq_model, dataloader, device, num_classes)\n\n    # Evaluate baseline model\n    print(\"Evaluating baseline model...\")\n    baseline_results = evaluate_model(baseline_model, dataloader, device, num_classes)\n\n    # Combine results (assuming same order)\n    combined_results = []\n    for (idx1, iou1, img1, mask1, pred_vq), (idx2, iou2, img2, mask2, pred_baseline) in zip(vq_results, baseline_results):\n        assert idx1 == idx2, \"Sample indices don't match\"\n        assert torch.equal(img1, img2), \"Images don't match\"\n        assert torch.equal(mask1, mask2), \"Masks don't match\"\n\n        combined_results.append((idx1, iou1, img1, mask1, pred_baseline, pred_vq))\n\n    # Find best and worst based on VQ IoU\n    best_samples, worst_samples = find_best_worst_samples(combined_results, n_best, n_worst)\n\n    return best_samples, worst_samples\n</code></pre> denormalize_image \u00b6 <pre><code>denormalize_image(\n    image,\n    mean=(0.485, 0.456, 0.406),\n    std=(0.229, 0.224, 0.225),\n)\n</code></pre> <p>Denormalize image for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Normalized image tensor [C, H, W]</p> required <code>mean</code> <code>Tuple[float, float, float]</code> <p>Normalization mean</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>Tuple[float, float, float]</code> <p>Normalization std</p> <code>(0.229, 0.224, 0.225)</code> <p>Returns:</p> Name Type Description <code>denormalized</code> <code>Tensor</code> <p>Denormalized image tensor</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def denormalize_image(image: torch.Tensor, mean: Tuple[float, float, float] = (0.485, 0.456, 0.406), \n                     std: Tuple[float, float, float] = (0.229, 0.224, 0.225)) -&gt; torch.Tensor:\n    \"\"\"\n    Denormalize image for visualization.\n\n    Args:\n        image: Normalized image tensor [C, H, W]\n        mean: Normalization mean\n        std: Normalization std\n\n    Returns:\n        denormalized: Denormalized image tensor\n    \"\"\"\n    image = image.clone()\n    for t, m, s in zip(image, mean, std):\n        t.mul_(s).add_(m)\n    return torch.clamp(image, 0, 1)\n</code></pre> create_segmentation_colormap \u00b6 <pre><code>create_segmentation_colormap(num_classes=21)\n</code></pre> <p>Create a colormap for segmentation visualization.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> <p>Returns:</p> Name Type Description <code>colormap</code> <code>ListedColormap</code> <p>Matplotlib colormap</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def create_segmentation_colormap(num_classes: int = 21) -&gt; ListedColormap:\n    \"\"\"\n    Create a colormap for segmentation visualization.\n\n    Args:\n        num_classes: Number of classes\n\n    Returns:\n        colormap: Matplotlib colormap\n    \"\"\"\n    # Generate distinct colors\n    colors = sns.color_palette(\"husl\", num_classes)\n    colors = [(0, 0, 0)] + colors  # Add black for background\n    return ListedColormap(colors)\n</code></pre> visualize_comparison \u00b6 <pre><code>visualize_comparison(\n    samples, title, output_path, num_classes=21\n)\n</code></pre> <p>Create visualization comparing baseline and VQ predictions.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor, Tensor]]</code> <p>List of (idx, iou, image, mask, pred_baseline, pred_vq) tuples</p> required <code>title</code> <code>str</code> <p>Figure title</p> required <code>output_path</code> <code>str</code> <p>Path to save figure</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def visualize_comparison(samples: List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]], \n                        title: str, output_path: str, num_classes: int = 21):\n    \"\"\"\n    Create visualization comparing baseline and VQ predictions.\n\n    Args:\n        samples: List of (idx, iou, image, mask, pred_baseline, pred_vq) tuples\n        title: Figure title\n        output_path: Path to save figure\n        num_classes: Number of classes\n    \"\"\"\n    n_samples = len(samples)\n    fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4 * n_samples))\n\n    if n_samples == 1:\n        axes = axes.reshape(1, -1)\n\n    colormap = create_segmentation_colormap(num_classes)\n\n    for i, (idx, iou, image, mask, pred_baseline, pred_vq) in enumerate(samples):\n        # Denormalize image\n        image_vis = denormalize_image(image)\n\n        # Convert to numpy for plotting\n        image_np = image_vis.permute(1, 2, 0).cpu().numpy()\n        mask_np = mask.cpu().numpy()\n        pred_baseline_np = pred_baseline.cpu().numpy()\n        pred_vq_np = pred_vq.cpu().numpy()\n\n        # Plot original image\n        axes[i, 0].imshow(image_np)\n        axes[i, 0].set_title(f\"Original Image\\nSample {idx}\")\n        axes[i, 0].axis('off')\n\n        # Plot ground truth\n        axes[i, 1].imshow(mask_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 1].set_title(\"Ground Truth\")\n        axes[i, 1].axis('off')\n\n        # Plot baseline prediction\n        axes[i, 2].imshow(pred_baseline_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 2].set_title(\"Baseline Prediction\")\n        axes[i, 2].axis('off')\n\n        # Plot VQ prediction\n        axes[i, 3].imshow(pred_vq_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 3].set_title(f\"VQ Prediction\\nIoU: {iou:.3f}\")\n        axes[i, 3].axis('off')\n\n    plt.suptitle(title, fontsize=16)\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"Visualization saved to: {output_path}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.compression","title":"compression","text":"<p>Compression analysis and metrics utilities.</p>"},{"location":"api/all/#embeddings_squeeze.utils.compression-functions","title":"Functions","text":"measure_compression \u00b6 <pre><code>measure_compression(\n    vq_model, backbone, test_loader, device\n)\n</code></pre> <p>Measure compression ratio achieved by VQ.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VectorQuantizer model</p> required <code>backbone</code> <p>Segmentation backbone</p> required <code>test_loader</code> <p>Test data loader</p> required <code>device</code> <p>Device to run on</p> required <p>Returns:</p> Name Type Description <code>compression_ratio</code> <p>Compression ratio achieved</p> Source code in <code>embeddings_squeeze\\utils\\compression.py</code> <pre><code>def measure_compression(vq_model, backbone, test_loader, device):\n    \"\"\"\n    Measure compression ratio achieved by VQ.\n\n    Args:\n        vq_model: VectorQuantizer model\n        backbone: Segmentation backbone\n        test_loader: Test data loader\n        device: Device to run on\n\n    Returns:\n        compression_ratio: Compression ratio achieved\n    \"\"\"\n    vq_model.eval()\n\n    total_original_bits = 0\n    total_compressed_bits = 0\n    num_features = 0\n\n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            features = backbone.extract_features(images)\n\n            B, C, H, W = features.shape\n            feat_flat = features.permute(0, 2, 3, 1).reshape(-1, C)\n\n            # Original size (float32 = 32 bits)\n            original_bits = feat_flat.numel() * 32\n\n            # Compressed size (only indices)\n            num_codes = vq_model.num_vectors\n            bits_per_index = np.ceil(np.log2(num_codes))\n            compressed_bits = feat_flat.shape[0] * bits_per_index\n\n            total_original_bits += original_bits\n            total_compressed_bits += compressed_bits\n            num_features += feat_flat.shape[0]\n\n    compression_ratio = total_original_bits / total_compressed_bits\n\n    print(\"=\"*70)\n    print(\"COMPRESSION ANALYSIS\")\n    print(\"=\"*70)\n    print(f\"Total features processed: {num_features}\")\n    print(f\"Feature dimension: {vq_model.vector_dim}\")\n    print(f\"Codebook size: {vq_model.num_vectors}\")\n    print()\n    print(f\"Original storage:\")\n    print(f\"  Per feature: {vq_model.vector_dim} \u00d7 32 bits = {vq_model.vector_dim * 32} bits = {vq_model.vector_dim * 4} bytes\")\n    print(f\"  Total: {total_original_bits / 8 / 1024 / 1024:.2f} MB\")\n    print()\n    print(f\"Compressed storage:\")\n    print(f\"  Per feature: {bits_per_index:.0f} bits (index)\")\n    print(f\"  Total: {total_compressed_bits / 8 / 1024:.2f} KB\")\n    print(f\"  + Codebook: {vq_model.num_vectors * vq_model.vector_dim * 4 / 1024:.2f} KB\")\n    print()\n    print(f\"Compression ratio: {compression_ratio:.1f}x\")\n    print(f\"Space savings: {(1 - 1/compression_ratio)*100:.1f}%\")\n    print(\"=\"*70)\n\n    return compression_ratio\n</code></pre> compute_iou_metrics \u00b6 <pre><code>compute_iou_metrics(\n    predictions, targets, num_classes, ignore_index=255\n)\n</code></pre> <p>Compute IoU metrics for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <p>Predicted masks [B, H, W]</p> required <code>targets</code> <p>Ground truth masks [B, H, W]</p> required <code>num_classes</code> <p>Number of classes</p> required <code>ignore_index</code> <p>Index to ignore in computation</p> <code>255</code> <p>Returns:</p> Name Type Description <code>metrics</code> <p>Dictionary with IoU metrics</p> Source code in <code>embeddings_squeeze\\utils\\compression.py</code> <pre><code>def compute_iou_metrics(predictions, targets, num_classes, ignore_index=255):\n    \"\"\"\n    Compute IoU metrics for segmentation.\n\n    Args:\n        predictions: Predicted masks [B, H, W]\n        targets: Ground truth masks [B, H, W]\n        num_classes: Number of classes\n        ignore_index: Index to ignore in computation\n\n    Returns:\n        metrics: Dictionary with IoU metrics\n    \"\"\"\n    predictions = predictions.cpu().numpy()\n    targets = targets.cpu().numpy()\n\n    # Flatten arrays\n    pred_flat = predictions.flatten()\n    target_flat = targets.flatten()\n\n    # Remove ignored pixels\n    valid_mask = target_flat != ignore_index\n    pred_flat = pred_flat[valid_mask]\n    target_flat = target_flat[valid_mask]\n\n    # Compute IoU for each class\n    ious = []\n    for cls in range(num_classes):\n        pred_cls = (pred_flat == cls)\n        target_cls = (target_flat == cls)\n\n        intersection = (pred_cls &amp; target_cls).sum()\n        union = pred_cls.sum() + target_cls.sum() - intersection\n\n        if union &gt; 0:\n            iou = intersection / union\n        else:\n            iou = 0.0\n\n        ious.append(iou)\n\n    # Compute mean IoU\n    mean_iou = np.mean(ious)\n\n    # Compute pixel accuracy\n    pixel_acc = (pred_flat == target_flat).mean()\n\n    metrics = {\n        'mean_iou': mean_iou,\n        'pixel_accuracy': pixel_acc,\n        'class_ious': ious\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.utils.initialization","title":"initialization","text":"<p>Codebook initialization utilities.</p>"},{"location":"api/all/#embeddings_squeeze.utils.initialization-functions","title":"Functions","text":"initialize_codebook_from_data \u00b6 <pre><code>initialize_codebook_from_data(\n    vq_model,\n    backbone,\n    train_loader,\n    device,\n    max_samples=50000,\n)\n</code></pre> <p>Initialize codebook using k-means clustering on real data.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VectorQuantizer model</p> required <code>backbone</code> <p>Segmentation backbone</p> required <code>train_loader</code> <p>Training data loader</p> required <code>device</code> <p>Device to run on</p> required <code>max_samples</code> <code>int</code> <p>Maximum number of samples for k-means</p> <code>50000</code> Source code in <code>embeddings_squeeze\\utils\\initialization.py</code> <pre><code>def initialize_codebook_from_data(\n    vq_model, \n    backbone, \n    train_loader, \n    device, \n    max_samples: int = 50_000\n):\n    \"\"\"\n    Initialize codebook using k-means clustering on real data.\n\n    Args:\n        vq_model: VectorQuantizer model\n        backbone: Segmentation backbone\n        train_loader: Training data loader\n        device: Device to run on\n        max_samples: Maximum number of samples for k-means\n    \"\"\"\n    print(\"Collecting features for k-means initialization...\")\n    all_features = []\n\n    backbone.eval()\n    with torch.no_grad():\n        i = 0\n        for images, _ in train_loader:\n            print(f\"Processing batch {i}\")\n            features = backbone.extract_features(images.to(device))\n            i += 1\n\n            B, C, H, W = features.shape\n            feat_flat = features.permute(0, 2, 3, 1).reshape(-1, C)\n            all_features.append(feat_flat.cpu())\n\n            if len(all_features) * feat_flat.shape[0] &gt; max_samples:\n                break\n\n    all_features = torch.cat(all_features).numpy()\n\n    print(f\"Running k-means on {all_features.shape[0]} features...\")\n    kmeans = MiniBatchKMeans(\n        n_clusters=vq_model.num_vectors,\n        random_state=0,\n        batch_size=1000,\n        max_iter=100\n    )\n    kmeans.fit(all_features)\n\n    # Update codebook with cluster centers\n    vq_model.codebook.embeddings.data = torch.tensor(kmeans.cluster_centers_).to(device).float()\n\n    print(f\"Codebook initialized:\")\n    print(f\"  Mean: {vq_model.codebook.embeddings.mean():.2f}\")\n    print(f\"  Std: {vq_model.codebook.embeddings.std():.2f}\")\n    print(f\"  Norm: {vq_model.codebook.embeddings.norm(dim=1).mean():.2f}\")\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.visualize","title":"visualize","text":"<p>Visualization script for comparing VQ vs baseline segmentation results.</p> Usage <p>python visualize.py --vq_checkpoint ./outputs/vq_squeeze/version_0/last.ckpt --baseline_checkpoint ./outputs/baseline_segmentation_baseline/version_0/last.ckpt</p>"},{"location":"api/all/#embeddings_squeeze.visualize-functions","title":"Functions","text":""},{"location":"api/all/#embeddings_squeeze.visualize.create_backbone","title":"create_backbone","text":"<pre><code>create_backbone(config)\n</code></pre> <p>Create segmentation backbone based on config.</p> Source code in <code>embeddings_squeeze\\visualize.py</code> <pre><code>def create_backbone(config):\n    \"\"\"Create segmentation backbone based on config.\"\"\"\n    if config.model.backbone.lower() == \"vit\":\n        backbone = ViTSegmentationBackbone(\n            num_classes=config.model.num_classes,\n            freeze_backbone=config.model.freeze_backbone\n        )\n    elif config.model.backbone.lower() == \"deeplab\":\n        backbone = DeepLabV3SegmentationBackbone(\n            weights_name=config.model.deeplab_weights,\n            num_classes=config.model.num_classes,\n            freeze_backbone=config.model.freeze_backbone\n        )\n    else:\n        raise ValueError(f\"Unknown backbone: {config.model.backbone}\")\n\n    return backbone\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.visualize.create_data_module","title":"create_data_module","text":"<pre><code>create_data_module(config)\n</code></pre> <p>Create data module based on config.</p> Source code in <code>embeddings_squeeze\\visualize.py</code> <pre><code>def create_data_module(config):\n    \"\"\"Create data module based on config.\"\"\"\n    if config.data.dataset.lower() == \"oxford_pet\":\n        data_module = OxfordPetDataModule(\n            data_path=config.data.data_path,\n            batch_size=config.training.batch_size,\n            num_workers=config.training.num_workers,\n            pin_memory=config.training.pin_memory,\n            image_size=config.data.image_size,\n            subset_size=config.data.subset_size\n        )\n    else:\n        raise ValueError(f\"Unknown dataset: {config.data.dataset}\")\n\n    return data_module\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.visualize.load_models","title":"load_models","text":"<pre><code>load_models(\n    vq_checkpoint_path,\n    baseline_checkpoint_path,\n    config,\n    device,\n)\n</code></pre> <p>Load VQ and baseline models from checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>vq_checkpoint_path</code> <code>str</code> <p>Path to VQ model checkpoint</p> required <code>baseline_checkpoint_path</code> <code>str</code> <p>Path to baseline model checkpoint</p> required <code>config</code> <p>Configuration object</p> required <code>device</code> <p>Device to load models on</p> required <p>Returns:</p> Name Type Description <code>vq_model</code> <p>Loaded VQ model</p> <code>baseline_model</code> <p>Loaded baseline model</p> Source code in <code>embeddings_squeeze\\visualize.py</code> <pre><code>def load_models(vq_checkpoint_path: str, baseline_checkpoint_path: str, config, device):\n    \"\"\"\n    Load VQ and baseline models from checkpoints.\n\n    Args:\n        vq_checkpoint_path: Path to VQ model checkpoint\n        baseline_checkpoint_path: Path to baseline model checkpoint\n        config: Configuration object\n        device: Device to load models on\n\n    Returns:\n        vq_model: Loaded VQ model\n        baseline_model: Loaded baseline model\n    \"\"\"\n    print(f\"Loading VQ model from: {vq_checkpoint_path}\")\n    vq_model = VQSqueezeModule.load_from_checkpoint(vq_checkpoint_path)\n    vq_model.to(device)\n    vq_model.eval()\n\n    print(f\"Loading baseline model from: {baseline_checkpoint_path}\")\n    baseline_model = BaselineSegmentationModule.load_from_checkpoint(baseline_checkpoint_path)\n    baseline_model.to(device)\n    baseline_model.eval()\n\n    return vq_model, baseline_model\n</code></pre>"},{"location":"api/all/#embeddings_squeeze.visualize.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main visualization function.</p> Source code in <code>embeddings_squeeze\\visualize.py</code> <pre><code>def main():\n    \"\"\"Main visualization function.\"\"\"\n    parser = argparse.ArgumentParser(description=\"VQ vs Baseline Segmentation Visualization\")\n\n    # Required checkpoint paths\n    parser.add_argument(\"--vq_checkpoint\", type=str, required=True,\n                       help=\"Path to VQ model checkpoint\")\n    parser.add_argument(\"--baseline_checkpoint\", type=str, required=True,\n                       help=\"Path to baseline model checkpoint\")\n\n    # Dataset arguments\n    parser.add_argument(\"--dataset_split\", type=str, default=\"test\",\n                       choices=[\"test\", \"val\"], help=\"Dataset split to use\")\n    parser.add_argument(\"--dataset\", type=str, default=\"oxford_pet\",\n                       choices=[\"oxford_pet\"], help=\"Dataset name\")\n    parser.add_argument(\"--data_path\", type=str, default=\"./data\",\n                       help=\"Path to dataset\")\n    parser.add_argument(\"--subset_size\", type=int, default=None,\n                       help=\"Subset size for quick testing\")\n\n    # Visualization arguments\n    parser.add_argument(\"--n_best\", type=int, default=5,\n                       help=\"Number of best results to show\")\n    parser.add_argument(\"--n_worst\", type=int, default=5,\n                       help=\"Number of worst results to show\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"./visualizations\",\n                       help=\"Output directory for visualization figures\")\n\n    # Model arguments (should match training config)\n    parser.add_argument(\"--model\", type=str, default=\"vit\",\n                       choices=[\"vit\", \"deeplab\"], help=\"Backbone model\")\n    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"Batch size\")\n\n    args = parser.parse_args()\n\n    # Create configuration\n    config = get_default_config()\n    config = update_config_from_args(config, vars(args))\n\n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Create output directory\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    # Load models\n    vq_model, baseline_model = load_models(\n        args.vq_checkpoint, \n        args.baseline_checkpoint, \n        config, \n        device\n    )\n\n    # Create data module\n    data_module = create_data_module(config)\n    data_module.setup(args.dataset_split)\n\n    # Get appropriate dataloader\n    if args.dataset_split == \"test\":\n        dataloader = data_module.test_dataloader()\n    else:\n        dataloader = data_module.val_dataloader()\n\n    print(f\"Evaluating on {args.dataset_split} split with {len(dataloader.dataset)} samples\")\n\n    # Prepare visualization data\n    best_samples, worst_samples = prepare_visualization_data(\n        vq_model=vq_model,\n        baseline_model=baseline_model,\n        dataloader=dataloader,\n        device=device,\n        num_classes=config.model.num_classes,\n        n_best=args.n_best,\n        n_worst=args.n_worst\n    )\n\n    # Create visualizations\n    print(f\"Creating visualization for {len(best_samples)} best samples...\")\n    best_output_path = os.path.join(args.output_dir, f\"best_results_{args.dataset_split}.png\")\n    visualize_comparison(\n        samples=best_samples,\n        title=f\"Best VQ Results ({args.dataset_split} split)\",\n        output_path=best_output_path,\n        num_classes=config.model.num_classes\n    )\n\n    print(f\"Creating visualization for {len(worst_samples)} worst samples...\")\n    worst_output_path = os.path.join(args.output_dir, f\"worst_results_{args.dataset_split}.png\")\n    visualize_comparison(\n        samples=worst_samples,\n        title=f\"Worst VQ Results ({args.dataset_split} split)\",\n        output_path=worst_output_path,\n        num_classes=config.model.num_classes\n    )\n\n    # Print summary statistics\n    best_ious = [sample[1] for sample in best_samples]\n    worst_ious = [sample[1] for sample in worst_samples]\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"VISUALIZATION SUMMARY\")\n    print(\"=\"*60)\n    print(f\"Dataset split: {args.dataset_split}\")\n    print(f\"Model backbone: {config.model.backbone}\")\n    print(f\"Number of classes: {config.model.num_classes}\")\n    print()\n    print(f\"Best VQ IoU scores: {[f'{iou:.3f}' for iou in best_ious]}\")\n    print(f\"Worst VQ IoU scores: {[f'{iou:.3f}' for iou in worst_ious]}\")\n    print(f\"Best IoU range: {min(best_ious):.3f} - {max(best_ious):.3f}\")\n    print(f\"Worst IoU range: {min(worst_ious):.3f} - {max(worst_ious):.3f}\")\n    print()\n    print(f\"Visualizations saved to:\")\n    print(f\"  Best results: {best_output_path}\")\n    print(f\"  Worst results: {worst_output_path}\")\n    print(\"=\"*60)\n</code></pre>"},{"location":"api/functions/","title":"\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0439","text":""},{"location":"api/functions/#embeddings_squeeze.utils","title":"utils","text":"<p>Utility functions for VQ compression.</p>"},{"location":"api/loggers/","title":"\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043b\u043e\u0433\u0433\u0435\u0440\u043e\u0432","text":""},{"location":"api/loggers/#embeddings_squeeze.loggers","title":"loggers","text":"<p>Logger integrations for embeddings_squeeze.</p>"},{"location":"api/loggers/#embeddings_squeeze.loggers-classes","title":"Classes","text":""},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger","title":"ClearMLLogger","text":"<pre><code>ClearMLLogger(task)\n</code></pre> <p>Wrapper for ClearML logging compatible with PyTorch Lightning. Supports scalar metrics, plots, images, and text logging.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def __init__(self, task: Task):\n    self.task = task\n    self.logger = task.get_logger() if task else None\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger-functions","title":"Functions","text":""},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.log_metrics","title":"log_metrics","text":"<pre><code>log_metrics(metrics, step=None)\n</code></pre> <p>Log metrics to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: int = None):\n    \"\"\"Log metrics to ClearML.\"\"\"\n    if self.logger is None:\n        return\n\n    for key, value in metrics.items():\n        # Split key into title and series (e.g., \"train/loss\" -&gt; title=\"train\", series=\"loss\")\n        if '/' in key:\n            title, series = key.split('/', 1)\n        else:\n            title = 'metrics'\n            series = key\n\n        self.logger.report_scalar(\n            title=title,\n            series=series,\n            value=value,\n            iteration=step\n        )\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.log_scalar","title":"log_scalar","text":"<pre><code>log_scalar(title, series, value, iteration)\n</code></pre> <p>Log a single scalar value to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Graph title (e.g., \"loss\", \"accuracy\")</p> required <code>series</code> <code>str</code> <p>Series name within the graph (e.g., \"train\", \"val\")</p> required <code>value</code> <code>float</code> <p>Scalar value to log</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100) logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_scalar(self, title: str, series: str, value: float, iteration: int):\n    \"\"\"\n    Log a single scalar value to ClearML.\n\n    Args:\n        title: Graph title (e.g., \"loss\", \"accuracy\")\n        series: Series name within the graph (e.g., \"train\", \"val\")\n        value: Scalar value to log\n        iteration: Iteration/step number\n\n    Example:\n        logger.log_scalar(\"loss\", \"train\", 0.5, iteration=100)\n        logger.log_scalar(\"loss\", \"val\", 0.3, iteration=100)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_scalar(\n        title=title,\n        series=series,\n        value=value,\n        iteration=iteration\n    )\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.log_image","title":"log_image","text":"<pre><code>log_image(title, series, image, iteration)\n</code></pre> <p>Log an image to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name (e.g., \"predictions\", \"ground_truth\")</p> required <code>image</code> <p>Image as numpy array (H, W) or (H, W, C) for grayscale/RGB    Supports uint8 (0-255) or float (0-1)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_image(self, title: str, series: str, image, iteration: int):\n    \"\"\"\n    Log an image to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name (e.g., \"predictions\", \"ground_truth\")\n        image: Image as numpy array (H, W) or (H, W, C) for grayscale/RGB\n               Supports uint8 (0-255) or float (0-1)\n        iteration: Iteration/step number\n\n    Example:\n        # Grayscale image\n        img = np.eye(256, 256, dtype=np.uint8) * 255\n        logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)\n\n        # RGB image\n        img_rgb = np.zeros((256, 256, 3), dtype=np.uint8)\n        img_rgb[:, :, 0] = 255  # Red channel\n        logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_image(\n        title=title,\n        series=series,\n        iteration=iteration,\n        image=image\n    )\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.log_image--grayscale-image","title":"Grayscale image","text":"<p>img = np.eye(256, 256, dtype=np.uint8) * 255 logger.log_image(\"predictions\", \"epoch_1\", img, iteration=0)</p>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.log_image--rgb-image","title":"RGB image","text":"<p>img_rgb = np.zeros((256, 256, 3), dtype=np.uint8) img_rgb[:, :, 0] = 255  # Red channel logger.log_image(\"predictions\", \"epoch_1_rgb\", img_rgb, iteration=0)</p>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.log_images_batch","title":"log_images_batch","text":"<pre><code>log_images_batch(title, series, images, iteration)\n</code></pre> <p>Log multiple images to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Image title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>images</code> <code>list</code> <p>List of images (numpy arrays)</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required Example <p>images = [img1, img2, img3] logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_images_batch(self, title: str, series: str, images: list, iteration: int):\n    \"\"\"\n    Log multiple images to ClearML.\n\n    Args:\n        title: Image title/group\n        series: Series name\n        images: List of images (numpy arrays)\n        iteration: Iteration/step number\n\n    Example:\n        images = [img1, img2, img3]\n        logger.log_images_batch(\"samples\", \"batch_0\", images, iteration=0)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    for idx, image in enumerate(images):\n        self.logger.report_image(\n            title=title,\n            series=f\"{series}_img_{idx}\",\n            iteration=iteration,\n            image=image\n        )\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.log_text","title":"log_text","text":"<pre><code>log_text(text, title='Info')\n</code></pre> <p>Log text to ClearML.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def log_text(self, text: str, title: str = \"Info\"):\n    \"\"\"Log text to ClearML.\"\"\"\n    if self.logger is None:\n        return\n    self.logger.report_text(text, print_console=True)\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.report_text","title":"report_text","text":"<pre><code>report_text(text)\n</code></pre> <p>Report text to ClearML (alias for log_text with default title).</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_text(self, text: str):\n    \"\"\"Report text to ClearML (alias for log_text with default title).\"\"\"\n    self.log_text(text)\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.report_plotly","title":"report_plotly","text":"<pre><code>report_plotly(title, series, iteration, figure)\n</code></pre> <p>Report a Plotly figure to ClearML.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Plot title/group</p> required <code>series</code> <code>str</code> <p>Series name</p> required <code>iteration</code> <code>int</code> <p>Iteration/step number</p> required <code>figure</code> <p>Plotly figure object</p> required Example <p>import plotly.graph_objects as go fig = go.Figure() fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data')) fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\") logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def report_plotly(self, title: str, series: str, iteration: int, figure):\n    \"\"\"\n    Report a Plotly figure to ClearML.\n\n    Args:\n        title: Plot title/group\n        series: Series name\n        iteration: Iteration/step number\n        figure: Plotly figure object\n\n    Example:\n        import plotly.graph_objects as go\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=[1,2,3], y=[4,5,6], mode='lines', name='data'))\n        fig.update_layout(title=\"My Plot\", xaxis_title=\"x\", yaxis_title=\"y\")\n        logger.report_plotly(\"metrics\", \"loss\", iteration=0, figure=fig)\n    \"\"\"\n    if self.logger is None:\n        return\n\n    self.logger.report_plotly(\n        title=title,\n        series=series,\n        iteration=iteration,\n        figure=figure\n    )\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLLogger.finalize","title":"finalize","text":"<pre><code>finalize()\n</code></pre> <p>Finalize logging and close task.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def finalize(self):\n    \"\"\"Finalize logging and close task.\"\"\"\n    if self.task:\n        self.task.close()\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLUploadCallback","title":"ClearMLUploadCallback","text":"<pre><code>ClearMLUploadCallback(\n    task,\n    clearml_logger=None,\n    checkpoint_dir=\"checkpoints\",\n    embedding_dir=\"embeddings\",\n)\n</code></pre> <p>               Bases: <code>Callback</code></p> <p>PyTorch Lightning callback for logging checkpoint and embedding paths to ClearML.</p> <p>Automatically logs local file paths for: - Latest checkpoint after each validation epoch - Per-epoch validation embeddings</p> Usage <p>from pytorch_lightning import Trainer from embeddings_squeeze.loggers import ClearMLUploadCallback, setup_clearml</p> <p>task = setup_clearml(project_name=\"my_project\", task_name=\"experiment_1\") logger = ClearMLLogger(task) if task else None callback = ClearMLUploadCallback(task, logger, checkpoint_dir=\"checkpoints\")</p> <p>trainer = Trainer(callbacks=[callback], ...)</p> <p>Initialize ClearML path logging callback.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>ClearML Task object</p> required <code>clearml_logger</code> <code>ClearMLLogger</code> <p>ClearML logger for text reporting (optional)</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>Directory containing checkpoints</p> <code>'checkpoints'</code> <code>embedding_dir</code> <code>str</code> <p>Directory containing embeddings</p> <code>'embeddings'</code> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def __init__(self, task: Task, clearml_logger: ClearMLLogger = None, \n             checkpoint_dir: str = \"checkpoints\", embedding_dir: str = \"embeddings\"):\n    \"\"\"\n    Initialize ClearML path logging callback.\n\n    Args:\n        task: ClearML Task object\n        clearml_logger: ClearML logger for text reporting (optional)\n        checkpoint_dir: Directory containing checkpoints\n        embedding_dir: Directory containing embeddings\n    \"\"\"\n    super().__init__()\n    self.task = task\n    self.clearml_logger = clearml_logger\n    self.checkpoint_dir = checkpoint_dir\n    self.embedding_dir = embedding_dir\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLUploadCallback-functions","title":"Functions","text":""},{"location":"api/loggers/#embeddings_squeeze.loggers.ClearMLUploadCallback.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end(trainer, pl_module)\n</code></pre> <p>Called after validation epoch ends.</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def on_validation_epoch_end(self, trainer, pl_module):\n    \"\"\"Called after validation epoch ends.\"\"\"\n    if self.task is None:\n        return\n\n    # Log checkpoint path\n    try:\n        ckpt_path = self._find_latest_checkpoint()\n        if ckpt_path:\n            abs_path = os.path.abspath(ckpt_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Checkpoint saved: {abs_path}\")\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed finding checkpoint: {e}\")\n\n    # Log embedding path\n    try:\n        emb_path = os.path.join(\n            self.embedding_dir, \n            f\"val_embedding_epoch{pl_module.current_epoch}.pt\"\n        )\n        if os.path.exists(emb_path):\n            abs_path = os.path.abspath(emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Embedding saved: {abs_path}\")\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed logging embedding path: {e}\")\n</code></pre>"},{"location":"api/loggers/#embeddings_squeeze.loggers-functions","title":"Functions","text":""},{"location":"api/loggers/#embeddings_squeeze.loggers.setup_clearml","title":"setup_clearml","text":"<pre><code>setup_clearml(project_name, task_name, auto_connect=True)\n</code></pre> <p>Setup ClearML with credentials from config file.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>ClearML project name</p> required <code>task_name</code> <code>str</code> <p>ClearML task name</p> required <code>auto_connect</code> <code>bool</code> <p>If True, automatically connect frameworks</p> <code>True</code> <p>Returns:</p> Type Description <p>Task object</p> Source code in <code>embeddings_squeeze\\loggers\\clearml_logger.py</code> <pre><code>def setup_clearml(project_name: str, task_name: str, auto_connect: bool = True):\n    \"\"\"\n    Setup ClearML with credentials from config file.\n\n    Args:\n        project_name: ClearML project name\n        task_name: ClearML task name\n        auto_connect: If True, automatically connect frameworks\n\n    Returns:\n        Task object\n    \"\"\"\n    # Load credentials\n    config_dir = Path(__file__).parent.parent / 'configs'\n\n    try:\n        creds = load_credentials(config_dir)\n\n        # Set credentials\n        clearml.Task.set_credentials(\n            api_host=creds.get('api_host', 'https://api.clear.ml'),\n            web_host=creds.get('web_host', 'https://app.clear.ml'),\n            files_host=creds.get('files_host', 'https://files.clear.ml'),\n            key=creds['api_key'],\n            secret=creds['api_secret']\n        )\n\n        # Initialize task\n        task = Task.init(\n            project_name=project_name,\n            task_name=task_name,\n            auto_connect_frameworks=auto_connect\n        )\n        return task\n    except FileNotFoundError as e:\n        print(f\"Warning: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n    except Exception as e:\n        print(f\"Warning: Failed to setup ClearML: {e}\")\n        print(\"ClearML logging disabled. Using TensorBoard instead.\")\n        return None\n</code></pre>"},{"location":"api/models/","title":"\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439","text":""},{"location":"api/models/#embeddings_squeeze.models","title":"models","text":"<p>Model architectures and components.</p>"},{"location":"api/models/#embeddings_squeeze.models-classes","title":"Classes","text":""},{"location":"api/models/#embeddings_squeeze.models.VQWithProjection","title":"VQWithProjection","text":"<pre><code>VQWithProjection(\n    input_dim,\n    codebook_size=512,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Vector Quantization (VQ-VAE) with projections</p> <p>Uses EMA for codebook updates (no gradients needed for codebook) ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection (e.g., 2048 -&gt; 64)\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Vector Quantization\n    self.vq = VectorQuantize(\n        dim=bottleneck_dim,\n        codebook_size=codebook_size,\n        decay=decay,  # EMA decay for codebook\n        commitment_weight=commitment_weight  # Commitment loss weight\n    )\n\n    # Up projection (64 -&gt; 2048)\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.FSQWithProjection","title":"FSQWithProjection","text":"<pre><code>FSQWithProjection(input_dim, levels=None)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Finite Scalar Quantization (FSQ)</p> <p>Quantization without codebook - each dimension quantized independently ~10 bits per vector at levels=[8,5,5,5]</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int, levels: list = None):\n    super().__init__(input_dim)\n    if levels is None:\n        levels = [8, 5, 5, 5]  # 8*5*5*5 = 1000 codes \u2248 2^10\n\n    self.num_levels = len(levels)\n\n    # Projection to quantization space\n    self.project_in = nn.Linear(input_dim, self.num_levels)\n\n    # FSQ quantization\n    self.fsq = FSQ(levels=levels, dim=self.num_levels)\n\n    # Projection back\n    self.project_out = nn.Linear(self.num_levels, input_dim)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.LFQWithProjection","title":"LFQWithProjection","text":"<pre><code>LFQWithProjection(\n    input_dim,\n    codebook_size=512,\n    entropy_loss_weight=0.1,\n    diversity_gamma=0.1,\n    spherical=False,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Lookup-Free Quantization (LFQ)</p> <p>Uses entropy loss for code diversity ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512,\n    entropy_loss_weight: float = 0.1, \n    diversity_gamma: float = 0.1, \n    spherical: bool = False\n):\n    super().__init__(input_dim)\n    # Quantization dimension = log2(codebook_size)\n    self.quant_dim = int(math.log2(codebook_size))\n\n    # Projection with normalization\n    self.project_in = nn.Sequential(\n        nn.Linear(input_dim, self.quant_dim),\n        nn.LayerNorm(self.quant_dim)\n    )\n\n    # LFQ quantization\n    self.lfq = LFQ(\n        dim=self.quant_dim,\n        codebook_size=codebook_size,\n        entropy_loss_weight=entropy_loss_weight,\n        diversity_gamma=diversity_gamma,\n        spherical=spherical\n    )\n\n    # Projection back\n    self.project_out = nn.Linear(self.quant_dim, input_dim)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.ResidualVQWithProjection","title":"ResidualVQWithProjection","text":"<pre><code>ResidualVQWithProjection(\n    input_dim,\n    num_quantizers=4,\n    codebook_size=256,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Residual Vector Quantization (RVQ)</p> <p>Multi-level quantization - each level quantizes the residual of the previous 32 bits per vector at num_quantizers=4, codebook_size=256 (4*8 bits)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    num_quantizers: int = 4,\n    codebook_size: int = 256, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Residual VQ\n    self.residual_vq = ResidualVQ(\n        dim=bottleneck_dim,\n        num_quantizers=num_quantizers,  # Number of levels\n        codebook_size=codebook_size,\n        decay=decay,\n        commitment_weight=commitment_weight\n    )\n\n    # Up projection\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaseQuantizer","title":"BaseQuantizer","text":"<pre><code>BaseQuantizer(input_dim)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for all quantizers</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int):\n    super().__init__()\n    self.input_dim = input_dim\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaseQuantizer-functions","title":"Functions","text":""},{"location":"api/models/#embeddings_squeeze.models.BaseQuantizer.quantize_spatial","title":"quantize_spatial","text":"<pre><code>quantize_spatial(features)\n</code></pre> <p>Quantize spatial features [B, C, H, W]</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>quantized</code> <p>Quantized features [B, C, H, W]</p> <code>loss</code> <p>Quantization loss (scalar)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def quantize_spatial(self, features: torch.Tensor):\n    \"\"\"\n    Quantize spatial features [B, C, H, W]\n\n    Args:\n        features: Tensor of shape [B, C, H, W]\n\n    Returns:\n        quantized: Quantized features [B, C, H, W]\n        loss: Quantization loss (scalar)\n    \"\"\"\n    B, C, H, W = features.shape\n    # Transform [B, C, H, W] -&gt; [B, H*W, C]\n    seq = features.permute(0, 2, 3, 1).reshape(B, H * W, C)\n\n    # Quantize\n    quantized, indices, loss = self.forward(seq)\n\n    # Transform back [B, H*W, C] -&gt; [B, C, H, W]\n    quantized = quantized.reshape(B, H, W, C).permute(0, 3, 1, 2)\n\n    # Handle loss (may be tensor with multiple elements)\n    if isinstance(loss, torch.Tensor) and loss.numel() &gt; 1:\n        loss = loss.mean()\n\n    return quantized, loss\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.DiceLoss","title":"DiceLoss","text":"<pre><code>DiceLoss(smooth=1.0)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Dice Loss for multi-class segmentation</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, smooth: float = 1.0):\n    super().__init__()\n    self.smooth = smooth\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.FocalLoss","title":"FocalLoss","text":"<pre><code>FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Focal Loss for handling class imbalance (multi-class via CE per-pixel)</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):\n    super().__init__()\n    self.alpha = alpha\n    self.gamma = gamma\n    self.reduction = reduction\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.CombinedLoss","title":"CombinedLoss","text":"<pre><code>CombinedLoss(\n    ce_weight=1.0,\n    dice_weight=1.0,\n    focal_weight=0.5,\n    class_weights=None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Combined loss: CE + Dice + Focal. Returns (total, ce, dice, focal).</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(\n    self, \n    ce_weight: float = 1.0, \n    dice_weight: float = 1.0, \n    focal_weight: float = 0.5, \n    class_weights=None\n):\n    super().__init__()\n    # class_weights can be None or a tensor/list\n    if class_weights is not None:\n        # Leave tensor creation to forward (to place on correct device) but store raw\n        self._class_weights = class_weights\n    else:\n        self._class_weights = None\n\n    self.ce_weight = ce_weight\n    self.dice_weight = dice_weight\n    self.focal_weight = focal_weight\n\n    # Instantiate component losses\n    self.dice_loss = DiceLoss()\n    self.focal_loss = FocalLoss()\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.SegmentationBackbone","title":"SegmentationBackbone","text":"<pre><code>SegmentationBackbone()\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation backbones.</p> <p>All segmentation backbones should inherit from this class and implement the required methods for feature extraction and full segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.SegmentationBackbone-attributes","title":"Attributes","text":""},{"location":"api/models/#embeddings_squeeze.models.SegmentationBackbone.feature_dim","title":"feature_dim  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>feature_dim\n</code></pre> <p>Return the feature dimension.</p>"},{"location":"api/models/#embeddings_squeeze.models.SegmentationBackbone.num_classes","title":"num_classes  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>num_classes\n</code></pre> <p>Return the number of output classes.</p>"},{"location":"api/models/#embeddings_squeeze.models.SegmentationBackbone-functions","title":"Functions","text":""},{"location":"api/models/#embeddings_squeeze.models.SegmentationBackbone.extract_features","title":"extract_features  <code>abstractmethod</code>","text":"<pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract features from input images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, feature_dim, H', W']</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef extract_features(self, images, detach=True):\n    \"\"\"\n    Extract features from input images.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, feature_dim, H', W']\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.SegmentationBackbone.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(images)\n</code></pre> <p>Full forward pass for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef forward(self, images):\n    \"\"\"\n    Full forward pass for segmentation.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule","title":"VQSqueezeModule","text":"<pre><code>VQSqueezeModule(\n    backbone,\n    quantizer=None,\n    num_classes=21,\n    learning_rate=0.0001,\n    vq_loss_weight=0.1,\n    loss_type=\"ce\",\n    class_weights=None,\n    add_adapter=False,\n    feature_dim=2048,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for VQ compression training.</p> <p>Features: - Multiple quantizer support (VQ, FSQ, LFQ, RVQ) - Adapter layers for fine-tuning frozen backbones - Advanced loss functions (CE, Dice, Focal, Combined) - Embedding extraction and saving</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    quantizer: Optional[nn.Module] = None,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    vq_loss_weight: float = 0.1,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    add_adapter: bool = False,\n    feature_dim: int = 2048,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'quantizer', 'clearml_logger'])\n\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n    self.vq_loss_weight = vq_loss_weight\n    self.loss_type = loss_type\n    self.add_adapter = add_adapter\n    self.feature_dim = feature_dim\n\n    # Setup backbone with optional adapters\n    self.backbone = backbone\n    self._setup_backbone_with_adapters(feature_dim, add_adapter)\n\n    # Quantizer (optional)\n    self.quantizer = quantizer\n\n    # Loss function\n    self.criterion = self._init_loss(loss_type, class_weights)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n\n    # Embedding storage (per-epoch, first batch only)\n    self.embedding_dir = \"embeddings\"\n    os.makedirs(self.embedding_dir, exist_ok=True)\n    self._first_val_batch_features = None\n\n    # UMAP visualization storage\n    self._val_backbone_embeddings = []\n    self._val_quantized_embeddings = []\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule-functions","title":"Functions","text":""},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule.forward","title":"forward","text":"<pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone + optional quantizer + decoder.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> <code>quant_loss</code> <p>Quantization loss (0 if no quantizer)</p> <code>original_features</code> <p>Extracted features (before quantization)</p> <code>quantized_features</code> <p>Features after quantization (same as original if no quantizer)</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone + optional quantizer + decoder.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n        quant_loss: Quantization loss (0 if no quantizer)\n        original_features: Extracted features (before quantization)\n        quantized_features: Features after quantization (same as original if no quantizer)\n    \"\"\"\n    # Extract features\n    features = self.backbone.extract_features(images, detach=self.feature_adapter is not None)\n\n    # Apply adapter if present\n    if self.feature_adapter is not None:\n        features = features + self.feature_adapter(features)\n\n    # Store original features for embedding extraction\n    original_features = features\n\n    # Quantize if quantizer is present\n    quant_loss = torch.tensor(0.0, device=images.device)\n    quantized_features = original_features  # Default to original if no quantizer\n    if self.quantizer is not None:\n        features, quant_loss = self.quantizer.quantize_spatial(features)\n        quantized_features = features\n\n    # Decode to segmentation logits\n    output = self.backbone.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear', align_corners=False)\n\n    return output, quant_loss, original_features, quantized_features\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, _, _ = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return loss\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, backbone_features, quantized_features = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    # Accumulate embeddings for UMAP visualization\n    self._val_backbone_embeddings.append(backbone_features.detach().cpu())\n    self._val_quantized_embeddings.append(quantized_features.detach().cpu())\n\n    # Save only first batch features for this epoch\n    if batch_idx == 0:\n        self._first_val_batch_features = backbone_features.detach().cpu()\n\n    return loss\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule.on_validation_epoch_start","title":"on_validation_epoch_start","text":"<pre><code>on_validation_epoch_start()\n</code></pre> <p>Clear accumulated embeddings at the start of each validation epoch.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_start(self):\n    \"\"\"Clear accumulated embeddings at the start of each validation epoch.\"\"\"\n    self._val_backbone_embeddings.clear()\n    self._val_quantized_embeddings.clear()\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations and save embeddings.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations and save embeddings.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    keys = [\n        \"train/loss\", \"val/loss\", \"train/iou\", \"val/iou\",\n        \"train/precision\", \"val/precision\", \"train/recall\", \"val/recall\",\n        \"train/f1\", \"val/f1\"\n    ]\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n\n    # Generate UMAP visualizations on even epochs\n    if self.current_epoch % 2 == 0:\n        try:\n            import umap.umap_ as umap_module\n\n            # Only proceed if we have embeddings\n            if len(self._val_backbone_embeddings) &gt; 0 and len(self._val_quantized_embeddings) &gt; 0:\n                # Concatenate all accumulated embeddings\n                backbone_emb_flat = torch.cat(self._val_backbone_embeddings, dim=0)\n                quantized_emb_flat = torch.cat(self._val_quantized_embeddings, dim=0)\n\n                # Flatten spatial dimensions: [B, C, H, W] -&gt; [B*H*W, C]\n                backbone_emb_flat = backbone_emb_flat.permute(0, 2, 3, 1).reshape(-1, backbone_emb_flat.shape[1])\n                quantized_emb_flat = quantized_emb_flat.permute(0, 2, 3, 1).reshape(-1, quantized_emb_flat.shape[1])\n\n                # Convert to numpy\n                backbone_emb_np = backbone_emb_flat.numpy()\n                quantized_emb_np = quantized_emb_flat.numpy()\n\n                # Limit samples for performance (take subset if too large)\n                max_samples = 10000\n                if len(backbone_emb_np) &gt; max_samples:\n                    indices = np.random.choice(len(backbone_emb_np), max_samples, replace=False)\n                    backbone_emb_np = backbone_emb_np[indices]\n                    quantized_emb_np = quantized_emb_np[indices]\n\n                # Generate 2D UMAP\n                fig_2d, axs_2d = plt.subplots(1, 2, figsize=(12, 6))\n\n                proj_2d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(backbone_emb_np)\n                axs_2d[0].scatter(proj_2d_backbone[:, 0], proj_2d_backbone[:, 1], alpha=0.3)\n                axs_2d[0].set_title('2D UMAP: Backbone Embeddings')\n\n                proj_2d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(quantized_emb_np)\n                axs_2d[1].scatter(proj_2d_quantized[:, 0], proj_2d_quantized[:, 1], alpha=0.3)\n                axs_2d[1].set_title('2D UMAP: Quantized Embeddings')\n\n                # Convert 2D plot to image and log\n                fig_2d.canvas.draw()\n                img_2d = np.frombuffer(fig_2d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_2d = img_2d.reshape(fig_2d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_2d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"2d_embeddings_epoch_{self.current_epoch}\", \n                        img_2d, \n                        iteration=self.current_epoch\n                    )\n\n                # Generate 3D UMAP\n                fig_3d = plt.figure(figsize=(12, 6))\n                ax1 = fig_3d.add_subplot(121, projection='3d')\n                ax2 = fig_3d.add_subplot(122, projection='3d')\n\n                proj_3d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(backbone_emb_np)\n                ax1.scatter(proj_3d_backbone[:, 0], proj_3d_backbone[:, 1], proj_3d_backbone[:, 2], alpha=0.3)\n                ax1.set_title('3D UMAP: Backbone Embeddings')\n\n                proj_3d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(quantized_emb_np)\n                ax2.scatter(proj_3d_quantized[:, 0], proj_3d_quantized[:, 1], proj_3d_quantized[:, 2], alpha=0.3)\n                ax2.set_title('3D UMAP: Quantized Embeddings')\n\n                # Convert 3D plot to image and log\n                fig_3d.canvas.draw()\n                img_3d = np.frombuffer(fig_3d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_3d = img_3d.reshape(fig_3d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_3d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"3d_embeddings_epoch_{self.current_epoch}\", \n                        img_3d, \n                        iteration=self.current_epoch\n                    )\n\n            # Clear accumulated embeddings after logging\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n        except Exception as e:\n            if self.clearml_logger:\n                self.clearml_logger.report_text(\n                    f\"UMAP visualization failed at epoch {self.current_epoch}: {e}\"\n                )\n            # Clear embeddings even if visualization failed\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n    # Save per-epoch embedding (first validation batch only)\n    try:\n        if self._first_val_batch_features is not None:\n            emb_path = os.path.join(\n                self.embedding_dir,\n                f\"val_embedding_epoch{self.current_epoch}.pt\"\n            )\n            torch.save(self._first_val_batch_features, emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Saved small embedding: {emb_path}\")\n            # Reset for next epoch\n            self._first_val_batch_features = None\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed saving epoch embedding: {e}\")\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer - only trainable parameters.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer - only trainable parameters.\"\"\"\n    params = []\n\n    # Add adapter parameters if present\n    if self.feature_adapter is not None:\n        params += list(self.feature_adapter.parameters())\n\n    # Add quantizer parameters if present\n    if self.quantizer is not None:\n        params += list(self.quantizer.parameters())\n\n    # Add backbone parameters if not frozen\n    if self.feature_adapter is None:\n        params += [p for p in self.backbone.parameters() if p.requires_grad]\n\n    # Remove duplicates\n    params = list({id(p): p for p in params}.values())\n\n    if not params:\n        raise ValueError(\"No trainable parameters found!\")\n\n    return torch.optim.AdamW(params, lr=self.learning_rate)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.VQSqueezeModule.on_train_start","title":"on_train_start","text":"<pre><code>on_train_start()\n</code></pre> <p>Ensure frozen backbone stays in eval mode.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_train_start(self):\n    \"\"\"Ensure frozen backbone stays in eval mode.\"\"\"\n    if self.feature_adapter is not None:\n        self.backbone.eval()\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule","title":"BaselineSegmentationModule","text":"<pre><code>BaselineSegmentationModule(\n    backbone,\n    num_classes=21,\n    learning_rate=0.0001,\n    loss_type=\"ce\",\n    class_weights=None,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for baseline segmentation training.</p> <p>Wraps segmentation backbone without Vector Quantization for comparison.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'clearml_logger'])\n\n    self.backbone = backbone\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n\n    # Segmentation loss\n    if class_weights is not None:\n        weight = torch.tensor(class_weights, dtype=torch.float32)\n        self.seg_criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=255)\n    else:\n        self.seg_criterion = nn.CrossEntropyLoss(ignore_index=255)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule-functions","title":"Functions","text":""},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule.forward","title":"forward","text":"<pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    output = self.backbone(images)\n    # Handle both dict and tensor returns\n    if isinstance(output, dict):\n        return output['out']\n    return output\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', seg_loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer.\"\"\"\n    # Optimize only trainable params\n    params = [p for p in self.parameters() if p.requires_grad]\n    return torch.optim.Adam(params, lr=self.learning_rate)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule.predict","title":"predict","text":"<pre><code>predict(images)\n</code></pre> <p>Predict segmentation masks.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <p>Segmentation predictions [B, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict(self, images):\n    \"\"\"\n    Predict segmentation masks.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        predictions: Segmentation predictions [B, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        output = self(images)\n        predictions = output.argmax(dim=1)\n    return predictions\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.BaselineSegmentationModule.predict_logits","title":"predict_logits","text":"<pre><code>predict_logits(images)\n</code></pre> <p>Predict segmentation logits.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>logits</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict_logits(self, images):\n    \"\"\"\n    Predict segmentation logits.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        logits: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        logits = self(images)\n    return logits\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models-modules","title":"Modules","text":""},{"location":"api/models/#embeddings_squeeze.models.backbones","title":"backbones","text":"<p>Segmentation backbone implementations.</p>"},{"location":"api/models/#embeddings_squeeze.models.backbones-classes","title":"Classes","text":""},{"location":"api/models/#embeddings_squeeze.models.backbones.SegmentationBackbone","title":"SegmentationBackbone","text":"<pre><code>SegmentationBackbone()\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation backbones.</p> <p>All segmentation backbones should inherit from this class and implement the required methods for feature extraction and full segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Attributes\u00b6 feature_dim <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return the feature dimension.</p> num_classes <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return the number of output classes.</p> Functions\u00b6 extract_features <code>abstractmethod</code> \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract features from input images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, feature_dim, H', W']</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef extract_features(self, images, detach=True):\n    \"\"\"\n    Extract features from input images.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, feature_dim, H', W']\n    \"\"\"\n    pass\n</code></pre> forward <code>abstractmethod</code> \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full forward pass for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef forward(self, images):\n    \"\"\"\n    Full forward pass for segmentation.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.backbones.ViTSegmentationBackbone","title":"ViTSegmentationBackbone","text":"<pre><code>ViTSegmentationBackbone(\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>ViT-based segmentation backbone.</p> <p>Uses ViT-B/32 as backbone with custom segmentation head.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def __init__(\n    self,\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n    base_vit = model_fn(weights=weights)\n    self.backbone = _ViTBackboneWrapper(base_vit, freeze=freeze_backbone)\n    self.classifier = _ViTSegmentationHead(self.backbone.hidden_dim, num_classes)\n\n    self._num_classes = num_classes\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return ViT hidden dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract ViT backbone feature maps.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, hidden_dim, H/patch, W/patch]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract ViT backbone feature maps.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, hidden_dim, H/patch, W/patch]\n    \"\"\"\n    feats = self.backbone(images)['out']\n    return feats.detach() if detach else feats\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full ViT segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full ViT segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    logits = self.classifier(features)\n    logits = F.interpolate(logits, size=images.shape[-2:], mode='bilinear', align_corners=False)\n    return {'out': logits}\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.backbones.DeepLabV3SegmentationBackbone","title":"DeepLabV3SegmentationBackbone","text":"<pre><code>DeepLabV3SegmentationBackbone(\n    weights_name=\"COCO_WITH_VOC_LABELS_V1\",\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>DeepLabV3-ResNet50 segmentation backbone.</p> <p>Uses pre-trained DeepLabV3-ResNet50 for segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def __init__(\n    self,\n    weights_name='COCO_WITH_VOC_LABELS_V1',\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n\n    weights = getattr(DeepLabV3_ResNet50_Weights, weights_name)\n    model = deeplabv3_resnet50(weights=weights)\n\n    self.backbone = model.backbone\n    self.classifier = model.classifier\n\n    self._num_classes = num_classes\n\n    if freeze_backbone:\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        self.backbone.eval()\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return DeepLabV3 feature dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract DeepLabV3 backbone features.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, 2048, H/8, W/8]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract DeepLabV3 backbone features.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, 2048, H/8, W/8]\n    \"\"\"\n    with torch.set_grad_enabled(not detach):\n        features = self.backbone(images)['out']\n    return features\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full DeepLabV3 segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full DeepLabV3 segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    output = self.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear')\n    return {'out': output}\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.backbones-modules","title":"Modules","text":""},{"location":"api/models/#embeddings_squeeze.models.backbones.base","title":"base","text":"<p>Abstract base class for segmentation backbones.</p> Classes\u00b6 SegmentationBackbone \u00b6 <pre><code>SegmentationBackbone()\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Abstract base class for segmentation backbones.</p> <p>All segmentation backbones should inherit from this class and implement the required methods for feature extraction and full segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre> Attributes\u00b6 feature_dim <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return the feature dimension.</p> num_classes <code>abstractmethod</code> <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return the number of output classes.</p> Functions\u00b6 extract_features <code>abstractmethod</code> \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract features from input images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, feature_dim, H', W']</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef extract_features(self, images, detach=True):\n    \"\"\"\n    Extract features from input images.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, feature_dim, H', W']\n    \"\"\"\n    pass\n</code></pre> forward <code>abstractmethod</code> \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full forward pass for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\base.py</code> <pre><code>@abstractmethod\ndef forward(self, images):\n    \"\"\"\n    Full forward pass for segmentation.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.backbones.deeplab","title":"deeplab","text":"<p>DeepLabV3-ResNet50 segmentation backbone implementation.</p> Classes\u00b6 DeepLabV3SegmentationBackbone \u00b6 <pre><code>DeepLabV3SegmentationBackbone(\n    weights_name=\"COCO_WITH_VOC_LABELS_V1\",\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>DeepLabV3-ResNet50 segmentation backbone.</p> <p>Uses pre-trained DeepLabV3-ResNet50 for segmentation.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def __init__(\n    self,\n    weights_name='COCO_WITH_VOC_LABELS_V1',\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n\n    weights = getattr(DeepLabV3_ResNet50_Weights, weights_name)\n    model = deeplabv3_resnet50(weights=weights)\n\n    self.backbone = model.backbone\n    self.classifier = model.classifier\n\n    self._num_classes = num_classes\n\n    if freeze_backbone:\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        self.backbone.eval()\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return DeepLabV3 feature dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract DeepLabV3 backbone features.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, 2048, H/8, W/8]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract DeepLabV3 backbone features.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, 2048, H/8, W/8]\n    \"\"\"\n    with torch.set_grad_enabled(not detach):\n        features = self.backbone(images)['out']\n    return features\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full DeepLabV3 segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\deeplab.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full DeepLabV3 segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    output = self.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear')\n    return {'out': output}\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.backbones.vit","title":"vit","text":"<p>ViT-based segmentation backbone implementation.</p> Classes\u00b6 ViTSegmentationBackbone \u00b6 <pre><code>ViTSegmentationBackbone(\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone=True,\n)\n</code></pre> <p>               Bases: <code>SegmentationBackbone</code></p> <p>ViT-based segmentation backbone.</p> <p>Uses ViT-B/32 as backbone with custom segmentation head.</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def __init__(\n    self,\n    model_fn=vit_b_32,\n    weights=ViT_B_32_Weights.IMAGENET1K_V1,\n    num_classes=21,\n    freeze_backbone: bool = True,\n):\n    super().__init__()\n    base_vit = model_fn(weights=weights)\n    self.backbone = _ViTBackboneWrapper(base_vit, freeze=freeze_backbone)\n    self.classifier = _ViTSegmentationHead(self.backbone.hidden_dim, num_classes)\n\n    self._num_classes = num_classes\n</code></pre> Attributes\u00b6 feature_dim <code>property</code> \u00b6 <pre><code>feature_dim\n</code></pre> <p>Return ViT hidden dimension.</p> num_classes <code>property</code> \u00b6 <pre><code>num_classes\n</code></pre> <p>Return number of segmentation classes.</p> Functions\u00b6 extract_features \u00b6 <pre><code>extract_features(images, detach=True)\n</code></pre> <p>Extract ViT backbone feature maps.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <code>detach</code> <p>Whether to detach gradients from backbone</p> <code>True</code> <p>Returns:</p> Name Type Description <code>features</code> <p>Feature maps [B, hidden_dim, H/patch, W/patch]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def extract_features(self, images, detach=True):\n    \"\"\"\n    Extract ViT backbone feature maps.\n\n    Args:\n        images: Input images [B, C, H, W]\n        detach: Whether to detach gradients from backbone\n\n    Returns:\n        features: Feature maps [B, hidden_dim, H/patch, W/patch]\n    \"\"\"\n    feats = self.backbone(images)['out']\n    return feats.detach() if detach else feats\n</code></pre> forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Full ViT segmentation forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\backbones\\vit.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Full ViT segmentation forward pass.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    features = self.backbone(images)['out']\n    logits = self.classifier(features)\n    logits = F.interpolate(logits, size=images.shape[-2:], mode='bilinear', align_corners=False)\n    return {'out': logits}\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.baseline_module","title":"baseline_module","text":"<p>PyTorch Lightning module for baseline segmentation training without VQ.</p>"},{"location":"api/models/#embeddings_squeeze.models.baseline_module-classes","title":"Classes","text":""},{"location":"api/models/#embeddings_squeeze.models.baseline_module.BaselineSegmentationModule","title":"BaselineSegmentationModule","text":"<pre><code>BaselineSegmentationModule(\n    backbone,\n    num_classes=21,\n    learning_rate=0.0001,\n    loss_type=\"ce\",\n    class_weights=None,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for baseline segmentation training.</p> <p>Wraps segmentation backbone without Vector Quantization for comparison.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'clearml_logger'])\n\n    self.backbone = backbone\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n\n    # Segmentation loss\n    if class_weights is not None:\n        weight = torch.tensor(class_weights, dtype=torch.float32)\n        self.seg_criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=255)\n    else:\n        self.seg_criterion = nn.CrossEntropyLoss(ignore_index=255)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n</code></pre> Functions\u00b6 forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    output = self.backbone(images)\n    # Handle both dict and tensor returns\n    if isinstance(output, dict):\n        return output['out']\n    return output\n</code></pre> training_step \u00b6 <pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', seg_loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre> validation_step \u00b6 <pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n    masks = masks.squeeze(1).long()\n\n    # Forward pass\n    output = self(images)\n\n    # Compute loss\n    seg_loss = self.seg_criterion(output, masks)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', seg_loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    return seg_loss\n</code></pre> on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n</code></pre> configure_optimizers \u00b6 <pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer.</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer.\"\"\"\n    # Optimize only trainable params\n    params = [p for p in self.parameters() if p.requires_grad]\n    return torch.optim.Adam(params, lr=self.learning_rate)\n</code></pre> predict \u00b6 <pre><code>predict(images)\n</code></pre> <p>Predict segmentation masks.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <p>Segmentation predictions [B, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict(self, images):\n    \"\"\"\n    Predict segmentation masks.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        predictions: Segmentation predictions [B, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        output = self(images)\n        predictions = output.argmax(dim=1)\n    return predictions\n</code></pre> predict_logits \u00b6 <pre><code>predict_logits(images)\n</code></pre> <p>Predict segmentation logits.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>logits</code> <p>Segmentation logits [B, num_classes, H, W]</p> Source code in <code>embeddings_squeeze\\models\\baseline_module.py</code> <pre><code>def predict_logits(self, images):\n    \"\"\"\n    Predict segmentation logits.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        logits: Segmentation logits [B, num_classes, H, W]\n    \"\"\"\n    self.eval()\n    with torch.no_grad():\n        logits = self(images)\n    return logits\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.lightning_module","title":"lightning_module","text":"<p>PyTorch Lightning module for VQ compression training with advanced features. Supports multiple quantizers (VQ, FSQ, LFQ, RVQ), adapters, and loss functions.</p>"},{"location":"api/models/#embeddings_squeeze.models.lightning_module-classes","title":"Classes","text":""},{"location":"api/models/#embeddings_squeeze.models.lightning_module.VQSqueezeModule","title":"VQSqueezeModule","text":"<pre><code>VQSqueezeModule(\n    backbone,\n    quantizer=None,\n    num_classes=21,\n    learning_rate=0.0001,\n    vq_loss_weight=0.1,\n    loss_type=\"ce\",\n    class_weights=None,\n    add_adapter=False,\n    feature_dim=2048,\n    clearml_logger=None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for VQ compression training.</p> <p>Features: - Multiple quantizer support (VQ, FSQ, LFQ, RVQ) - Adapter layers for fine-tuning frozen backbones - Advanced loss functions (CE, Dice, Focal, Combined) - Embedding extraction and saving</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def __init__(\n    self,\n    backbone: SegmentationBackbone,\n    quantizer: Optional[nn.Module] = None,\n    num_classes: int = 21,\n    learning_rate: float = 1e-4,\n    vq_loss_weight: float = 0.1,\n    loss_type: str = 'ce',\n    class_weights: Optional[list] = None,\n    add_adapter: bool = False,\n    feature_dim: int = 2048,\n    clearml_logger: Optional[Any] = None,\n    **kwargs\n):\n    super().__init__()\n    self.save_hyperparameters(ignore=['backbone', 'quantizer', 'clearml_logger'])\n\n    self.num_classes = num_classes\n    self.learning_rate = learning_rate\n    self.vq_loss_weight = vq_loss_weight\n    self.loss_type = loss_type\n    self.add_adapter = add_adapter\n    self.feature_dim = feature_dim\n\n    # Setup backbone with optional adapters\n    self.backbone = backbone\n    self._setup_backbone_with_adapters(feature_dim, add_adapter)\n\n    # Quantizer (optional)\n    self.quantizer = quantizer\n\n    # Loss function\n    self.criterion = self._init_loss(loss_type, class_weights)\n\n    # Metrics\n    self.train_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.val_iou = JaccardIndex(task=\"multiclass\", num_classes=num_classes)\n    self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n    self.train_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_prec = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_rec = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n    self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n\n    # Epoch-wise stats tracking for Plotly\n    self.epoch_stats: Dict[str, list] = {\n        \"train_loss\": [], \"val_loss\": [], \n        \"train_iou\": [], \"val_iou\": [],\n        \"train_precision\": [], \"val_precision\": [], \n        \"train_recall\": [], \"val_recall\": [],\n        \"train_f1\": [], \"val_f1\": []\n    }\n\n    # ClearML logger\n    self.clearml_logger = clearml_logger\n\n    # Embedding storage (per-epoch, first batch only)\n    self.embedding_dir = \"embeddings\"\n    os.makedirs(self.embedding_dir, exist_ok=True)\n    self._first_val_batch_features = None\n\n    # UMAP visualization storage\n    self._val_backbone_embeddings = []\n    self._val_quantized_embeddings = []\n</code></pre> Functions\u00b6 forward \u00b6 <pre><code>forward(images)\n</code></pre> <p>Forward pass through backbone + optional quantizer + decoder.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>Input images [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>output</code> <p>Segmentation logits [B, num_classes, H, W]</p> <code>quant_loss</code> <p>Quantization loss (0 if no quantizer)</p> <code>original_features</code> <p>Extracted features (before quantization)</p> <code>quantized_features</code> <p>Features after quantization (same as original if no quantizer)</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def forward(self, images):\n    \"\"\"\n    Forward pass through backbone + optional quantizer + decoder.\n\n    Args:\n        images: Input images [B, C, H, W]\n\n    Returns:\n        output: Segmentation logits [B, num_classes, H, W]\n        quant_loss: Quantization loss (0 if no quantizer)\n        original_features: Extracted features (before quantization)\n        quantized_features: Features after quantization (same as original if no quantizer)\n    \"\"\"\n    # Extract features\n    features = self.backbone.extract_features(images, detach=self.feature_adapter is not None)\n\n    # Apply adapter if present\n    if self.feature_adapter is not None:\n        features = features + self.feature_adapter(features)\n\n    # Store original features for embedding extraction\n    original_features = features\n\n    # Quantize if quantizer is present\n    quant_loss = torch.tensor(0.0, device=images.device)\n    quantized_features = original_features  # Default to original if no quantizer\n    if self.quantizer is not None:\n        features, quant_loss = self.quantizer.quantize_spatial(features)\n        quantized_features = features\n\n    # Decode to segmentation logits\n    output = self.backbone.classifier(features)\n    output = F.interpolate(output, size=images.shape[-2:], mode='bilinear', align_corners=False)\n\n    return output, quant_loss, original_features, quantized_features\n</code></pre> training_step \u00b6 <pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, _, _ = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.train_iou(output, masks)\n    acc = self.train_acc(output, masks)\n    prec = self.train_prec(output, masks)\n    rec = self.train_rec(output, masks)\n    f1 = self.train_f1(output, masks)\n\n    # Log metrics\n    self.log('train_step/loss', loss, on_step=True, on_epoch=False, prog_bar=False)\n\n    self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('train/precision', prec, on_step=False, on_epoch=True)\n    self.log('train/recall', rec, on_step=False, on_epoch=True)\n    self.log('train/f1', f1, on_step=False, on_epoch=True)\n\n    return loss\n</code></pre> validation_step \u00b6 <pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    images, masks = batch\n\n    # Handle mask dimensions\n    if masks.dim() == 4:\n        masks = masks.squeeze(1)\n    masks = masks.long()\n\n    # Forward pass\n    output, quant_loss, backbone_features, quantized_features = self(images)\n\n    # Compute loss\n    loss = self._compute_loss(output, masks, quant_loss)\n\n    # Compute metrics\n    iou = self.val_iou(output, masks)\n    acc = self.val_acc(output, masks)\n    prec = self.val_prec(output, masks)\n    rec = self.val_rec(output, masks)\n    f1 = self.val_f1(output, masks)\n\n    # Log metrics\n    self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/iou', iou, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n    self.log('val/precision', prec, on_step=False, on_epoch=True)\n    self.log('val/recall', rec, on_step=False, on_epoch=True)\n    self.log('val/f1', f1, on_step=False, on_epoch=True)\n\n    # Accumulate embeddings for UMAP visualization\n    self._val_backbone_embeddings.append(backbone_features.detach().cpu())\n    self._val_quantized_embeddings.append(quantized_features.detach().cpu())\n\n    # Save only first batch features for this epoch\n    if batch_idx == 0:\n        self._first_val_batch_features = backbone_features.detach().cpu()\n\n    return loss\n</code></pre> on_validation_epoch_start \u00b6 <pre><code>on_validation_epoch_start()\n</code></pre> <p>Clear accumulated embeddings at the start of each validation epoch.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_start(self):\n    \"\"\"Clear accumulated embeddings at the start of each validation epoch.\"\"\"\n    self._val_backbone_embeddings.clear()\n    self._val_quantized_embeddings.clear()\n</code></pre> on_validation_epoch_end \u00b6 <pre><code>on_validation_epoch_end()\n</code></pre> <p>Called after validation epoch ends - log Plotly visualizations and save embeddings.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_validation_epoch_end(self):\n    \"\"\"Called after validation epoch ends - log Plotly visualizations and save embeddings.\"\"\"\n    # Collect epoch stats from trainer callback metrics\n    cm = self.trainer.callback_metrics\n\n    def push_if_exists(k_from, k_to):\n        \"\"\"Helper to extract metrics from callback_metrics.\"\"\"\n        if k_from in cm:\n            val = cm[k_from]\n            try:\n                v = float(val)\n            except Exception:\n                v = val.item()\n            self.epoch_stats[k_to].append(v)\n\n    # Push metrics to epoch_stats\n    keys = [\n        \"train/loss\", \"val/loss\", \"train/iou\", \"val/iou\",\n        \"train/precision\", \"val/precision\", \"train/recall\", \"val/recall\",\n        \"train/f1\", \"val/f1\"\n    ]\n    key_mapping = {\n        \"train/loss\": \"train_loss\", \"val/loss\": \"val_loss\",\n        \"train/iou\": \"train_iou\", \"val/iou\": \"val_iou\",\n        \"train/precision\": \"train_precision\", \"val/precision\": \"val_precision\",\n        \"train/recall\": \"train_recall\", \"val/recall\": \"val_recall\",\n        \"train/f1\": \"train_f1\", \"val/f1\": \"val_f1\"\n    }\n    for k_from, k_to in key_mapping.items():\n        push_if_exists(k_from, k_to)\n\n    # Generate Plotly visualizations\n    try:\n        import plotly.graph_objects as go\n\n        epoch = self.current_epoch\n        epochs = list(range(len(self.epoch_stats[\"val_loss\"])))\n\n        # Loss plot\n        fig_loss = go.Figure()\n        if len(self.epoch_stats[\"train_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"train_loss\"],\n                mode=\"lines+markers\", name=\"train_loss\"\n            ))\n        if len(self.epoch_stats[\"val_loss\"]) &gt; 0:\n            fig_loss.add_trace(go.Scatter(\n                x=epochs, y=self.epoch_stats[\"val_loss\"],\n                mode=\"lines+markers\", name=\"val_loss\"\n            ))\n        fig_loss.update_layout(title=\"Loss\", xaxis_title=\"epoch\", yaxis_title=\"loss\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Loss\", series=\"loss\", iteration=epoch, figure=fig_loss\n            )\n\n        # Metrics plot\n        fig_m = go.Figure()\n        metrics_to_plot = [\n            (\"train_iou\", \"val_iou\"),\n            (\"train_precision\", \"val_precision\"),\n            (\"train_recall\", \"val_recall\"),\n            (\"train_f1\", \"val_f1\")\n        ]\n        for train_k, val_k in metrics_to_plot:\n            if len(self.epoch_stats[train_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[train_k],\n                    mode=\"lines+markers\", name=train_k\n                ))\n            if len(self.epoch_stats[val_k]) &gt; 0:\n                fig_m.add_trace(go.Scatter(\n                    x=epochs, y=self.epoch_stats[val_k],\n                    mode=\"lines+markers\", name=val_k\n                ))\n        fig_m.update_layout(title=\"Metrics\", xaxis_title=\"epoch\", yaxis_title=\"value\")\n\n        if self.clearml_logger:\n            self.clearml_logger.report_plotly(\n                title=\"Metrics\", series=\"metrics\", iteration=epoch, figure=fig_m\n            )\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(\n                f\"Plotly reporting failed at epoch {self.current_epoch}: {e}\"\n            )\n\n    # Generate UMAP visualizations on even epochs\n    if self.current_epoch % 2 == 0:\n        try:\n            import umap.umap_ as umap_module\n\n            # Only proceed if we have embeddings\n            if len(self._val_backbone_embeddings) &gt; 0 and len(self._val_quantized_embeddings) &gt; 0:\n                # Concatenate all accumulated embeddings\n                backbone_emb_flat = torch.cat(self._val_backbone_embeddings, dim=0)\n                quantized_emb_flat = torch.cat(self._val_quantized_embeddings, dim=0)\n\n                # Flatten spatial dimensions: [B, C, H, W] -&gt; [B*H*W, C]\n                backbone_emb_flat = backbone_emb_flat.permute(0, 2, 3, 1).reshape(-1, backbone_emb_flat.shape[1])\n                quantized_emb_flat = quantized_emb_flat.permute(0, 2, 3, 1).reshape(-1, quantized_emb_flat.shape[1])\n\n                # Convert to numpy\n                backbone_emb_np = backbone_emb_flat.numpy()\n                quantized_emb_np = quantized_emb_flat.numpy()\n\n                # Limit samples for performance (take subset if too large)\n                max_samples = 10000\n                if len(backbone_emb_np) &gt; max_samples:\n                    indices = np.random.choice(len(backbone_emb_np), max_samples, replace=False)\n                    backbone_emb_np = backbone_emb_np[indices]\n                    quantized_emb_np = quantized_emb_np[indices]\n\n                # Generate 2D UMAP\n                fig_2d, axs_2d = plt.subplots(1, 2, figsize=(12, 6))\n\n                proj_2d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(backbone_emb_np)\n                axs_2d[0].scatter(proj_2d_backbone[:, 0], proj_2d_backbone[:, 1], alpha=0.3)\n                axs_2d[0].set_title('2D UMAP: Backbone Embeddings')\n\n                proj_2d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine').fit_transform(quantized_emb_np)\n                axs_2d[1].scatter(proj_2d_quantized[:, 0], proj_2d_quantized[:, 1], alpha=0.3)\n                axs_2d[1].set_title('2D UMAP: Quantized Embeddings')\n\n                # Convert 2D plot to image and log\n                fig_2d.canvas.draw()\n                img_2d = np.frombuffer(fig_2d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_2d = img_2d.reshape(fig_2d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_2d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"2d_embeddings_epoch_{self.current_epoch}\", \n                        img_2d, \n                        iteration=self.current_epoch\n                    )\n\n                # Generate 3D UMAP\n                fig_3d = plt.figure(figsize=(12, 6))\n                ax1 = fig_3d.add_subplot(121, projection='3d')\n                ax2 = fig_3d.add_subplot(122, projection='3d')\n\n                proj_3d_backbone = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(backbone_emb_np)\n                ax1.scatter(proj_3d_backbone[:, 0], proj_3d_backbone[:, 1], proj_3d_backbone[:, 2], alpha=0.3)\n                ax1.set_title('3D UMAP: Backbone Embeddings')\n\n                proj_3d_quantized = umap_module.UMAP(n_neighbors=3, min_dist=0.1, metric='cosine', n_components=3).fit_transform(quantized_emb_np)\n                ax2.scatter(proj_3d_quantized[:, 0], proj_3d_quantized[:, 1], proj_3d_quantized[:, 2], alpha=0.3)\n                ax2.set_title('3D UMAP: Quantized Embeddings')\n\n                # Convert 3D plot to image and log\n                fig_3d.canvas.draw()\n                img_3d = np.frombuffer(fig_3d.canvas.tostring_rgb(), dtype=np.uint8)\n                img_3d = img_3d.reshape(fig_3d.canvas.get_width_height()[::-1] + (3,))\n                plt.close(fig_3d)\n\n                if self.clearml_logger:\n                    self.clearml_logger.log_image(\n                        \"umap_visualizations\", \n                        f\"3d_embeddings_epoch_{self.current_epoch}\", \n                        img_3d, \n                        iteration=self.current_epoch\n                    )\n\n            # Clear accumulated embeddings after logging\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n        except Exception as e:\n            if self.clearml_logger:\n                self.clearml_logger.report_text(\n                    f\"UMAP visualization failed at epoch {self.current_epoch}: {e}\"\n                )\n            # Clear embeddings even if visualization failed\n            self._val_backbone_embeddings.clear()\n            self._val_quantized_embeddings.clear()\n\n    # Save per-epoch embedding (first validation batch only)\n    try:\n        if self._first_val_batch_features is not None:\n            emb_path = os.path.join(\n                self.embedding_dir,\n                f\"val_embedding_epoch{self.current_epoch}.pt\"\n            )\n            torch.save(self._first_val_batch_features, emb_path)\n            if self.clearml_logger:\n                self.clearml_logger.report_text(f\"Saved small embedding: {emb_path}\")\n            # Reset for next epoch\n            self._first_val_batch_features = None\n    except Exception as e:\n        if self.clearml_logger:\n            self.clearml_logger.report_text(f\"Failed saving epoch embedding: {e}\")\n</code></pre> configure_optimizers \u00b6 <pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizer - only trainable parameters.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizer - only trainable parameters.\"\"\"\n    params = []\n\n    # Add adapter parameters if present\n    if self.feature_adapter is not None:\n        params += list(self.feature_adapter.parameters())\n\n    # Add quantizer parameters if present\n    if self.quantizer is not None:\n        params += list(self.quantizer.parameters())\n\n    # Add backbone parameters if not frozen\n    if self.feature_adapter is None:\n        params += [p for p in self.backbone.parameters() if p.requires_grad]\n\n    # Remove duplicates\n    params = list({id(p): p for p in params}.values())\n\n    if not params:\n        raise ValueError(\"No trainable parameters found!\")\n\n    return torch.optim.AdamW(params, lr=self.learning_rate)\n</code></pre> on_train_start \u00b6 <pre><code>on_train_start()\n</code></pre> <p>Ensure frozen backbone stays in eval mode.</p> Source code in <code>embeddings_squeeze\\models\\lightning_module.py</code> <pre><code>def on_train_start(self):\n    \"\"\"Ensure frozen backbone stays in eval mode.\"\"\"\n    if self.feature_adapter is not None:\n        self.backbone.eval()\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.losses","title":"losses","text":"<p>Loss functions for segmentation tasks. Includes: Cross Entropy, Dice Loss, Focal Loss, and Combined Loss.</p>"},{"location":"api/models/#embeddings_squeeze.models.losses-classes","title":"Classes","text":""},{"location":"api/models/#embeddings_squeeze.models.losses.DiceLoss","title":"DiceLoss","text":"<pre><code>DiceLoss(smooth=1.0)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Dice Loss for multi-class segmentation</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, smooth: float = 1.0):\n    super().__init__()\n    self.smooth = smooth\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.losses.FocalLoss","title":"FocalLoss","text":"<pre><code>FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Focal Loss for handling class imbalance (multi-class via CE per-pixel)</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(self, alpha: float = 1.0, gamma: float = 2.0, reduction: str = 'mean'):\n    super().__init__()\n    self.alpha = alpha\n    self.gamma = gamma\n    self.reduction = reduction\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.losses.CombinedLoss","title":"CombinedLoss","text":"<pre><code>CombinedLoss(\n    ce_weight=1.0,\n    dice_weight=1.0,\n    focal_weight=0.5,\n    class_weights=None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Combined loss: CE + Dice + Focal. Returns (total, ce, dice, focal).</p> Source code in <code>embeddings_squeeze\\models\\losses.py</code> <pre><code>def __init__(\n    self, \n    ce_weight: float = 1.0, \n    dice_weight: float = 1.0, \n    focal_weight: float = 0.5, \n    class_weights=None\n):\n    super().__init__()\n    # class_weights can be None or a tensor/list\n    if class_weights is not None:\n        # Leave tensor creation to forward (to place on correct device) but store raw\n        self._class_weights = class_weights\n    else:\n        self._class_weights = None\n\n    self.ce_weight = ce_weight\n    self.dice_weight = dice_weight\n    self.focal_weight = focal_weight\n\n    # Instantiate component losses\n    self.dice_loss = DiceLoss()\n    self.focal_loss = FocalLoss()\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.quantizers","title":"quantizers","text":"<p>Vector Quantization implementations using vector_quantize_pytorch library. Supports: VQ-VAE, FSQ, LFQ, and Residual VQ.</p>"},{"location":"api/models/#embeddings_squeeze.models.quantizers-classes","title":"Classes","text":""},{"location":"api/models/#embeddings_squeeze.models.quantizers.BaseQuantizer","title":"BaseQuantizer","text":"<pre><code>BaseQuantizer(input_dim)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for all quantizers</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int):\n    super().__init__()\n    self.input_dim = input_dim\n</code></pre> Functions\u00b6 quantize_spatial \u00b6 <pre><code>quantize_spatial(features)\n</code></pre> <p>Quantize spatial features [B, C, H, W]</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Tensor of shape [B, C, H, W]</p> required <p>Returns:</p> Name Type Description <code>quantized</code> <p>Quantized features [B, C, H, W]</p> <code>loss</code> <p>Quantization loss (scalar)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def quantize_spatial(self, features: torch.Tensor):\n    \"\"\"\n    Quantize spatial features [B, C, H, W]\n\n    Args:\n        features: Tensor of shape [B, C, H, W]\n\n    Returns:\n        quantized: Quantized features [B, C, H, W]\n        loss: Quantization loss (scalar)\n    \"\"\"\n    B, C, H, W = features.shape\n    # Transform [B, C, H, W] -&gt; [B, H*W, C]\n    seq = features.permute(0, 2, 3, 1).reshape(B, H * W, C)\n\n    # Quantize\n    quantized, indices, loss = self.forward(seq)\n\n    # Transform back [B, H*W, C] -&gt; [B, C, H, W]\n    quantized = quantized.reshape(B, H, W, C).permute(0, 3, 1, 2)\n\n    # Handle loss (may be tensor with multiple elements)\n    if isinstance(loss, torch.Tensor) and loss.numel() &gt; 1:\n        loss = loss.mean()\n\n    return quantized, loss\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.quantizers.VQWithProjection","title":"VQWithProjection","text":"<pre><code>VQWithProjection(\n    input_dim,\n    codebook_size=512,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Vector Quantization (VQ-VAE) with projections</p> <p>Uses EMA for codebook updates (no gradients needed for codebook) ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection (e.g., 2048 -&gt; 64)\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Vector Quantization\n    self.vq = VectorQuantize(\n        dim=bottleneck_dim,\n        codebook_size=codebook_size,\n        decay=decay,  # EMA decay for codebook\n        commitment_weight=commitment_weight  # Commitment loss weight\n    )\n\n    # Up projection (64 -&gt; 2048)\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.quantizers.FSQWithProjection","title":"FSQWithProjection","text":"<pre><code>FSQWithProjection(input_dim, levels=None)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Finite Scalar Quantization (FSQ)</p> <p>Quantization without codebook - each dimension quantized independently ~10 bits per vector at levels=[8,5,5,5]</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(self, input_dim: int, levels: list = None):\n    super().__init__(input_dim)\n    if levels is None:\n        levels = [8, 5, 5, 5]  # 8*5*5*5 = 1000 codes \u2248 2^10\n\n    self.num_levels = len(levels)\n\n    # Projection to quantization space\n    self.project_in = nn.Linear(input_dim, self.num_levels)\n\n    # FSQ quantization\n    self.fsq = FSQ(levels=levels, dim=self.num_levels)\n\n    # Projection back\n    self.project_out = nn.Linear(self.num_levels, input_dim)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.quantizers.LFQWithProjection","title":"LFQWithProjection","text":"<pre><code>LFQWithProjection(\n    input_dim,\n    codebook_size=512,\n    entropy_loss_weight=0.1,\n    diversity_gamma=0.1,\n    spherical=False,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Lookup-Free Quantization (LFQ)</p> <p>Uses entropy loss for code diversity ~9 bits per vector at codebook_size=512</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    codebook_size: int = 512,\n    entropy_loss_weight: float = 0.1, \n    diversity_gamma: float = 0.1, \n    spherical: bool = False\n):\n    super().__init__(input_dim)\n    # Quantization dimension = log2(codebook_size)\n    self.quant_dim = int(math.log2(codebook_size))\n\n    # Projection with normalization\n    self.project_in = nn.Sequential(\n        nn.Linear(input_dim, self.quant_dim),\n        nn.LayerNorm(self.quant_dim)\n    )\n\n    # LFQ quantization\n    self.lfq = LFQ(\n        dim=self.quant_dim,\n        codebook_size=codebook_size,\n        entropy_loss_weight=entropy_loss_weight,\n        diversity_gamma=diversity_gamma,\n        spherical=spherical\n    )\n\n    # Projection back\n    self.project_out = nn.Linear(self.quant_dim, input_dim)\n</code></pre>"},{"location":"api/models/#embeddings_squeeze.models.quantizers.ResidualVQWithProjection","title":"ResidualVQWithProjection","text":"<pre><code>ResidualVQWithProjection(\n    input_dim,\n    num_quantizers=4,\n    codebook_size=256,\n    bottleneck_dim=64,\n    decay=0.99,\n    commitment_weight=0.25,\n)\n</code></pre> <p>               Bases: <code>BaseQuantizer</code></p> <p>Residual Vector Quantization (RVQ)</p> <p>Multi-level quantization - each level quantizes the residual of the previous 32 bits per vector at num_quantizers=4, codebook_size=256 (4*8 bits)</p> Source code in <code>embeddings_squeeze\\models\\quantizers.py</code> <pre><code>def __init__(\n    self, \n    input_dim: int, \n    num_quantizers: int = 4,\n    codebook_size: int = 256, \n    bottleneck_dim: int = 64,\n    decay: float = 0.99, \n    commitment_weight: float = 0.25\n):\n    super().__init__(input_dim)\n    self.bottleneck_dim = bottleneck_dim\n\n    # Down projection\n    self.project_in = nn.Linear(input_dim, bottleneck_dim)\n\n    # Residual VQ\n    self.residual_vq = ResidualVQ(\n        dim=bottleneck_dim,\n        num_quantizers=num_quantizers,  # Number of levels\n        codebook_size=codebook_size,\n        decay=decay,\n        commitment_weight=commitment_weight\n    )\n\n    # Up projection\n    self.project_out = nn.Linear(bottleneck_dim, input_dim)\n</code></pre>"},{"location":"api/utils/","title":"\u0414\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0443\u0442\u0438\u043b\u0438\u0442","text":""},{"location":"api/utils/#embeddings_squeeze.utils","title":"utils","text":"<p>Utility functions for VQ compression.</p>"},{"location":"api/utils/#embeddings_squeeze.utils-functions","title":"Functions","text":""},{"location":"api/utils/#embeddings_squeeze.utils.measure_compression","title":"measure_compression","text":"<pre><code>measure_compression(\n    vq_model, backbone, test_loader, device\n)\n</code></pre> <p>Measure compression ratio achieved by VQ.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VectorQuantizer model</p> required <code>backbone</code> <p>Segmentation backbone</p> required <code>test_loader</code> <p>Test data loader</p> required <code>device</code> <p>Device to run on</p> required <p>Returns:</p> Name Type Description <code>compression_ratio</code> <p>Compression ratio achieved</p> Source code in <code>embeddings_squeeze\\utils\\compression.py</code> <pre><code>def measure_compression(vq_model, backbone, test_loader, device):\n    \"\"\"\n    Measure compression ratio achieved by VQ.\n\n    Args:\n        vq_model: VectorQuantizer model\n        backbone: Segmentation backbone\n        test_loader: Test data loader\n        device: Device to run on\n\n    Returns:\n        compression_ratio: Compression ratio achieved\n    \"\"\"\n    vq_model.eval()\n\n    total_original_bits = 0\n    total_compressed_bits = 0\n    num_features = 0\n\n    with torch.no_grad():\n        for images, _ in test_loader:\n            images = images.to(device)\n            features = backbone.extract_features(images)\n\n            B, C, H, W = features.shape\n            feat_flat = features.permute(0, 2, 3, 1).reshape(-1, C)\n\n            # Original size (float32 = 32 bits)\n            original_bits = feat_flat.numel() * 32\n\n            # Compressed size (only indices)\n            num_codes = vq_model.num_vectors\n            bits_per_index = np.ceil(np.log2(num_codes))\n            compressed_bits = feat_flat.shape[0] * bits_per_index\n\n            total_original_bits += original_bits\n            total_compressed_bits += compressed_bits\n            num_features += feat_flat.shape[0]\n\n    compression_ratio = total_original_bits / total_compressed_bits\n\n    print(\"=\"*70)\n    print(\"COMPRESSION ANALYSIS\")\n    print(\"=\"*70)\n    print(f\"Total features processed: {num_features}\")\n    print(f\"Feature dimension: {vq_model.vector_dim}\")\n    print(f\"Codebook size: {vq_model.num_vectors}\")\n    print()\n    print(f\"Original storage:\")\n    print(f\"  Per feature: {vq_model.vector_dim} \u00d7 32 bits = {vq_model.vector_dim * 32} bits = {vq_model.vector_dim * 4} bytes\")\n    print(f\"  Total: {total_original_bits / 8 / 1024 / 1024:.2f} MB\")\n    print()\n    print(f\"Compressed storage:\")\n    print(f\"  Per feature: {bits_per_index:.0f} bits (index)\")\n    print(f\"  Total: {total_compressed_bits / 8 / 1024:.2f} KB\")\n    print(f\"  + Codebook: {vq_model.num_vectors * vq_model.vector_dim * 4 / 1024:.2f} KB\")\n    print()\n    print(f\"Compression ratio: {compression_ratio:.1f}x\")\n    print(f\"Space savings: {(1 - 1/compression_ratio)*100:.1f}%\")\n    print(\"=\"*70)\n\n    return compression_ratio\n</code></pre>"},{"location":"api/utils/#embeddings_squeeze.utils.compute_iou_metrics","title":"compute_iou_metrics","text":"<pre><code>compute_iou_metrics(\n    predictions, targets, num_classes, ignore_index=255\n)\n</code></pre> <p>Compute IoU metrics for segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <p>Predicted masks [B, H, W]</p> required <code>targets</code> <p>Ground truth masks [B, H, W]</p> required <code>num_classes</code> <p>Number of classes</p> required <code>ignore_index</code> <p>Index to ignore in computation</p> <code>255</code> <p>Returns:</p> Name Type Description <code>metrics</code> <p>Dictionary with IoU metrics</p> Source code in <code>embeddings_squeeze\\utils\\compression.py</code> <pre><code>def compute_iou_metrics(predictions, targets, num_classes, ignore_index=255):\n    \"\"\"\n    Compute IoU metrics for segmentation.\n\n    Args:\n        predictions: Predicted masks [B, H, W]\n        targets: Ground truth masks [B, H, W]\n        num_classes: Number of classes\n        ignore_index: Index to ignore in computation\n\n    Returns:\n        metrics: Dictionary with IoU metrics\n    \"\"\"\n    predictions = predictions.cpu().numpy()\n    targets = targets.cpu().numpy()\n\n    # Flatten arrays\n    pred_flat = predictions.flatten()\n    target_flat = targets.flatten()\n\n    # Remove ignored pixels\n    valid_mask = target_flat != ignore_index\n    pred_flat = pred_flat[valid_mask]\n    target_flat = target_flat[valid_mask]\n\n    # Compute IoU for each class\n    ious = []\n    for cls in range(num_classes):\n        pred_cls = (pred_flat == cls)\n        target_cls = (target_flat == cls)\n\n        intersection = (pred_cls &amp; target_cls).sum()\n        union = pred_cls.sum() + target_cls.sum() - intersection\n\n        if union &gt; 0:\n            iou = intersection / union\n        else:\n            iou = 0.0\n\n        ious.append(iou)\n\n    # Compute mean IoU\n    mean_iou = np.mean(ious)\n\n    # Compute pixel accuracy\n    pixel_acc = (pred_flat == target_flat).mean()\n\n    metrics = {\n        'mean_iou': mean_iou,\n        'pixel_accuracy': pixel_acc,\n        'class_ious': ious\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/utils/#embeddings_squeeze.utils.initialize_codebook_from_data","title":"initialize_codebook_from_data","text":"<pre><code>initialize_codebook_from_data(\n    vq_model,\n    backbone,\n    train_loader,\n    device,\n    max_samples=50000,\n)\n</code></pre> <p>Initialize codebook using k-means clustering on real data.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VectorQuantizer model</p> required <code>backbone</code> <p>Segmentation backbone</p> required <code>train_loader</code> <p>Training data loader</p> required <code>device</code> <p>Device to run on</p> required <code>max_samples</code> <code>int</code> <p>Maximum number of samples for k-means</p> <code>50000</code> Source code in <code>embeddings_squeeze\\utils\\initialization.py</code> <pre><code>def initialize_codebook_from_data(\n    vq_model, \n    backbone, \n    train_loader, \n    device, \n    max_samples: int = 50_000\n):\n    \"\"\"\n    Initialize codebook using k-means clustering on real data.\n\n    Args:\n        vq_model: VectorQuantizer model\n        backbone: Segmentation backbone\n        train_loader: Training data loader\n        device: Device to run on\n        max_samples: Maximum number of samples for k-means\n    \"\"\"\n    print(\"Collecting features for k-means initialization...\")\n    all_features = []\n\n    backbone.eval()\n    with torch.no_grad():\n        i = 0\n        for images, _ in train_loader:\n            print(f\"Processing batch {i}\")\n            features = backbone.extract_features(images.to(device))\n            i += 1\n\n            B, C, H, W = features.shape\n            feat_flat = features.permute(0, 2, 3, 1).reshape(-1, C)\n            all_features.append(feat_flat.cpu())\n\n            if len(all_features) * feat_flat.shape[0] &gt; max_samples:\n                break\n\n    all_features = torch.cat(all_features).numpy()\n\n    print(f\"Running k-means on {all_features.shape[0]} features...\")\n    kmeans = MiniBatchKMeans(\n        n_clusters=vq_model.num_vectors,\n        random_state=0,\n        batch_size=1000,\n        max_iter=100\n    )\n    kmeans.fit(all_features)\n\n    # Update codebook with cluster centers\n    vq_model.codebook.embeddings.data = torch.tensor(kmeans.cluster_centers_).to(device).float()\n\n    print(f\"Codebook initialized:\")\n    print(f\"  Mean: {vq_model.codebook.embeddings.mean():.2f}\")\n    print(f\"  Std: {vq_model.codebook.embeddings.std():.2f}\")\n    print(f\"  Norm: {vq_model.codebook.embeddings.norm(dim=1).mean():.2f}\")\n</code></pre>"},{"location":"api/utils/#embeddings_squeeze.utils.compute_sample_iou","title":"compute_sample_iou","text":"<pre><code>compute_sample_iou(\n    prediction, target, num_classes, ignore_index=255\n)\n</code></pre> <p>Compute IoU for a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>Predicted mask [H, W]</p> required <code>target</code> <code>Tensor</code> <p>Ground truth mask [H, W]</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> required <code>ignore_index</code> <code>int</code> <p>Index to ignore in computation</p> <code>255</code> <p>Returns:</p> Name Type Description <code>iou</code> <code>float</code> <p>Mean IoU across all classes</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def compute_sample_iou(prediction: torch.Tensor, target: torch.Tensor, num_classes: int, ignore_index: int = 255) -&gt; float:\n    \"\"\"\n    Compute IoU for a single sample.\n\n    Args:\n        prediction: Predicted mask [H, W]\n        target: Ground truth mask [H, W]\n        num_classes: Number of classes\n        ignore_index: Index to ignore in computation\n\n    Returns:\n        iou: Mean IoU across all classes\n    \"\"\"\n    prediction = prediction.cpu().numpy()\n    target = target.cpu().numpy()\n\n    # Flatten arrays\n    pred_flat = prediction.flatten()\n    target_flat = target.flatten()\n\n    # Remove ignored pixels\n    valid_mask = target_flat != ignore_index\n    pred_flat = pred_flat[valid_mask]\n    target_flat = target_flat[valid_mask]\n\n    # Compute IoU for each class\n    ious = []\n    for cls in range(num_classes):\n        pred_cls = (pred_flat == cls)\n        target_cls = (target_flat == cls)\n\n        intersection = (pred_cls &amp; target_cls).sum()\n        union = pred_cls.sum() + target_cls.sum() - intersection\n\n        if union &gt; 0:\n            iou = intersection / union\n        else:\n            iou = 0.0\n\n        ious.append(iou)\n\n    # Return mean IoU\n    return np.mean(ious)\n</code></pre>"},{"location":"api/utils/#embeddings_squeeze.utils.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(model, dataloader, device, num_classes=21)\n</code></pre> <p>Evaluate model on dataset and collect results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Model to evaluate</p> required <code>dataloader</code> <p>Data loader</p> required <code>device</code> <p>Device to run on</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> <p>Returns:</p> Name Type Description <code>results</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor]]</code> <p>List of (sample_idx, iou, image, mask, prediction) tuples</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def evaluate_model(model, dataloader, device, num_classes: int = 21) -&gt; List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Evaluate model on dataset and collect results.\n\n    Args:\n        model: Model to evaluate\n        dataloader: Data loader\n        device: Device to run on\n        num_classes: Number of classes\n\n    Returns:\n        results: List of (sample_idx, iou, image, mask, prediction) tuples\n    \"\"\"\n    model.eval()\n    results = []\n\n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(dataloader):\n            images = images.to(device)\n            masks = masks.squeeze(1).long().to(device)\n\n            # Get predictions\n            if hasattr(model, 'predict'):\n                predictions = model.predict(images)\n            else:\n                # For VQ model\n                predictions = model.predict_with_vq(images)\n\n            # Process each sample in batch\n            for i in range(images.shape[0]):\n                image = images[i]\n                mask = masks[i]\n                pred = predictions[i]\n\n                # Compute IoU\n                iou = compute_sample_iou(pred, mask, num_classes)\n\n                # Store results\n                sample_idx = batch_idx * dataloader.batch_size + i\n                results.append((sample_idx, iou, image, mask, pred))\n\n    return results\n</code></pre>"},{"location":"api/utils/#embeddings_squeeze.utils.find_best_worst_samples","title":"find_best_worst_samples","text":"<pre><code>find_best_worst_samples(results, n_best=5, n_worst=5)\n</code></pre> <p>Find best and worst samples based on IoU.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor]]</code> <p>List of (sample_idx, iou, image, mask, prediction) tuples</p> required <code>n_best</code> <code>int</code> <p>Number of best samples to return</p> <code>5</code> <code>n_worst</code> <code>int</code> <p>Number of worst samples to return</p> <code>5</code> <p>Returns:</p> Name Type Description <code>best_samples</code> <code>List</code> <p>List of best sample tuples</p> <code>worst_samples</code> <code>List</code> <p>List of worst sample tuples</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def find_best_worst_samples(results: List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor]], \n                           n_best: int = 5, n_worst: int = 5) -&gt; Tuple[List, List]:\n    \"\"\"\n    Find best and worst samples based on IoU.\n\n    Args:\n        results: List of (sample_idx, iou, image, mask, prediction) tuples\n        n_best: Number of best samples to return\n        n_worst: Number of worst samples to return\n\n    Returns:\n        best_samples: List of best sample tuples\n        worst_samples: List of worst sample tuples\n    \"\"\"\n    # Sort by IoU (descending)\n    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n\n    # Get best and worst\n    best_samples = sorted_results[:n_best]\n    worst_samples = sorted_results[-n_worst:]\n\n    return best_samples, worst_samples\n</code></pre>"},{"location":"api/utils/#embeddings_squeeze.utils.prepare_visualization_data","title":"prepare_visualization_data","text":"<pre><code>prepare_visualization_data(\n    vq_model,\n    baseline_model,\n    dataloader,\n    device,\n    num_classes=21,\n    n_best=5,\n    n_worst=5,\n)\n</code></pre> <p>Prepare data for visualization by running both models and ranking results.</p> <p>Parameters:</p> Name Type Description Default <code>vq_model</code> <p>VQ model</p> required <code>baseline_model</code> <p>Baseline model</p> required <code>dataloader</code> <p>Data loader</p> required <code>device</code> <p>Device to run on</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> <code>n_best</code> <code>int</code> <p>Number of best samples</p> <code>5</code> <code>n_worst</code> <code>int</code> <p>Number of worst samples</p> <code>5</code> <p>Returns:</p> Name Type Description <code>best_samples</code> <p>List of best sample tuples with both predictions</p> <code>worst_samples</code> <p>List of worst sample tuples with both predictions</p> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def prepare_visualization_data(vq_model, baseline_model, dataloader, device, \n                              num_classes: int = 21, n_best: int = 5, n_worst: int = 5):\n    \"\"\"\n    Prepare data for visualization by running both models and ranking results.\n\n    Args:\n        vq_model: VQ model\n        baseline_model: Baseline model\n        dataloader: Data loader\n        device: Device to run on\n        num_classes: Number of classes\n        n_best: Number of best samples\n        n_worst: Number of worst samples\n\n    Returns:\n        best_samples: List of best sample tuples with both predictions\n        worst_samples: List of worst sample tuples with both predictions\n    \"\"\"\n    # Evaluate VQ model\n    print(\"Evaluating VQ model...\")\n    vq_results = evaluate_model(vq_model, dataloader, device, num_classes)\n\n    # Evaluate baseline model\n    print(\"Evaluating baseline model...\")\n    baseline_results = evaluate_model(baseline_model, dataloader, device, num_classes)\n\n    # Combine results (assuming same order)\n    combined_results = []\n    for (idx1, iou1, img1, mask1, pred_vq), (idx2, iou2, img2, mask2, pred_baseline) in zip(vq_results, baseline_results):\n        assert idx1 == idx2, \"Sample indices don't match\"\n        assert torch.equal(img1, img2), \"Images don't match\"\n        assert torch.equal(mask1, mask2), \"Masks don't match\"\n\n        combined_results.append((idx1, iou1, img1, mask1, pred_baseline, pred_vq))\n\n    # Find best and worst based on VQ IoU\n    best_samples, worst_samples = find_best_worst_samples(combined_results, n_best, n_worst)\n\n    return best_samples, worst_samples\n</code></pre>"},{"location":"api/utils/#embeddings_squeeze.utils.visualize_comparison","title":"visualize_comparison","text":"<pre><code>visualize_comparison(\n    samples, title, output_path, num_classes=21\n)\n</code></pre> <p>Create visualization comparing baseline and VQ predictions.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>List[Tuple[int, float, Tensor, Tensor, Tensor, Tensor]]</code> <p>List of (idx, iou, image, mask, pred_baseline, pred_vq) tuples</p> required <code>title</code> <code>str</code> <p>Figure title</p> required <code>output_path</code> <code>str</code> <p>Path to save figure</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>21</code> Source code in <code>embeddings_squeeze\\utils\\comparison.py</code> <pre><code>def visualize_comparison(samples: List[Tuple[int, float, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]], \n                        title: str, output_path: str, num_classes: int = 21):\n    \"\"\"\n    Create visualization comparing baseline and VQ predictions.\n\n    Args:\n        samples: List of (idx, iou, image, mask, pred_baseline, pred_vq) tuples\n        title: Figure title\n        output_path: Path to save figure\n        num_classes: Number of classes\n    \"\"\"\n    n_samples = len(samples)\n    fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4 * n_samples))\n\n    if n_samples == 1:\n        axes = axes.reshape(1, -1)\n\n    colormap = create_segmentation_colormap(num_classes)\n\n    for i, (idx, iou, image, mask, pred_baseline, pred_vq) in enumerate(samples):\n        # Denormalize image\n        image_vis = denormalize_image(image)\n\n        # Convert to numpy for plotting\n        image_np = image_vis.permute(1, 2, 0).cpu().numpy()\n        mask_np = mask.cpu().numpy()\n        pred_baseline_np = pred_baseline.cpu().numpy()\n        pred_vq_np = pred_vq.cpu().numpy()\n\n        # Plot original image\n        axes[i, 0].imshow(image_np)\n        axes[i, 0].set_title(f\"Original Image\\nSample {idx}\")\n        axes[i, 0].axis('off')\n\n        # Plot ground truth\n        axes[i, 1].imshow(mask_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 1].set_title(\"Ground Truth\")\n        axes[i, 1].axis('off')\n\n        # Plot baseline prediction\n        axes[i, 2].imshow(pred_baseline_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 2].set_title(\"Baseline Prediction\")\n        axes[i, 2].axis('off')\n\n        # Plot VQ prediction\n        axes[i, 3].imshow(pred_vq_np, cmap=colormap, vmin=0, vmax=num_classes-1)\n        axes[i, 3].set_title(f\"VQ Prediction\\nIoU: {iou:.3f}\")\n        axes[i, 3].axis('off')\n\n    plt.suptitle(title, fontsize=16)\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"Visualization saved to: {output_path}\")\n</code></pre>"}]}